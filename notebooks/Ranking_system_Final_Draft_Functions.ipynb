{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant Ranking Algorithm\n",
    "The method of ranking each merchant, will take inspiration from the methods proposed in (https://sapinsider.org/leveraging-analytical-method-for-ranking-suppliers/), in which we scale each sub_attribute with min-max norminalisation, then sum them together to get the overall score for a particular Metric\n",
    "\n",
    "The Key Attributes/Metrics are: \n",
    "- Finantial \n",
    "- Customer_Base \n",
    "- Sustainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions (probably already implemented from other processing)\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import feature as H\n",
    "from pyspark.sql.functions import coalesce\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This opens the spark session \n",
    "def open_spark():\n",
    "    spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate())\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously Unused Functions\n",
    "For some reason these functions are not applied in the process of obtaining the processed/merhcants.parquet file, which will be requierd to be added to ensure this notebook/functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to standardise the description of each merchant\n",
    "def text_process(text):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    # Remove all punctuation and numbers \n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    # Remove all stopwords\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    # lemmatize and output\n",
    "    return ' '.join([stemmer.lemmatize(word) for word in nopunc])\n",
    "\n",
    "# this function standardises the tags attribute, creating a list with the 'description', 'revenue band' and 'BNPL service charge'\n",
    "def tag_extract(tag_string): \n",
    "    # first need to preprocess\n",
    "    string =  re.sub('\\[','(', tag_string.lower())\n",
    "    string = re.sub('\\]',')', string)\n",
    "    # break the string into sections\n",
    "    string_cut = string.split('),')\n",
    "    new_string = []\n",
    "    # first extract the description and pre process\n",
    "    descr = str(string_cut[0].strip('(('))\n",
    "    new_string.append(text_process(descr))\n",
    "    # second extract the band\n",
    "    new_string.append(str(re.search(r'[a-z]',string_cut[1]).group()))\n",
    "    # finally the take rate\n",
    "    new_string.append(float(re.search(r'[0-9]+\\.[0-9]+',string_cut[2]).group()))\n",
    "    return(new_string)\n",
    "\n",
    "# This takes the dataset, and assigns the tag an unique integer value\n",
    "def description_convert(processed_tags):\n",
    "    # first work out how many tags in list \n",
    "    # Now assign each tag a value \n",
    "    assign = 0\n",
    "    lookup_tags = {}\n",
    "    for tag in processed_tags:\n",
    "        if tag not in lookup_tags.keys():\n",
    "            lookup_tags[tag] = assign\n",
    "            assign = assign + 1\n",
    "    # Write lookup table\n",
    "    with open('../data/tables/description_lookup.csv', 'w') as f:\n",
    "        for key in lookup_tags.keys():\n",
    "           f.write(\"%s, %s\\n\" % (key, lookup_tags[key]))\n",
    "    # now convert values\n",
    "    new_tags = []\n",
    "    for tag in processed_tags:\n",
    "        new_tags.append(lookup_tags[tag])\n",
    "    return new_tags\n",
    "\n",
    "# This function takes the pandas dataframe containing merchant information and pre_processes it\n",
    "def merchant_process(merchants, spark):\n",
    "    merchants = merchants.toPandas()\n",
    "    # Lets process the tags\n",
    "    tags = merchants['tags']\n",
    "    processed_tags = []\n",
    "    for i in tags:\n",
    "        processed_tags.append(tag_extract(i))\n",
    "    merchant_tbl = pd.DataFrame(processed_tags, columns=('Description', 'Earnings_Class', 'BNPL_Fee'))\n",
    "    merchant_tbl['Description'] = description_convert(list(merchant_tbl['Description'].values))\n",
    "    merchant_tbl = pd.concat([merchants, merchant_tbl], axis=1)\n",
    "    # drop the tags column \n",
    "    merchant_tbl.drop(columns='tags', inplace=True)\n",
    "    merchant_tbl = spark.createDataFrame(merchant_tbl)\n",
    "    return merchant_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our scaling function\n",
    "def feature_standardisation(dataset, max_columns, min_columns = False): \n",
    "    # if higher values are prevered\n",
    "    for col_name in max_columns:\n",
    "        values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "        dataset = dataset.withColumn(col_name, F.round((F.col(col_name) - values.select('low').head()[0]) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    # if lower values are prefered\n",
    "    if min_columns != False:\n",
    "        for col_name in min_columns:\n",
    "            values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "            dataset = dataset.withColumn(col_name, F.round((values.select('high').head()[0] - F.col(col_name)) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finacial Metric\n",
    "For this we will look into:\n",
    "- Total_Revenue\n",
    "- Average_Revenue_Growth\n",
    "\n",
    "These are both taken over the last 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the revenue growth score\n",
    "def revenue_growth_score(revenue_data):\n",
    "    # Revenye Growth\n",
    "    revenue_data_table = revenue_data.select('merchant_abn').distinct()\n",
    "    # now for the past 6 monts calculate THIS IS HARD CODED\n",
    "    months = [3,4,5,6,7,8,9]\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "    revenue_data_table = revenue_data_table.fillna(1)\n",
    "    # Now need to calculate growth rate for each month\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_4', (F.col('Month_4') - F.col('Month_3')) / F.col('Month_4'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_5', (F.col('Month_5') - F.col('Month_4')) / F.col('Month_5'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Revenue_Growth_Avg', F.round((F.col('Growth_4') + F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9')) / 6 , 4))\n",
    "    revenue_data_table = revenue_data_table.select('merchant_abn', 'Revenue_Growth_Avg')\n",
    "    return revenue_data_table\n",
    "\n",
    "# This function scores each merchant finantially (i.e. Finantial Score)\n",
    "def finantial_score(RECENCY, transactions, fraud_data):\n",
    "    # first d\n",
    "    revenue_data = transactions.where(F.col('order_datetime') > RECENCY).select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue')\n",
    "    # Now join fraud prob \n",
    "    revenue_data = revenue_data.join(fraud_data, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    # Firstly, calculate the revenue score\n",
    "\n",
    "    # Weight the revenue weighted with probability\n",
    "    revenue_data = revenue_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction'))\n",
    "                                                                 )\n",
    "    # Now can caluculate each Merchants revenue\n",
    "    revenue_final = revenue_data.groupBy('merchant_abn').agg(F.round(F.sum('BNPL_weighted_Revenue'), 2).alias('Total_Revenue'))\n",
    "    # Next, get the revenue growth score \n",
    "    revenue_final = revenue_final.join(revenue_growth_score(revenue_data), on='merchant_abn', how='left')\n",
    "    # Finally scale\n",
    "    revenue_final = feature_standardisation(revenue_final, ['Revenue_Growth_Avg', 'Total_Revenue'])\n",
    "    # save file \n",
    "    revenue_final.write.parquet('../data/Normalised/revenue_scaled1', mode ='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability \n",
    "- Experience\n",
    "- Customer Growth (Last 6 months)\n",
    "- Postcode_Entropy\n",
    "- Industry score (added when scoring each metric [next notebook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_growth(full_dataset):\n",
    "    # First grab total Unique customers \n",
    "    Unique_cust = full_dataset.where(F.col('order_datetime') < F.lit('2022-05-01')).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'))\n",
    "    # now for the past 6 monts calculate \n",
    "    months = [5,6,7,8,9,10]\n",
    "    customer_counting_data = full_dataset.select('merchant_abn', 'user_id', 'order_datetime', 'month')\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "    Unique_cust = Unique_cust.fillna(1)\n",
    "    # Now need to calculate growth rate for each month\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_5', F.col('Unique_Customers') / F.col('Month_5'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_10',(F.col('Month_10') - F.col('Month_9')) / F.col('Month_10'))\n",
    "    # Finally get the average customer growth rate over the last 6 months\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.round((F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9') + F.col('Growth_10')) / 6 , 4))\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.when(F.col('Customer_Growth_Avg').isNull(), 0).otherwise(F.col('Customer_Growth_Avg')))\n",
    "    return Unique_cust.select('merchant_abn', 'Customer_Growth_Avg')\n",
    "\n",
    "def sustainability_score(merchants, full_dataset):\n",
    "    merchants_sub = merchants.select(['merchant_abn', 'postcode_entropy'])\n",
    "    merchants_sub = merchants_sub.join(customer_growth(full_dataset), on='merchant_abn', how='left')\n",
    "    # calculate experience\n",
    "    number_of_dates_by_merchant = full_dataset.groupBy('merchant_abn','order_datetime').count()\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.drop('order_datetime')\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.groupBy('merchant_abn').sum('count')\n",
    "    merchants_sub = merchants_sub.join(number_of_dates_by_merchant.withColumnRenamed('sum(count)','Total_Business_Days'), on='merchant_abn', how='left')\n",
    "    merchants_final = feature_standardisation(merchants_sub, ['Total_Business_Days', 'Customer_Growth_Avg'], ['postcode_entropy'])\n",
    "    # save file \n",
    "    merchants_final.write.parquet('../data/Normalised/sustainability_scaled1', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Base\n",
    "In this sections we use the features:\n",
    "- Transaction_per_User\n",
    "- customer_wealth\n",
    "- Unique_Customers\n",
    "- Loyal_AVG\n",
    "- postcode_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loyalty(full_dataset):\n",
    "    # Create Loyalfy feature\n",
    "    grouped = full_dataset.groupBy(\"user_id\", \"merchant_abn\")\n",
    "    RPR = grouped.count().withColumnRenamed(\"count\", \"RPR\")\n",
    "    upSell = RPR.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"upsell\")\n",
    "    CLV = grouped.sum(\"dollar_value\").withColumnRenamed(\"sum(dollar_value)\", \"CLV\")\n",
    "    # Define the window\n",
    "    w = Window.partitionBy([\"user_id\", \"merchant_abn\"]).orderBy(\"order_datetime\")\n",
    "\n",
    "    retention = full_dataset.withColumn(\n",
    "        'diff',\n",
    "        F.datediff(F.col(\"order_datetime\"), F.lag(\"order_datetime\").over(w))\n",
    "    ).groupBy(\"user_id\", \"merchant_abn\").agg(F.avg(F.col(\"diff\")).alias(\"retention\"))\n",
    "    loyal = retention.na.fill(value=365).join(RPR, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(CLV, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(upSell, on=[\"user_id\"], how=\"left\")\n",
    "    loyal = loyal.withColumn(\"loyal\", F.col(\"RPR\") * F.col(\"CLV\") * F.col(\"upSell\") / F.col(\"retention\"))\n",
    "    loyal = loyal.select(*(col(c).cast(\"float\").alias(c) for c in loyal.columns))\n",
    "    loyal = loyal.select('user_id', 'merchant_abn', 'loyal')\n",
    "    magic_percentile = F.expr('percentile_approx(loyal, 0.5)')\n",
    "    loyal = loyal.join(loyal.groupBy('user_id').agg(magic_percentile.alias('med_val')), on='user_id', how='left')\n",
    "    loyal = loyal.withColumn('loyal', coalesce('loyal', 'med_val')).select(['user_id', 'merchant_abn', 'loyal'])\n",
    "    loyal_agg = loyal.groupBy('merchant_abn').agg(F.round(F.avg('loyal'), 2).alias('Loyal_AVG'))\n",
    "    return loyal_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_entropy(full_dataset):\n",
    "    # Get customer Entropy \n",
    "    cust_entro = full_dataset.select('merchant_abn', 'user_id', 'postcode', 'order_datetime')\n",
    "    Entropy_cust = cust_entro.select('merchant_abn').toDF('merchant_abn').drop_duplicates().crossJoin(cust_entro.select('postcode').toDF('postcode').drop_duplicates())\n",
    "    Entropy_cust = Entropy_cust.join(cust_entro.groupBy('merchant_abn', 'postcode').agg(F.countDistinct('user_id').alias('Count')), on=['merchant_abn', 'postcode'], how='left')\n",
    "    Entropy_cust = Entropy_cust.fillna(1)\n",
    "    Entropy_cust = Entropy_cust.join(Entropy_cust.groupBy(\"merchant_abn\").sum('Count'), on='merchant_abn', how='left')\n",
    "    Entropy_cust = Entropy_cust.withColumn('Probability', F.col('Count') / F.col('sum(Count)'))\n",
    "    Entropy_cust = Entropy_cust.groupBy(\"merchant_abn\").agg((-F.sum(F.col(\"Probability\") * F.log2(col(\"Probability\")))).alias(\"Customer_Entropy\"))\n",
    "    return Entropy_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_score(full_dataset, customers):\n",
    "    full_dataset = full_dataset.join(customers, on='user_id')\n",
    "    # add some of the attributes\n",
    "    cust_data = full_dataset.groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'), F.round(F.count('user_id') / F.countDistinct('user_id'), 2).alias('Transaction_per_User'), F.round(F.avg('Average taxable income or loss'),2).alias('customer_wealth'))\n",
    "    # add the loyalty score\n",
    "    cust_data = cust_data.join(loyalty(full_dataset), on='merchant_abn')\n",
    "    cust_data = cust_data.join(customer_entropy(full_dataset), on='merchant_abn', how='left')\n",
    "    cust_final = feature_standardisation(cust_data, ['Transaction_per_User', 'customer_wealth', 'Unique_Customers', 'Loyal_AVG'], ['Customer_Entropy'])\n",
    "    # save file \n",
    "    cust_final.write.parquet('../data/Normalised/customer_scaled1', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry/Environment \n",
    "We add a general score, based on their description tags, in relation to which industry they belong to, including:\n",
    "- Market Dominance\n",
    "- Survival Rate\n",
    "- Fraud_Prob_Avg\n",
    "- Customer_Base\n",
    "- Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_transformation(spark):\n",
    "    # Read lookup table \n",
    "    description_lookup = spark.read.csv('../data/tables/description_lookup.csv')\n",
    "    description_lookup = description_lookup.withColumnRenamed('_c0', 'Full_description')\n",
    "    description_lookup = description_lookup.withColumnRenamed('_c1', 'Description_Key')\n",
    "    # Next assign each to one of the 4 industries\n",
    "    #Administrative_Support_Services= [\"equipment tool furniture appliance rent al leasing\", \"florist supply nursery stock flower\", \"lawn garden supply outlet including nursery\"]\n",
    "    Administrative_Support_Services = [8, 10, 23]\n",
    "    #Personal_Services= [\"shoe shop\", \"gift card novelty souvenir shop\", \"antique shop sale repair restoration service\", \"watch clock jewellery repair shop\", \"jewellery watch clock silverware shop\",  \"motor vehicle supply new part\", \"furniture home furnishing equipment shop manufa...\", \"tent awning shop\", \"optician optical good eyeglass\"]\n",
    "    Personal_Services = [19, 5, 11, 3, 2, 12, 0, 15, 20]\n",
    "    #Arts_Recreation_Services = [\"digital good book movie music\", \"music shop musical instrument piano sheet music\", \"health beauty spa\", \"bicycle shop sale service\", \"art dealer gallery\", \"hobby toy game shop\", \"stationery office supply printing writing paper\"]\n",
    "    Arts_Recreation_Services = [18, 4, 21, 17, 16, 22, 14]\n",
    "    #Information_Media_Telecommunications = [\"telecom\", \"computer programming data processing integrated...\", \"book periodical newspaper\", \"artist supply craft shop\", \"computer computer peripheral equipment software\", \"cable satellite pay television radio service\"]\n",
    "    Information_Media_Telecommunications=[24, 6,7,9,1,13]\n",
    "    desc = []\n",
    "    tags = []\n",
    "    for ele in Administrative_Support_Services:\n",
    "        desc.append(\"Administrative_Support_Services\")\n",
    "    tags += Administrative_Support_Services\n",
    "    for ele in Personal_Services:\n",
    "        desc.append(\"Personal_Services\")\n",
    "    tags += Personal_Services\n",
    "    for ele in Arts_Recreation_Services:\n",
    "        desc.append(\"Arts_Recreation_Services\")\n",
    "    tags += Arts_Recreation_Services\n",
    "    for ele in Information_Media_Telecommunications:\n",
    "        desc.append(\"Information_Media_Telecommunications\")\n",
    "    tags += Information_Media_Telecommunications\n",
    "\n",
    "    to_df = {\n",
    "        \"sector\" :  desc,\n",
    "        \"Description\" : tags\n",
    "    }\n",
    "    # write dictionary\n",
    "    pd.DataFrame(to_df).to_csv('../data/curated/sector_lookup_table.csv')\n",
    "    sectors = spark.createDataFrame(pd.DataFrame(to_df))\n",
    "    dataset_sector = description_lookup.join(sectors, on= description_lookup.Description_Key == sectors.Description).drop('Description')\n",
    "    # save sectors\n",
    "    return dataset_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry(full_dataset, fraud_probabilities, spark, RECENCY):\n",
    "    industry_data = full_dataset.select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue', 'Description')\n",
    "    # Now join fraud prob \n",
    "    industry_data = industry_data.join(fraud_probabilities, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    industry_data = industry_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction'))\n",
    "                                                                 )\n",
    "    dataset_sector = description_transformation(spark)\n",
    "    industry_data = industry_data.join(dataset_sector, on=industry_data.Description == dataset_sector.Description_Key, how='left').drop('Full_Description', 'Description_Key')\n",
    "    # Define the toggle \n",
    "    #SECTOR_SIZE = 'Description'\n",
    "    # or \n",
    "    SECTOR_SIZE = 'sector'\n",
    "    Merchant_Industry = industry_data.select('merchant_abn', 'sector').distinct()\n",
    "    # Market Dominance\n",
    "    industry_table = industry_data.where(F.col('order_datetime') > RECENCY).groupBy('sector').agg(F.sum('BNPL_weighted_Revenue').alias('Total_Weighted_Revenue'), F.count('BNPL_weighted_Revenue').alias('Total_Transactions'), F.avg('BNPL_weighted_Revenue').alias('Average_Weighted_Revenue'))\n",
    "    total_rev = industry_table.groupBy().sum().collect()[0][0]\n",
    "    total_transa = industry_table.groupBy().sum().collect()[0][0]\n",
    "    industry_table = industry_table.withColumn('Total_Weighted_Revenue', F.col('Total_Weighted_Revenue') / total_rev).withColumnRenamed('Total_Weighted_Revenue', 'Portion_of_Total_Revenue')\n",
    "    industry_table = industry_table.withColumn('Total_Transactions', F.col('Total_Transactions') / total_transa).withColumnRenamed('Total_Transactions', 'Portion_of_Total_Transactions')\n",
    "    # Survival Rate\n",
    "    sector_info = spark.read.parquet('../data/tables/sector_information/')\n",
    "    industry_table = industry_table.join(sector_info.select(F.col('sector'), F.col('survival_rate').cast('double').alias(\"survival_rate\")), on='sector', how='left')\n",
    "    industry_table = feature_standardisation(industry_table, ['Portion_of_Total_Revenue', 'Portion_of_Total_Transactions', 'survival_rate', 'Average_Weighted_Revenue'])\n",
    "    # ensure assigned to each merchant\n",
    "    industry_table = Merchant_Industry.join(industry_table, on='sector', how='left')\n",
    "    # save file \n",
    "    industry_table.write.parquet('../data/Normalised/industry_scaled1', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is the main function, which can run everything\n",
    "def main():\n",
    "    spark = open_spark()\n",
    "    customers = spark.read.parquet('../data/processed/customers/')\n",
    "    transactions = spark.read.parquet('../data/processed/transactions')\n",
    "    merchants =  spark.read.parquet('../data/processed/merchants/')\n",
    "    final_data_collection = merchants.select('merchant_abn')\n",
    "    fraud_probabilities = spark.read.parquet('../models/random_forest_output_full/')\n",
    "    fraud_probabilities = fraud_probabilities.select('merchant_abn', 'user_id', 'order_datetime', 'prediction')\n",
    "\n",
    "    # APPLY PREVIOUSELY SHARED & UNIMPLEMENTED FUNCTIONS\n",
    "    merchants = merchant_process(merchants, spark)\n",
    "    full_dataset = transactions.join(merchants, on='merchant_abn', how='inner')\n",
    "    # calculate the BNPL unweighted revenue \n",
    "    full_dataset = full_dataset.withColumn('BNPL_Revenue', F.col('dollar_value') * F.col('BNPL_Fee'))\n",
    "    # First calculate the finantial score\n",
    "\n",
    "    # We will for now determine as last 6 months (i.e march) \n",
    "    RECENCY =  F.lit('2022-03-01')\n",
    "    # Now run each score, which will save results to file\n",
    "    # Note: customer score takes much linger, and sould probably run seperate\n",
    "    finantial_score(RECENCY, transactions, fraud_probabilities)\n",
    "    sustainability_score(merchants, full_dataset)\n",
    "    customer_score(full_dataset, customers)\n",
    "    industry(full_dataset, fraud_probabilities, spark, RECENCY)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
