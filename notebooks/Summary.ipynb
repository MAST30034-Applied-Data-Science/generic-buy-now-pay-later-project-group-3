{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 19:19:36 WARN Utils: Your hostname, Loky-PC resolves to a loopback address: 127.0.1.1; using 192.168.55.225 instead (on interface eth0)\n",
      "22/10/05 19:19:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 19:19:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Open Spark session \n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#read internal tables\n",
    "\n",
    "#dataframes: transaction_sample, merchants_tbl, customer_tbl  \n",
    "\n",
    "transactions_sample = spark.read.parquet('../data/tables/transactions_20210828_20220227_snapshot')\n",
    "transactions_sample2 = spark.read.parquet('../data/tables/transactions_20210228_20210827_snapshot')\n",
    "transactions_sample3 = spark.read.parquet('../data/tables/transactions_20220228_20220828_snapshot')\n",
    "transactions_sample.unionByName(transactions_sample2, True)\n",
    "transactions_sample.unionByName(transactions_sample3, True)\n",
    "consumer_details = spark.read.parquet('../data/tables/consumer_user_details.parquet')\n",
    "merchants_tbl = spark.read.parquet('../data/tables/tbl_merchants.parquet')\n",
    "customer_tbl = spark.read.option(\"delimiter\", \"|\").option(\"header\",True).csv('../data/tables/tbl_consumer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse \"tag\" of merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merchants_pd = merchants_tbl.toPandas()\n",
    "# this function standardises the tags attribute, creating a list with the 'description', 'revenue band' and 'BNPL service charge'\n",
    "def tag_extract(tag_string): \n",
    "    # first need to preprocess\n",
    "    string =  re.sub('\\[','(', tag_string.lower())\n",
    "    string = re.sub('\\]',')', string)\n",
    "    # break the string into sections\n",
    "    string_cut = string.split('),')\n",
    "    new_string = []\n",
    "    # first extract the description \n",
    "    new_string.append(str(string_cut[0].strip('((')))\n",
    "    # second extract the band\n",
    "    new_string.append(str(re.search(r'[a-z]',string_cut[1]).group()))\n",
    "    # finally the take rate\n",
    "    new_string.append(float(re.search(r'[0-9]+\\.[0-9]+',string_cut[2]).group()))\n",
    "    return(new_string)\n",
    "################\n",
    "# now we can run the algorithm\n",
    "tags = merchants_pd['tags']\n",
    "processed_tags = []\n",
    "for i in tags:\n",
    "    processed_tags.append(tag_extract(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_tag= pd.DataFrame(processed_tags, columns=('Description', 'Earnings_Class', 'BNPL_Fee'))\n",
    "merchants_pd = pd.concat([merchants_pd, merchant_tag], axis=1)\n",
    "# drop the tags column \n",
    "merchants_pd.drop(columns='tags', inplace=True)\n",
    "\n",
    "# and convert back to spark dataframe \n",
    "merchants_tbl = spark.createDataFrame(merchants_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all the internal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Earnings_Class: string (nullable = true)\n",
      " |-- BNPL_Fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First lets look at the number of registered customers and merchants without any data in the dataset\n",
    "customer_tbl = customer_tbl.join(consumer_details, ['consumer_id'])\n",
    "full_dataset = transactions_sample.join(customer_tbl, on = ['user_id'])\n",
    "merchants_tbl = merchants_tbl.withColumnRenamed('name','company_name')\n",
    "full_dataset = full_dataset.join(merchants_tbl, ['merchant_abn'])\n",
    "full_dataset.createOrReplaceTempView('full')\n",
    "full_dataset.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add a day (mon,...), weekly & monthly attribute\n",
    "import pyspark.sql.functions as F\n",
    "full_dataset = full_dataset.withColumn('Day', F.dayofweek('order_datetime'))\n",
    "full_dataset = full_dataset.withColumn('Month', F.month('order_datetime'))\n",
    "# now we can also add the bnpl revenue from a transaction \n",
    "full_dataset = full_dataset.withColumn('BNPL_Revenue', F.col('dollar_value') * 0.01 * F.col('BNPL_Fee'))\n",
    "full_dataset.createOrReplaceTempView('data')\n",
    "# we can remove name, location and customerID for now, due to being unnnesesary attributes (although company_name could also be removed)\n",
    "full_dataset = spark.sql(\"\"\"\n",
    "select merchant_abn, user_id, dollar_value, order_id, order_datetime, state, postcode, gender, company_name, \n",
    "        Description, Earnings_Class, BNPL_Fee, BNPL_Revenue, Day, Month, weekofyear(order_datetime) as weekofyear from data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined count: 4323692, raw count: 4508106, difference 184414 or 4.09%\n"
     ]
    }
   ],
   "source": [
    "# Compare difference in records between joined table and raw data\n",
    "full = full_dataset.count()\n",
    "raw = transactions_sample.count()\n",
    "print(f'Joined count: {full}, raw count: {raw}, difference {raw - full} or {round(100 * ((raw - full) / raw), 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of transactions without valid merchants abns in the merchants table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of transactions without valid merchants abns in the merchants table: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184414\n",
      "percentage of all transactions:\n",
      "0.04090720138346347\n"
     ]
    }
   ],
   "source": [
    "# First lets look at the number of transactions without valid merchants abns in the merchants table\n",
    "merchants_tbl.createOrReplaceTempView('merchants')\n",
    "customer_tbl.createOrReplaceTempView('consumer')\n",
    "transactions_sample.createOrReplaceTempView('trans')\n",
    "missing = spark.sql(\"\"\"\n",
    "select count(*) from trans\n",
    "where trans.merchant_abn not in (select merchant_abn from merchants)\n",
    "\"\"\")\n",
    "print(\"the number of transactions without valid merchants abns in the merchants table: \")\n",
    "print(missing.head()[0])\n",
    "\n",
    "print(\"percentage of all transactions:\")\n",
    "print(missing.head()[0]/transactions_sample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
