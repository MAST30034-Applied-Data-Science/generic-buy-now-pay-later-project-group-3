{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant Ranking Algorith\n",
    "The method of ranking each merchant, will take inspiration from the methods proposed in (https://sapinsider.org/leveraging-analytical-method-for-ranking-suppliers/), in which we rank each key attribute out of 5, then sum them together with weightsget a score for each merchant. \n",
    "\n",
    "The Key Attributes for now are: \n",
    "- Revenue\n",
    "- Customer_Base \n",
    "- Sustainability\n",
    "- Envirnment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/09 00:31:48 WARN Utils: Your hostname, James-N580VD-DM229T resolves to a loopback address: 127.0.1.1; using 172.30.158.247 instead (on interface eth0)\n",
      "22/10/09 00:31:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/09 00:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import feature as H\n",
    "# First lets reed the datasets\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the datasets \n",
    "transactions = spark.read.parquet('../data/processed/transactions')\n",
    "merchants =  spark.read.parquet('../data/processed/merchants/')\n",
    "full_dataset = spark.read.parquet('../data/curated/full_dataset/')\n",
    "final_data_collection = merchants.select('merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our scaliong function\n",
    "def feature_standardisation(colums, dataset): \n",
    "    for col_name in colums:\n",
    "        values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "        dataset = dataset.withColumn(col_name, F.round((F.col(col_name) - values.select('low').head()[0]) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concerns\n",
    "Just stuff i came across whilst implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Case: if we use a cut-off point for the fraud probability, and minus the dollar_value of 'fraud' transactions\n",
    "concern1 = full_dataset.where(F.col('merchant_abn') == 83412691377).select('Potential_Outlier', 'dollar_value').toPandas()\n",
    "import seaborn as sns\n",
    "sns.histplot(x='dollar_value', hue='Potential_Outlier', data=concern1)\n",
    "# From counting the total revenue for each merchant using such method, I found that some have negative ones (but just using outlier_attr).\n",
    "# However, due to most outluiers being larger than the mean, minusing it results in devestating loss to revenue, which is to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tansaction_Revenue\n",
    "For this we will look into:\n",
    "- Renevue (Recent)\n",
    "- Projected Revenue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For revenue, we first set the break point as what we determine as 'Recent'\n",
    "# We will for now determine as last 6 months (i.e march) \n",
    "RECENCY =  F.lit('2022-03-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset\n",
    "revenue_data = full_dataset.where(F.col('order_datetime') > RECENCY).select('merchant_abn', 'dollar_value', 'BNPL_Revenue', 'Potential_Outlier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper method (with the probabilities)\n",
    "# revenue_data = revenue_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * F.col('Fraud_probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Method\n",
    "revenue_data = revenue_data.withColumn('BNPL_weighted_Revenue', F.when(F.col('Potential_Outlier') == False, F.col('BNPL_Revenue'))\n",
    "                                                                 .otherwise(-1 * F.col('dollar_value'))\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can caluculate each Merchants revenue\n",
    "revenue_data = revenue_data.groupBy('merchant_abn').agg(F.round(F.sum('BNPL_weighted_Revenue'), 2).alias('Total_Revenue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardise\n",
    "revenue_data = feature_standardisation(['Total_Revenue'],revenue_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Projections (Need to add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability \n",
    "Next, we add a rating for a companies growth/longevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_sub = merchants.select(['merchant_abn', 'avg_monthly_inc', 'postcode_entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in merchants_sub.columns[1:]:\n",
    "    values = merchants_sub.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "    merchants_sub = merchants_sub.withColumn(col_name, (F.col(col_name) - values.select('low').head()[0]) / (values.select('high').head()[0] - values.select('low').head()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_sub = merchants_sub.withColumn('Sustainability_score', F.col('avg_monthly_inc') - F.col('postcode_entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_sub = merchants_sub.select(['merchant_abn', 'Sustainability_score'])\n",
    "final_data_collection = final_data_collection.join(merchants_sub, on=['merchant_abn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Base\n",
    "In this sections we use the features:\n",
    "- customer_loyalty_agg\n",
    "- unique_cust\n",
    "- Total_Customers*\n",
    "- Cust_tax (TBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create Loyalfy feature\n",
    "grouped = full_dataset.groupBy(\"user_id\", \"merchant_abn\")\n",
    "RPR = grouped.count().withColumnRenamed(\"count\", \"RPR\")\n",
    "upSell = RPR.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"upsell\")\n",
    "CLV = grouped.sum(\"dollar_value\").withColumnRenamed(\"sum(dollar_value)\", \"CLV\")\n",
    "# Define the window\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.partitionBy([\"user_id\", \"merchant_abn\"]).orderBy(\"order_datetime\")\n",
    "\n",
    "retention = full_dataset.withColumn(\n",
    "    'diff',\n",
    "    F.datediff(F.col(\"order_datetime\"), F.lag(\"order_datetime\").over(w))\n",
    ").groupBy(\"user_id\", \"merchant_abn\").agg(F.avg(F.col(\"diff\")).alias(\"retention\"))\n",
    "retention.agg({\"retention\":\"max\"}).collect()\n",
    "retention.na.fill(value=365)\n",
    "loyal = retention.na.fill(value=365).join(RPR, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(CLV, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(upSell, on=[\"user_id\"], how=\"left\")\n",
    "loyal = loyal.withColumn(\"loyal\", F.col(\"RPR\") * F.col(\"CLV\") * F.col(\"upSell\") / F.col(\"retention\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "loyal = loyal.select(*(col(c).cast(\"float\").alias(c) for c in loyal.columns))\n",
    "loyal = loyal.select('user_id', 'merchant_abn', 'loyal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_window = Window.partitionBy('grp')\n",
    "magic_percentile = F.expr('percentile_approx(loyal, 0.5)')\n",
    "loyal = loyal.join(loyal.groupBy('user_id').agg(magic_percentile.alias('med_val')), on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "loyal = loyal.withColumn('loyal', coalesce('loyal', 'med_val')).select(['user_id', 'merchant_abn', 'loyal'])\n",
    "loyal_agg = loyal.groupBy('merchant_abn').avg('loyal').alias('Loyal_AVG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add few other metrics (Need to still add Postcode Entropy...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_data =  full_dataset.groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'), F.round(F.count('user_id') / F.countDistinct('user_id'), 2).alias('Transaction_per_User'), F.round(F.avg('Average taxable income or loss'),2).alias('customer_wealth'))\n",
    "cust_data = cust_data.join(loyal_agg, on='merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cust_data = feature_standardisation(['Transaction_per_User', 'customer_wealth', 'Unique_Customers', 'Loyal_AVG'], cust_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
