{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run \"ETL\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Spark session \n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the files \n",
    "transactions_sample = spark.read.parquet('../data/tables/transactions_20210828_20220227_snapshot')\n",
    "transactions_sample2 = spark.read.parquet('../data/tables/transactions_20210228_20210827_snapshot')\n",
    "transactions_sample3 = spark.read.parquet('../data/tables/transactions_20220228_20220828_snapshot')\n",
    "transactions_sample.unionByName(transactions_sample2, True)\n",
    "transactions_sample.unionByName(transactions_sample3, True)\n",
    "consumer_details = spark.read.parquet('../data/tables/consumer_user_details.parquet')\n",
    "merchants_tbl = spark.read.parquet('../data/tables/tbl_merchants.parquet')\n",
    "customer_tbl = spark.read.option(\"delimiter\", \"|\").option(\"header\",True).csv('../data/tables/tbl_consumer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = merchants_tbl.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# this function standardises the tags attribute, creating a list with the 'description', 'revenue band' and 'BNPL service charge'\n",
    "def tag_extract(tag_string): \n",
    "    # first need to preprocess\n",
    "    string =  re.sub('\\[','(', tag_string.lower())\n",
    "    string = re.sub('\\]',')', string)\n",
    "    # break the string into sections\n",
    "    string_cut = string.split('),')\n",
    "    new_string = []\n",
    "    # first extract the description \n",
    "    new_string.append(str(string_cut[0].strip('((')))\n",
    "    # second extract the band\n",
    "    new_string.append(str(re.search(r'[a-z]',string_cut[1]).group()))\n",
    "    # finally the take rate\n",
    "    new_string.append(float(re.search(r'[0-9]+\\.[0-9]+',string_cut[2]).group()))\n",
    "    return(new_string)\n",
    "################\n",
    "# now we can run the algorithm\n",
    "tags = merchants['tags']\n",
    "processed_tags = []\n",
    "for i in tags:\n",
    "    processed_tags.append(tag_extract(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merchant_tbl = pd.DataFrame(processed_tags, columns=('Description', 'Earnings_Class', 'BNPL_Fee'))\n",
    "merchant_tbl = pd.concat([merchants, merchant_tbl], axis=1)\n",
    "# drop the tags column \n",
    "merchant_tbl.drop(columns='tags', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and convert back to spark dataframe \n",
    "merchants_tbl = spark.createDataFrame(merchant_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be further expanded in breaking the discription up further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation of Customers \n",
    "The objective of this section is to verify if a customer's details have been recorded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# dataset link \n",
    "link = 'https://www.matthewproctor.com/Content/postcodes/australian_postcodes.csv'\n",
    "# load data \n",
    "cust = customer_tbl.toPandas()\n",
    "postcodes = pd.read_csv(\"../data/tables/postcode_verification.csv\")\n",
    "postcodes['postcode'] = postcodes['postcode'].astype('str')\n",
    "keep_columns = ['postcode', 'state', 'sa3name', 'sa4name', 'SA3_NAME_2016', 'electoraterating', 'electorate']\n",
    "postcodes = postcodes[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First imputate missing values\n",
    "for col in postcodes.columns[1:]:\n",
    "    postcodes[col] = postcodes.groupby(\"state\")[col].transform(lambda x: x.fillna(x.mode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_agg = postcodes.groupby(['state', 'postcode'], as_index=False).agg(sa3name = pd.NamedAgg('sa3name',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 sa4name = pd.NamedAgg('sa4name',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 electoraterating = pd.NamedAgg('electoraterating',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 SA3_NAME_2016 = pd.NamedAgg('SA3_NAME_2016',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 electorate = pd.NamedAgg('electorate',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN)\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputate\n",
    "imputation = postcodes_agg.groupby('state', as_index=False).agg(sa3name_mode = pd.NamedAgg('sa3name',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 sa4name_mode = pd.NamedAgg('sa4name',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 electoraterating_mode = pd.NamedAgg('electoraterating',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 SA3_NAME_2016_mode = pd.NamedAgg('SA3_NAME_2016',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN),\n",
    "                                                 electorate_mode = pd.NamedAgg('electorate',lambda x: pd.Series.mode(x) if len(pd.Series.mode(x))>0 else np.NaN)\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_agg = postcodes_agg.merge(imputation, on='state', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_agg.sa3name.fillna(postcodes_agg.sa3name_mode, inplace=True)\n",
    "postcodes_agg.sa4name.fillna(postcodes_agg.sa4name_mode, inplace=True)\n",
    "postcodes_agg.electoraterating.fillna(postcodes_agg.electoraterating_mode, inplace=True)\n",
    "postcodes_agg.SA3_NAME_2016.fillna(postcodes_agg.SA3_NAME_2016_mode, inplace=True)\n",
    "postcodes_agg.electorate.fillna(postcodes_agg.electorate_mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_agg = postcodes_agg.drop(['sa3name_mode', 'sa4name_mode', 'electoraterating_mode', 'SA3_NAME_2016_mode', 'electorate_mode'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tax data addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_data = pd.read_csv(\"../data/tables/tax_income.csv\")\n",
    "tax_data['Postcode'] = tax_data['Postcode'].astype('str')\n",
    "# First remove duplicate columns \n",
    "tax_data = tax_data.T.drop_duplicates().T\n",
    "tax_columns = tax_data.columns[1:]\n",
    "# IMPUTATION TIME\n",
    "postcodes_agg = postcodes_agg.join(tax_data, lsuffix='postcode', rsuffix='Postcode', how='left')\n",
    "for col in tax_columns:\n",
    "    for agg_name in ['sa4name', 'electorate', 'electoraterating']:\n",
    "        postcodes_agg[col] = postcodes_agg.groupby(agg_name)[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    postcodes_agg[col] = postcodes_agg[col].apply(np.ceil).astype('int')\n",
    "postcodes_agg.set_index(['state', 'postcode'], inplace = True)\n",
    "cust.set_index(['state', 'postcode'], inplace= True)\n",
    "customer_tbl = cust.join(postcodes_agg, how='left')\n",
    "customer_tbl = customer_tbl.reset_index()\n",
    "customer_tbl.drop(columns='Postcode', inplace = True)\n",
    "# create schema\n",
    "from pyspark.sql.types import *\n",
    "from traitlets import Integer\n",
    "structure = []\n",
    "for att_name in customer_tbl.columns:\n",
    "        if att_name not in tax_columns:\n",
    "                structure.append(StructField(att_name, StringType(), True))\n",
    "        else: \n",
    "                structure.append(StructField(att_name, IntegerType(), True))\n",
    "schema = StructType(structure)\n",
    "# convert back to original form\n",
    "customer_tbl = spark.createDataFrame(customer_tbl, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_tbl = customer_tbl.join(consumer_details, ['consumer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = transactions_sample.join(customer_tbl, ['user_id'])\n",
    "merchants_tbl = merchants_tbl.withColumnRenamed('name','company_name')\n",
    "full_dataset = full_dataset.join(merchants_tbl, ['merchant_abn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add a day (mon,...), weekly & monthly attribute\n",
    "import pyspark.sql.functions as F\n",
    "full_dataset = full_dataset.withColumn('Day', F.dayofweek('order_datetime'))\n",
    "full_dataset = full_dataset.withColumn('Month', F.month('order_datetime'))\n",
    "# now we can also add the bnpl revenue from a transaction \n",
    "full_dataset = full_dataset.withColumn('BNPL_Revenue', F.col('dollar_value') * 0.01 * F.col('BNPL_Fee'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.createOrReplaceTempView('data')\n",
    "# we can remove name, location and customerID for now, due to being unnnesesary attributes (although company_name could also be removed)\n",
    "full_dataset = spark.sql(\"\"\"\n",
    "select merchant_abn, user_id, dollar_value, order_id, order_datetime, state, postcode, gender, company_name, \n",
    "        Description, Earnings_Class, BNPL_Fee, BNPL_Revenue, Day, Month, weekofyear(order_datetime) as weekofyear from data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can convert back to customer data anc continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation statistics/features for merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
