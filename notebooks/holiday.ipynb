{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holiday and Fraud analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType, FloatType\n",
    "from pyspark.sql import SparkSession, functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 01:03:59 WARN Utils: Your hostname, Loky-PC resolves to a loopback address: 127.0.1.1; using 192.168.55.225 instead (on interface eth0)\n",
      "22/10/06 01:03:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 01:04:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = ['2021-03-08','2021-04-02','2021-04-03','2021-04-04','2021-04-05','2021-04-25','2021-06-14','2021-09-24','2021-11-02','2021-12-25','2021-12-26','2021-12-27','2021-12-28',\n",
    "       '2022-01-01','2022-01-03','2022-01-26','2022-03-14','2022-04-15','2022-04-16','2022-04-17','2022-04-18','2022-04-25',\n",
    "       '2022-06-13','2022-09-22','2022-09-23']\n",
    "dates = pd.date_range(start='2021-02-01', end='2022-10-31')\n",
    "\n",
    "list1 = []\n",
    "for date in dates:\n",
    "    if date in data:\n",
    "        list1.append((date.strftime('%Y-%m-%d'), True))\n",
    "    else:\n",
    "        list1.append((date.strftime('%Y-%m-%d'), False))\n",
    "holiday_data = spark.createDataFrame(pd.DataFrame(list1, columns =['Date', 'holiday?']))\n",
    "holiday_data = holiday_data.withColumn(\"Date\",col(\"Date\").cast(DateType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>fraud_probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holiday?</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>12002.025951</td>\n",
       "      <td>14.903774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id  fraud_probability\n",
       "holiday?                                 \n",
       "False     12002.025951          14.903774"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_pd = holiday_data.toPandas()\n",
    "\n",
    "consumer_fraud = pd.read_csv(\"../data/tables/consumer_fraud_probability.csv\")\n",
    "consumer_fraud[\"date\"] = pd.to_datetime(consumer_fraud[\"order_datetime\"])\n",
    "consumer_fraud = consumer_fraud.drop(columns=\"order_datetime\")\n",
    "fraud_by_date = consumer_fraud.groupby(\"date\").mean(\"fraud_probablity\")\n",
    "holiday_pd[\"date\"] = pd.to_datetime(holiday_pd[\"Date\"])\n",
    "holiday_pd = holiday_pd.drop(columns=\"Date\")\n",
    "\n",
    "fraud_holiday = holiday_pd.join(fraud_by_date, on=\"date\")\n",
    "fraud_holiday.groupby(\"holiday?\").mean(\"fraud_probablity\").head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- holiday?: boolean (nullable = true)\n",
      "\n",
      "+----------+------------------+--------+\n",
      "|      Date|          avg(f_p)|holiday?|\n",
      "+----------+------------------+--------+\n",
      "|2021-02-28|15.891153547498915|   false|\n",
      "|2021-03-01|12.363714536031088|   false|\n",
      "|2021-03-02|16.292744318644207|   false|\n",
      "|2021-03-03|14.011275291442871|   false|\n",
      "|2021-03-04|15.537732404821059|   false|\n",
      "+----------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|holiday?|     avg(avg(f_p))|\n",
      "+--------+------------------+\n",
      "|   false|14.903774021734295|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holiday_data.printSchema()\n",
    "consumer_fraud = spark.read.option(\"header\", True).csv(\"../data/tables/consumer_fraud_probability.csv\")\n",
    "\n",
    "consumer_fraud = consumer_fraud.withColumn(\"Date\", col(\"order_datetime\").cast(DateType()))\n",
    "consumer_fraud = consumer_fraud.withColumn(\"f_p\", col(\"fraud_probability\").cast(FloatType()))\n",
    "\n",
    "consumer_fraud = consumer_fraud.groupBy(\"Date\").mean(\"f_p\")\n",
    "\n",
    "joined = consumer_fraud.join(holiday_data, on = \"Date\")\n",
    "joined.show(5)\n",
    "a = joined.groupBy(\"holiday?\").mean(\"avg(f_p)\")\n",
    "a.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
