{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, month, dayofmonth, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 17:39:22 WARN Utils: Your hostname, J-L resolves to a loopback address: 127.0.1.1; using 172.18.71.108 instead (on interface eth0)\n",
      "22/10/05 17:39:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 17:39:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.71.108:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Checker</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f26c54810d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SparkSession.builder.appName(\"Entropy\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------------+-----------+\n",
      "|user_id|merchant_abn|dollar_value|order_datetime|   order_id|\n",
      "+-------+------------+------------+--------------+-----------+\n",
      "|  14935| 79417999332|      136.07|    2021-11-26|68719476736|\n",
      "|      1| 46451548968|       72.62|    2021-11-26|68719476737|\n",
      "|  14936| 89518629617|        3.08|    2021-11-26|68719476738|\n",
      "+-------+------------+------------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = sp.read.option(\"inferSchema\", True).parquet(\"../data/curated/transactions\")\n",
    "trans.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "def compute_postcode_entropy(transactions: DataFrame, customers: DataFrame):\n",
    "    '''\n",
    "        function to compute entropy for each merchant based on different postcode of the customers of each \n",
    "        transaction.\n",
    "    '''\n",
    "    trans_with_postcode = transactions.join(customers.select([\"user_id\", \"postcode\"]), on=\"user_id\")\n",
    "    by_postcode = trans_with_postcode.groupBy(\"merchant_abn\", \"postcode\").count()\n",
    "    by_postcode = by_postcode.toPandas()\n",
    "    merchants_list = by_postcode[\"merchant_abn\"].unique().tolist()\n",
    "    \n",
    "\n",
    "    entropies = {}\n",
    "    for abn in  merchants_list:\n",
    "        this_merchant = by_postcode.loc[by_postcode['merchant_abn'] == abn]\n",
    "        num_transc = this_merchant[\"count\"]\n",
    "        entropies[abn] = entropy(num_transc)\n",
    "    return entropies\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_monthly_entropy(transactions: DataFrame):\n",
    "    '''\n",
    "    Compute entropy for each merchant, base on number of transactions each month\n",
    "    '''\n",
    "    monthly_trans = transactions.withColumn(\"order_month\", \n",
    "                                date_format('order_datetime','yyyy-MM'))\n",
    "    monthly = monthly_trans.groupBy(\"merchant_abn\", \"order_month\").count()\n",
    "\n",
    "    monthly = monthly.toPandas()\n",
    "    a = monthly[\"merchant_abn\"].unique().tolist()\n",
    "    #print(a)\n",
    "\n",
    "    entropies = {}\n",
    "    for abn in a:\n",
    "\n",
    "        this_merchant = monthly.loc[monthly['merchant_abn'] == abn]\n",
    "        by_month = this_merchant[\"count\"]\n",
    "        entropies[abn] = entropy(by_month)\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n",
      "+-----------+-----------------+--------------------+-----+--------+------+-------+\n",
      "|consumer_id|             name|             address|state|postcode|gender|user_id|\n",
      "+-----------+-----------------+--------------------+-----+--------+------+-------+\n",
      "|     870353|    Charles Davis|     048 Ward Common|   SA|    5261|  Male| 213579|\n",
      "|     923963|Jacqueline Nelson|151 Lynn Gateway ...|  QLD|    4744|Female| 213580|\n",
      "+-----------+-----------------+--------------------+-----+--------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custs = sp.read.option(\"inferSchema\", True).parquet(\"../data/curated/consumer_details/\")\n",
    "custs.printSchema()\n",
    "custs.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/05 17:52:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "postcode_entropy = compute_postcode_entropy(trans,custs)\n",
    "m_entropy = compute_monthly_entropy(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_entropy_pd = pd.DataFrame({\"abn\" : m_entropy.keys(), \"monthly entropy\" : m_entropy.values()})\n",
    "p_entropy_pd = pd.DataFrame({\"abn\": postcode_entropy.keys(), \"postcode entropy\": postcode_entropy.values()})\n",
    "\n",
    "final_entropy = pd.merge(m_entropy_pd, p_entropy_pd, on=\"abn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abn                 False\n",
       "monthly entropy     False\n",
       "postcode entropy    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_entropy.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_entropy.columns = [\"merchant_abn\", \"monthly_entropy\", \"postcode_entropy\"]\n",
    "final_entropy_sp = sp.createDataFrame(final_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------+-----------------+\n",
      "|merchant_abn|   monthly_entropy| postcode_entropy|          revenue|\n",
      "+------------+------------------+-----------------+-----------------+\n",
      "| 83412691377|2.9862164292283113|7.875183515435049|498536.9797888398|\n",
      "| 38700038932|2.9892000618696457|7.755078148797462|9546185.241102219|\n",
      "+------------+------------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merchant_data = final_entropy_sp.join(trans.groupBy(\"merchant_abn\").sum(\"dollar_value\"), on=\"merchant_abn\").withColumnRenamed(\"sum(dollar_value)\", \"revenue\")\n",
    "merchant_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------------+-----------------+\n",
      "|merchant_abn|monthly_entropy|postcode_entropy|          revenue|\n",
      "+------------+---------------+----------------+-----------------+\n",
      "| 83412691377|      2.9862165|       7.8751836|498536.9797888398|\n",
      "| 38700038932|         2.9892|       7.7550783|9546185.241102219|\n",
      "+------------+---------------+----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "merchant_data = merchant_data.withColumn(\"monthly_entropy\", col(\"monthly_entropy\").cast(FloatType()))\n",
    "merchant_data = merchant_data.withColumn(\"postcode_entropy\", col(\"postcode_entropy\").cast(FloatType()))\n",
    "\n",
    "merchant_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merchant_data.write.parquet(\"../data/tables/entropy\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
