{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:22:10 WARN Utils: Your hostname, J-L resolves to a loopback address: 127.0.1.1; using 172.28.113.244 instead (on interface eth0)\n",
      "22/10/08 12:22:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:22:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.28.113.244:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdbaca556d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = (\n",
    "    SparkSession.builder.appName(\"Model\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"+11\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .config('spark.sql.parquet.cacheMetadata', 'True')\n",
    "    .getOrCreate()\n",
    ")\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/transactions\")\n",
    "merchants = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/merchants\")\n",
    "customers = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|order_id|user_id|merchant_abn|dollar_value|order_datetime|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|       3|      3| 60956456424|      136.68|    2021-08-20|          0|                0|      0|        20|    8|        6|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "|merchant_abn|         name|Earnings_Class|BNPL_Fee|tags|avg_monthly_inc|monthly_entropy|postcode_entropy|          revenue|\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "| 10023283211|Felis Limited|             e|    0.18|   0|     -0.0952381|      2.9858725|        7.439226|703277.8708539009|\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "22/10/08 12:12:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(state='ACT', postcode=200, gender='Female', user_id=71674, Number of individuals lodging an income tax return=5524, Average taxable income or loss=66722, Median taxable income or loss=52958, Proportion with salary or wages=1, Count salary or wages=5009, Average salary or wages=64930, Median salary or wages=55579, Proportion with net rent=1, Count net rent=762, Average net rent=-4289, Median net rent=-2448, Average total income or loss=68991, Median total income or loss=54988, Average total deductions=2244, Median total deductions=872, Proportion with total business income=1, Count total business income=382, Average total business income=56170, Median total business income=18742, Proportion with total business expenses=1, Count total business expenses=343, Average total business expenses=42645, Median total business expenses=8664, Proportion with net tax=1, Count net tax=4586, Average net tax=18805, Median net tax=11482, Count super total accounts balance=7620, Average super total accounts balance=79163, Median super total accounts balance=10415)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.show(1)\n",
    "merchants.show(1)\n",
    "customers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING CUSTOMER FRAUD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+\n",
      "|user_id|order_datetime|fraud_probability|\n",
      "+-------+--------------+-----------------+\n",
      "|   6228|    2021-12-19|         97.62981|\n",
      "|  21419|    2021-12-10|         99.24738|\n",
      "+-------+--------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud = sp.read.option(\"inferSchema\", True).parquet(\"../data/curated/customer_fraud\")\n",
    "c_fraud = c_fraud.withColumn(\"order_datetime\", col(\"order_datetime\").cast(DateType()))\n",
    "c_fraud.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|user_id|order_datetime|order_id|merchant_abn|dollar_value|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|fraud_probability|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|    448|    2021-08-20|    1005| 94380689142|     6263.03|          0|                0|      0|        20|    8|        6|        14.681704|\n",
      "|   3116|    2021-08-20|    6989| 22248828825|     3958.86|          0|                0|      0|        20|    8|        6|         8.809071|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud_full = transactions.join(c_fraud, on=[\"user_id\", \"order_datetime\"])\n",
    "c_fraud_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80560"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_fraud_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=448, merchant_abn=94380689142, order_datetime=datetime.date(2021, 8, 20), order_id=1005, dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, holiday=0, dayofmonth=20, month=8, dayofweek=6, fraud_probability=14.681703567504883, name='Aliquet Ltd', Earnings_Class='b', BNPL_Fee=3.77, tags=12, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, state='WA', postcode=6170, gender='Female', Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = c_fraud_full.join(merchants, on=\"merchant_abn\").join(customers, on=\"user_id\")\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- fraud_probability: float (nullable = true)\n",
      " |-- Earnings_Class: string (nullable = true)\n",
      " |-- BNPL_Fee: double (nullable = true)\n",
      " |-- tags: integer (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Number of individuals lodging an income tax return: long (nullable = true)\n",
      " |-- Average taxable income or loss: long (nullable = true)\n",
      " |-- Median taxable income or loss: long (nullable = true)\n",
      " |-- Proportion with salary or wages: long (nullable = true)\n",
      " |-- Count salary or wages: long (nullable = true)\n",
      " |-- Average salary or wages: long (nullable = true)\n",
      " |-- Median salary or wages: long (nullable = true)\n",
      " |-- Proportion with net rent: long (nullable = true)\n",
      " |-- Count net rent: long (nullable = true)\n",
      " |-- Average net rent: long (nullable = true)\n",
      " |-- Median net rent: long (nullable = true)\n",
      " |-- Average total income or loss: long (nullable = true)\n",
      " |-- Median total income or loss: long (nullable = true)\n",
      " |-- Average total deductions: long (nullable = true)\n",
      " |-- Median total deductions: long (nullable = true)\n",
      " |-- Proportion with total business income: long (nullable = true)\n",
      " |-- Count total business income: long (nullable = true)\n",
      " |-- Average total business income: long (nullable = true)\n",
      " |-- Median total business income: long (nullable = true)\n",
      " |-- Proportion with total business expenses: long (nullable = true)\n",
      " |-- Count total business expenses: long (nullable = true)\n",
      " |-- Average total business expenses: long (nullable = true)\n",
      " |-- Median total business expenses: long (nullable = true)\n",
      " |-- Proportion with net tax: long (nullable = true)\n",
      " |-- Count net tax: long (nullable = true)\n",
      " |-- Average net tax: long (nullable = true)\n",
      " |-- Median net tax: long (nullable = true)\n",
      " |-- Count super total accounts balance: long (nullable = true)\n",
      " |-- Average super total accounts balance: long (nullable = true)\n",
      " |-- Median super total accounts balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(\"user_id\", \"merchant_abn\", \"order_datetime\", \"order_id\", \"name\", \"postcode\", \"holiday\")\n",
    "X.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical\n",
    "- holiday (done)\n",
    "- dayofmonth ?\n",
    "- dayofweek\n",
    "- month (done)\n",
    "- tags\n",
    "- state\n",
    "- gender\n",
    "- postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_processing(data: DataFrame, outcome: str):\n",
    "    categories = [\n",
    "        \"dayofmonth\",\n",
    "        \"dayofweek\",\n",
    "        \"month\",\n",
    "        \"tags\",\n",
    "        \"state\",\n",
    "        \"gender\",\n",
    "        \"Earnings_Class\"\n",
    "    ]\n",
    "\n",
    "    # Pipeline\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categories]\n",
    "    encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_encoded\") for c in categories]\n",
    "    transformed = Pipeline(stages=indexers + encoders).fit(data).transform(data)\n",
    "\n",
    "    for c in categories:\n",
    "        transformed = transformed.drop(c).drop(c+\"_index\")\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, fraud_probability=14.681703567504883, BNPL_Fee=3.77, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {5: 1.0}), state_encoded=SparseVector(7, {2: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_processed = category_processing(X, \"outcome\")\n",
    "category_processed.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, BNPL_Fee=3.77, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {5: 1.0}), state_encoded=SparseVector(7, {2: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), fraud_buckets=1.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "buckets = Bucketizer(splits=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], inputCol=\"fraud_probability\", outputCol=\"fraud_buckets\")\n",
    "X_bucks = buckets.transform(category_processed).drop(\"fraud_probability\")\n",
    "\n",
    "X_bucks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:============>                                             (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 1934|\n",
      "|          4.0|  990|\n",
      "|          5.0|  576|\n",
      "|          6.0|  360|\n",
      "|          7.0|  193|\n",
      "|          8.0|  102|\n",
      "|          9.0|   11|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_bucks.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95171"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fractions = [0, 0, 0, 2, 4, 7, 15, 20, 35, 250]\n",
    "\n",
    "X_adjusted = reduce(\n",
    "    DataFrame.unionAll,\n",
    "    [X_bucks.filter(X_bucks.fraud_buckets == float(x)).sample(withReplacement=True, fraction=float(fractions[x]), seed=69) for x in range(3, 10)]\n",
    ")\n",
    "X_adjusted = reduce(\n",
    "    DataFrame.unionAll,\n",
    "    [X_adjusted] + [X_bucks.filter(X_bucks.fraud_buckets == float(x)) for x in range(0, 3)]\n",
    ")\n",
    "\n",
    "X_adjusted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:==================================================>     (81 + 8) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 3910|\n",
      "|          4.0| 3931|\n",
      "|          5.0| 4025|\n",
      "|          6.0| 5450|\n",
      "|          7.0| 3876|\n",
      "|          8.0| 3583|\n",
      "|          9.0| 2749|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_adjusted.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:16:11 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66654\n",
      "22/10/08 12:16:58 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18834\n",
      "22/10/08 12:17:35 WARN DAGScheduler: Broadcasting large task binary with size 1318.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9683"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = X_adjusted.randomSplit([0.7, 0.2, 0.1], seed=69)\n",
    "\n",
    "print(train.count())\n",
    "print(val.count())\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numerical(data: DataFrame):\n",
    "    \"\"\"\n",
    "    Function to scale and process numerical columns\n",
    "    \"\"\"\n",
    "    # Scaler\n",
    "    columns = ['dollar_value', 'avg_monthly_inc', 'BNPL_Fee',\n",
    "    'monthly_entropy', 'postcode_entropy', 'revenue', 'Number of individuals lodging an income tax return', \n",
    "    'Average taxable income or loss', 'Median taxable income or loss', 'Proportion with salary or wages', 'Count salary or wages', \n",
    "    'Average salary or wages', 'Median salary or wages', 'Proportion with net rent', 'Count net rent', 'Average net rent', \n",
    "    'Median net rent', 'Average total income or loss', 'Median total income or loss', 'Average total deductions', \n",
    "    'Median total deductions', 'Proportion with total business income', 'Count total business income', \n",
    "    'Average total business income', 'Median total business income', 'Proportion with total business expenses', \n",
    "    'Count total business expenses', 'Average total business expenses', 'Median total business expenses', \n",
    "    'Proportion with net tax', 'Count net tax', 'Average net tax', 'Median net tax', 'Count super total accounts balance', \n",
    "    'Average super total accounts balance', 'Median super total accounts balance']\n",
    "\n",
    "    va = VectorAssembler(inputCols=columns, outputCol=\"to_scale\")\n",
    "    sc = StandardScaler(inputCol=\"to_scale\", outputCol=\"scaled\")\n",
    "\n",
    "    va_data = va.transform(data)\n",
    "    data = sc.fit(va_data).transform(va_data)\n",
    "    \n",
    "    # Drop other columns\n",
    "    for c in columns:\n",
    "        data = data.drop(c)\n",
    "    return data.drop(\"to_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:18:31 WARN DAGScheduler: Broadcasting large task binary with size 1365.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:19:19 WARN DAGScheduler: Broadcasting large task binary with size 1365.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:19:36 WARN DAGScheduler: Broadcasting large task binary with size 1365.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 12:19:54 WARN DAGScheduler: Broadcasting large task binary with size 1428.4 KiB\n",
      "22/10/08 12:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1428.4 KiB\n",
      "22/10/08 12:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1428.4 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Natural_var=0, Potential_Outlier=0, dayofmonth_encoded=SparseVector(30, {13: 1.0}), dayofweek_encoded=SparseVector(6, {5: 1.0}), month_encoded=SparseVector(11, {0: 1.0}), tags_encoded=SparseVector(24, {11: 1.0}), state_encoded=SparseVector(7, {2: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), fraud_buckets=3.0, scaled=DenseVector([0.0006, -1.4757, 2.6213, 11.8446, 5.6498, 2.6769, 0.4524, 3.4492, 5.3173, 0.0, 0.4319, 5.017, 5.9294, 44.0159, 0.3734, -0.0793, -0.2302, 3.3795, 5.4852, 1.2359, 2.6072, 37.2041, 0.4346, 1.7786, 2.4723, 31.132, 0.4411, 1.2678, 1.3093, 0.0, 0.446, 2.0525, 3.3059, 0.4536, 2.5189, 3.4209]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed = process_numerical(train)\n",
    "val_processed = process_numerical(val)\n",
    "test_processed = process_numerical(test)\n",
    "\n",
    "train_processed.head(1)\n",
    "val_processed.head(1)\n",
    "test_processed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_processed = sp.read.option(\"inferSchema\", True).parquet(\"../models/train_processed\")\n",
    "val_processed = sp.read.option(\"inferSchema\", True).parquet(\"../models/val_processed\")\n",
    "test_processed = sp.read.option(\"inferSchema\", True).parquet(\"../models/test_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data: DataFrame, outcome: str):\n",
    "    \"\"\"\n",
    "    Function to vectorize all the processed data\n",
    "    \"\"\"\n",
    "    data = data.withColumnRenamed(outcome, \"label\")\n",
    "    return VectorAssembler(\n",
    "        inputCols= [c for c in data.drop(\"label\").columns],\n",
    "        outputCol=\"features\"\n",
    "    ).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Natural_var=0, Potential_Outlier=0, dayofmonth_encoded=SparseVector(30, {6: 1.0}), dayofweek_encoded=SparseVector(6, {5: 1.0}), month_encoded=SparseVector(11, {5: 1.0}), tags_encoded=SparseVector(24, {17: 1.0}), state_encoded=SparseVector(7, {0: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {2: 1.0}), label=1.0, scaled=DenseVector([0.0001, 0.01, 1.1552, 11.8533, 5.5901, 0.3913, 1.0881, 4.0182, 6.3873, 0.0, 1.0838, 6.1235, 7.2708, 44.0159, 1.1971, 0.061, -0.0291, 3.9179, 6.5207, 1.2957, 3.8719, 37.2041, 0.9587, 1.9516, 1.9602, 31.132, 0.9613, 1.4767, 0.9168, 0.0, 1.0796, 2.6103, 4.6455, 1.1213, 2.7123, 4.0691]), features=SparseVector(122, {8: 1.0, 37: 1.0, 43: 1.0, 66: 1.0, 73: 1.0, 81: 1.0, 84: 1.0, 86: 0.0001, 87: 0.01, 88: 1.1552, 89: 11.8533, 90: 5.5901, 91: 0.3913, 92: 1.0881, 93: 4.0182, 94: 6.3873, 96: 1.0838, 97: 6.1235, 98: 7.2708, 99: 44.0159, 100: 1.1971, 101: 0.061, 102: -0.0291, 103: 3.9179, 104: 6.5207, 105: 1.2957, 106: 3.8719, 107: 37.2041, 108: 0.9587, 109: 1.9516, 110: 1.9602, 111: 31.132, 112: 0.9613, 113: 1.4767, 114: 0.9168, 116: 1.0796, 117: 2.6103, 118: 4.6455, 119: 1.1213, 120: 2.7123, 121: 4.0691}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector = vectorize(train_processed, \"fraud_buckets\")\n",
    "val_vector = vectorize(val_processed, \"fraud_buckets\")\n",
    "test_vector = vectorize(test_processed, \"fraud_buckets\")\n",
    "\n",
    "train_vector.head(1)\n",
    "val_vector.head(1)\n",
    "test_vector.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_vector.select(\"features\", \"label\").write.parquet(\"../models/train_vector\", mode=\"overwrite\")\n",
    "val_vector.select(\"features\", \"label\").write.parquet(\"../models/val_vector\", mode=\"overwrite\")\n",
    "test_vector.select(\"features\", \"label\").write.parquet(\"../models/test_vector\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/train_vector/\")\n",
    "val_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/val_vector/\")\n",
    "test_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/test_vector/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCount = 122                            # Seen from sparse vector column\n",
    "layers = [122, 256, 64, 10]\n",
    "model = MultilayerPerceptronClassifier(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    solver='gd',\n",
    "    maxIter=200,\n",
    "    layers=layers,\n",
    "    blockSize=64,\n",
    "    seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_vector.select(\"features\", \"label\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = model_fit.transform(train_vector)\n",
    "val_output = model_fit.transform(val_vector.dropna())\n",
    "test_output = model_fit.transform(test_vector.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 953:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.4044281618349793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "metrics = [\"accuracy\"]\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(\n",
    "        val_output.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.3971909532169782\n",
      "Train weightedFalsePositiveRate = 0.3971909532169782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "metrics = [\"accuracy\", \"weightedFalsePositiveRate\"]\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(\n",
    "        test_output.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|       rawPrediction|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  1.0|[1.21844183743563...|       1.0|\n",
      "|  1.0|[1.27374537812617...|       1.0|\n",
      "|  1.0|[1.25855328047493...|       1.0|\n",
      "|  1.0|[1.26524142233720...|       1.0|\n",
      "|  1.0|[1.25961303094817...|       1.0|\n",
      "|  1.0|[1.25153014281734...|       1.0|\n",
      "|  1.0|[1.26243904864357...|       1.0|\n",
      "|  1.0|[1.26507362918823...|       1.0|\n",
      "|  1.0|[1.26690124398601...|       1.0|\n",
      "|  1.0|[1.26907109442438...|       1.0|\n",
      "|  1.0|[1.26396004739482...|       1.0|\n",
      "|  1.0|[1.24820426333042...|       1.0|\n",
      "|  1.0|[1.27094470413560...|       1.0|\n",
      "|  1.0|[1.26930180670012...|       1.0|\n",
      "|  1.0|[1.26141111182877...|       1.0|\n",
      "|  1.0|[1.25830855450558...|       1.0|\n",
      "|  1.0|[1.24511597187381...|       1.0|\n",
      "|  1.0|[1.26306202361971...|       1.0|\n",
      "|  1.0|[1.27913547137144...|       1.0|\n",
      "|  1.0|[1.26010923848763...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_output.select(\"label\", \"rawPrediction\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def upper_prediction(pred):\n",
    "    if pred < 8:\n",
    "        return pred + 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---+\n",
      "|label|prediction|MSE|\n",
      "+-----+----------+---+\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "+-----+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_square_error_val_score = val_output.select(\"label\", \"prediction\").withColumn(\"MSE\" , (col(\"label\") - col(\"prediction\")))\n",
    "mean_square_error_val_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 983:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| MSE|count|\n",
      "+----+-----+\n",
      "| 0.0| 7617|\n",
      "|-1.0| 4518|\n",
      "| 1.0| 1176|\n",
      "| 3.0|  793|\n",
      "| 2.0|  791|\n",
      "| 4.0|  851|\n",
      "| 5.0| 1104|\n",
      "| 7.0|  705|\n",
      "| 6.0|  742|\n",
      "| 8.0|  537|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mean_square_error_val_score.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---+\n",
      "|label|prediction|MSE|\n",
      "+-----+----------+---+\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "+-----+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_square_error_test_score = test_output.select(\"label\", \"prediction\").withColumn(\"MSE\" , ((col(\"label\") - col(\"prediction\")) ** 2) ** 0.5)\n",
    "mean_square_error_test_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|MSE|count|\n",
      "+---+-----+\n",
      "|0.0| 3846|\n",
      "|1.0| 3013|\n",
      "|4.0|  418|\n",
      "|3.0|  448|\n",
      "|2.0|  369|\n",
      "|5.0|  563|\n",
      "|7.0|  389|\n",
      "|6.0|  391|\n",
      "|8.0|  246|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mean_square_error_test_score.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('virtual-p2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf6849838b3a8621666e21fdc4cc1583090fffb5f1906a909fbc1c95ae1bb65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
