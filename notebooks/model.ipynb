{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sp \u001b[39m=\u001b[39m (\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mModel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.session.timeZone\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m+11\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m6g\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m8g\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/sql/session.py:275\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m--> 275\u001b[0m         \u001b[39mgetattr\u001b[39;49m(session\u001b[39m.\u001b[39;49m_jvm, \u001b[39m\"\u001b[39;49m\u001b[39mSparkSession$\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1707\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1709\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1710\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1711\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1712\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1713\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1714\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "sp = (\n",
    "    SparkSession.builder.appName(\"Model\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"+11\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/transactions\")\n",
    "merchants = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/merchants\")\n",
    "customers = sp.read.option(\"inferSchema\", True).parquet(\"../data/processed/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|order_id|user_id|merchant_abn|dollar_value|order_datetime|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|       3|      3| 60956456424|      136.68|    2021-08-20|          0|                0|      0|        20|    8|        6|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+-----------------+--------------------+---------------+---------------+----------------+-----------------+\n",
      "|merchant_abn|             name|                tags|avg_monthly_inc|monthly_entropy|postcode_entropy|          revenue|\n",
      "+------------+-----------------+--------------------+---------------+---------------+----------------+-----------------+\n",
      "| 31823794994|Dolor Dapibus LLC|((hobby, Toy and ...|            0.0|      2.9600074|        6.064746|983068.1492156982|\n",
      "+------------+-----------------+--------------------+---------------+---------------+----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "22/10/05 21:03:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+-----+--------+------+-------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|state|postcode|gender|user_id|Number of individuals lodging an income tax return|Average taxable income or loss|Median taxable income or loss|Proportion with salary or wages|Count salary or wages|Average salary or wages|Median salary or wages|Proportion with net rent|Count net rent|Average net rent|Median net rent|Average total income or loss|Median total income or loss|Average total deductions|Median total deductions|Proportion with total business income|Count total business income|Average total business income|Median total business income|Proportion with total business expenses|Count total business expenses|Average total business expenses|Median total business expenses|Proportion with net tax|Count net tax|Average net tax|Median net tax|Count super total accounts balance|Average super total accounts balance|Median super total accounts balance|\n",
      "+-----+--------+------+-------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|  ACT|     200|Female|  71674|                                              5524|                         66722|                        52958|                              1|                 5009|                  64930|                 55579|                       1|           762|           -4289|          -2448|                       68991|                      54988|                    2244|                    872|                                    1|                        382|                        56170|                       18742|                                      1|                          343|                          42645|                          8664|                      1|         4586|          18805|         11482|                              7620|                               79163|                              10415|\n",
      "+-----+--------+------+-------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show(1)\n",
    "merchants.show(1)\n",
    "customers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+--------------------+--------------------+---------------+---------------+----------------+------------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|user_id|merchant_abn|order_id|dollar_value|order_datetime|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|                name|                tags|avg_monthly_inc|monthly_entropy|postcode_entropy|           revenue|state|postcode|gender|Number of individuals lodging an income tax return|Average taxable income or loss|Median taxable income or loss|Proportion with salary or wages|Count salary or wages|Average salary or wages|Median salary or wages|Proportion with net rent|Count net rent|Average net rent|Median net rent|Average total income or loss|Median total income or loss|Average total deductions|Median total deductions|Proportion with total business income|Count total business income|Average total business income|Median total business income|Proportion with total business expenses|Count total business expenses|Average total business expenses|Median total business expenses|Proportion with net tax|Count net tax|Average net tax|Median net tax|Count super total accounts balance|Average super total accounts balance|Median super total accounts balance|\n",
      "+-------+------------+--------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+--------------------+--------------------+---------------+---------------+----------------+------------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|      3| 60956456424|       3|      136.68|    2021-08-20|          0|                0|      0|        20|    8|        6|Ultricies Digniss...|([gift, card, Nov...|     -4.2380953|      2.9853823|        7.979236| 8026969.561502457|   NT|     862|Female|                                              1099|                         56030|                        45125|                              1|                  821|                  56184|                 49641|                       1|           155|             950|            818|                       58776|                      47279|                    2575|                    695|                                    1|                        144|                        64171|                       25168|                                      1|                          141|                          47773|                         14002|                      1|          795|          16719|         11089|                              1227|                              187532|                              80365|\n",
      "|  18482| 70501974849|       8|       68.75|    2021-08-20|          0|                0|      0|        20|    8|        6|Facilisis Lorem T...|((computers, comp...|    -0.85714287|      2.9847739|       7.8787856|1138649.2600500286|  NSW|    1430|  Male|                                              3241|                         62970|                        48875|                              1|                 2631|                  62722|                 53439|                       1|           468|            1250|            331|                       65378|                      51009|                    2396|                    715|                                    1|                        273|                       104471|                       30000|                                      1|                          251|                          75293|                         16014|                      1|         2462|          19177|         11585|                              3696|                              159242|                              66588|\n",
      "+-------+------------+--------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+--------------------+--------------------+---------------+---------------+----------------+------------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = transactions.join(merchants, on=\"merchant_abn\").join(customers, on=\"user_id\")\n",
    "final.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13614648"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- holiday: long (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Number of individuals lodging an income tax return: long (nullable = true)\n",
      " |-- Average taxable income or loss: long (nullable = true)\n",
      " |-- Median taxable income or loss: long (nullable = true)\n",
      " |-- Proportion with salary or wages: long (nullable = true)\n",
      " |-- Count salary or wages: long (nullable = true)\n",
      " |-- Average salary or wages: long (nullable = true)\n",
      " |-- Median salary or wages: long (nullable = true)\n",
      " |-- Proportion with net rent: long (nullable = true)\n",
      " |-- Count net rent: long (nullable = true)\n",
      " |-- Average net rent: long (nullable = true)\n",
      " |-- Median net rent: long (nullable = true)\n",
      " |-- Average total income or loss: long (nullable = true)\n",
      " |-- Median total income or loss: long (nullable = true)\n",
      " |-- Average total deductions: long (nullable = true)\n",
      " |-- Median total deductions: long (nullable = true)\n",
      " |-- Proportion with total business income: long (nullable = true)\n",
      " |-- Count total business income: long (nullable = true)\n",
      " |-- Average total business income: long (nullable = true)\n",
      " |-- Median total business income: long (nullable = true)\n",
      " |-- Proportion with total business expenses: long (nullable = true)\n",
      " |-- Count total business expenses: long (nullable = true)\n",
      " |-- Average total business expenses: long (nullable = true)\n",
      " |-- Median total business expenses: long (nullable = true)\n",
      " |-- Proportion with net tax: long (nullable = true)\n",
      " |-- Count net tax: long (nullable = true)\n",
      " |-- Average net tax: long (nullable = true)\n",
      " |-- Median net tax: long (nullable = true)\n",
      " |-- Count super total accounts balance: long (nullable = true)\n",
      " |-- Average super total accounts balance: long (nullable = true)\n",
      " |-- Median super total accounts balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- holiday: long (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Number of individuals lodging an income tax return: long (nullable = true)\n",
      " |-- Average taxable income or loss: long (nullable = true)\n",
      " |-- Median taxable income or loss: long (nullable = true)\n",
      " |-- Proportion with salary or wages: long (nullable = true)\n",
      " |-- Count salary or wages: long (nullable = true)\n",
      " |-- Average salary or wages: long (nullable = true)\n",
      " |-- Median salary or wages: long (nullable = true)\n",
      " |-- Proportion with net rent: long (nullable = true)\n",
      " |-- Count net rent: long (nullable = true)\n",
      " |-- Average net rent: long (nullable = true)\n",
      " |-- Median net rent: long (nullable = true)\n",
      " |-- Average total income or loss: long (nullable = true)\n",
      " |-- Median total income or loss: long (nullable = true)\n",
      " |-- Average total deductions: long (nullable = true)\n",
      " |-- Median total deductions: long (nullable = true)\n",
      " |-- Proportion with total business income: long (nullable = true)\n",
      " |-- Count total business income: long (nullable = true)\n",
      " |-- Average total business income: long (nullable = true)\n",
      " |-- Median total business income: long (nullable = true)\n",
      " |-- Proportion with total business expenses: long (nullable = true)\n",
      " |-- Count total business expenses: long (nullable = true)\n",
      " |-- Average total business expenses: long (nullable = true)\n",
      " |-- Median total business expenses: long (nullable = true)\n",
      " |-- Proportion with net tax: long (nullable = true)\n",
      " |-- Count net tax: long (nullable = true)\n",
      " |-- Average net tax: long (nullable = true)\n",
      " |-- Median net tax: long (nullable = true)\n",
      " |-- Count super total accounts balance: long (nullable = true)\n",
      " |-- Average super total accounts balance: long (nullable = true)\n",
      " |-- Median super total accounts balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = final.drop(\"user_id\", \"merchant_abn\", \"order_id\", \"order_datetime\", \"name\")\n",
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING CUSTOMER FRAUD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+\n",
      "|user_id|order_datetime|fraud_probability|\n",
      "+-------+--------------+-----------------+\n",
      "|   6228|    2021-12-19|         97.62981|\n",
      "|  21419|    2021-12-10|         99.24738|\n",
      "+-------+--------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud = sp.read.option(\"inferSchema\", True).parquet(\"../data/curated/customer_fraud\")\n",
    "c_fraud = c_fraud.withColumn(\"order_datetime\", col(\"order_datetime\").cast(DateType()))\n",
    "c_fraud.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|order_id|user_id|merchant_abn|dollar_value|order_datetime|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|       3|      3| 60956456424|      136.68|    2021-08-20|          0|                0|      0|        20|    8|        6|\n",
      "|       8|  18482| 70501974849|       68.75|    2021-08-20|          0|                0|      0|        20|    8|        6|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|user_id|order_datetime|order_id|merchant_abn|dollar_value|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|fraud_probability|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|    448|    2021-08-20|    1005| 94380689142|     6263.03|          0|                0|      0|        20|    8|        6|        14.681704|\n",
      "|   3116|    2021-08-20|    6989| 22248828825|     3958.86|          0|                0|      0|        20|    8|        6|         8.809071|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud_full = transactions.join(c_fraud, on=[\"user_id\", \"order_datetime\"])\n",
    "c_fraud_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80560"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_fraud_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+--------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+-----------+--------------------+---------------+---------------+----------------+----------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|user_id|merchant_abn|order_datetime|order_id|dollar_value|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|fraud_probability|       name|                tags|avg_monthly_inc|monthly_entropy|postcode_entropy|         revenue|state|postcode|gender|Number of individuals lodging an income tax return|Average taxable income or loss|Median taxable income or loss|Proportion with salary or wages|Count salary or wages|Average salary or wages|Median salary or wages|Proportion with net rent|Count net rent|Average net rent|Median net rent|Average total income or loss|Median total income or loss|Average total deductions|Median total deductions|Proportion with total business income|Count total business income|Average total business income|Median total business income|Proportion with total business expenses|Count total business expenses|Average total business expenses|Median total business expenses|Proportion with net tax|Count net tax|Average net tax|Median net tax|Count super total accounts balance|Average super total accounts balance|Median super total accounts balance|\n",
      "+-------+------------+--------------+--------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+-----------+--------------------+---------------+---------------+----------------+----------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "|    448| 94380689142|    2021-08-20|    1005|     6263.03|          0|                0|      0|        20|    8|        6|        14.681704|Aliquet Ltd|[(motor vehicle s...|            0.0|      2.7101815|       4.0600557|241562.580078125|   WA|    6170|Female|                                              4994|                         56564|                        44772|                              1|                 3916|                  57393|                 49510|                       1|           690|             863|            255|                       59730|                      47123|                    2865|                    598|                                    1|                        457|                        93034|                       32873|                                      1|                          436|                          76035|                         20422|                      1|         3801|          17124|         10380|                              5584|                              157038|                              62394|\n",
      "+-------+------------+--------------+--------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+-----------+--------------------+---------------+---------------+----------------+----------------+-----+--------+------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = c_fraud_full.join(merchants, on=\"merchant_abn\").join(customers, on=\"user_id\")\n",
    "X.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- holiday: long (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- fraud_probability: float (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Number of individuals lodging an income tax return: long (nullable = true)\n",
      " |-- Average taxable income or loss: long (nullable = true)\n",
      " |-- Median taxable income or loss: long (nullable = true)\n",
      " |-- Proportion with salary or wages: long (nullable = true)\n",
      " |-- Count salary or wages: long (nullable = true)\n",
      " |-- Average salary or wages: long (nullable = true)\n",
      " |-- Median salary or wages: long (nullable = true)\n",
      " |-- Proportion with net rent: long (nullable = true)\n",
      " |-- Count net rent: long (nullable = true)\n",
      " |-- Average net rent: long (nullable = true)\n",
      " |-- Median net rent: long (nullable = true)\n",
      " |-- Average total income or loss: long (nullable = true)\n",
      " |-- Median total income or loss: long (nullable = true)\n",
      " |-- Average total deductions: long (nullable = true)\n",
      " |-- Median total deductions: long (nullable = true)\n",
      " |-- Proportion with total business income: long (nullable = true)\n",
      " |-- Count total business income: long (nullable = true)\n",
      " |-- Average total business income: long (nullable = true)\n",
      " |-- Median total business income: long (nullable = true)\n",
      " |-- Proportion with total business expenses: long (nullable = true)\n",
      " |-- Count total business expenses: long (nullable = true)\n",
      " |-- Average total business expenses: long (nullable = true)\n",
      " |-- Median total business expenses: long (nullable = true)\n",
      " |-- Proportion with net tax: long (nullable = true)\n",
      " |-- Count net tax: long (nullable = true)\n",
      " |-- Average net tax: long (nullable = true)\n",
      " |-- Median net tax: long (nullable = true)\n",
      " |-- Count super total accounts balance: long (nullable = true)\n",
      " |-- Average super total accounts balance: long (nullable = true)\n",
      " |-- Median super total accounts balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(\"user_id\", \"merchant_abn\", \"order_datetime\", \"order_id\", \"name\")\n",
    "X.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical\n",
    "- holiday (done)\n",
    "- dayofmonth ?\n",
    "- dayofweek\n",
    "- month (done)\n",
    "- tags\n",
    "- state\n",
    "- gender\n",
    "- postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, GBTRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "def category_processing(data: DataFrame, outcome: str):\n",
    "    categories = [\n",
    "        \"dayofmonth\",\n",
    "        \"dayofweek\",\n",
    "        \"month\",\n",
    "        \"tags\",\n",
    "        \"state\",\n",
    "        \"gender\",\n",
    "        \"postcode\"\n",
    "    ]\n",
    "\n",
    "    # Pipeline\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categories]\n",
    "    encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_encoded\") for c in categories]\n",
    "    transformed = Pipeline(stages=indexers + encoders).fit(data).transform(data)\n",
    "\n",
    "    for c in categories:\n",
    "        transformed = transformed.drop(c).drop(c+\"_index\")\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+\n",
      "|dollar_value|Natural_var|Potential_Outlier|holiday|fraud_probability|avg_monthly_inc|monthly_entropy|postcode_entropy|         revenue|Number of individuals lodging an income tax return|Average taxable income or loss|Median taxable income or loss|Proportion with salary or wages|Count salary or wages|Average salary or wages|Median salary or wages|Proportion with net rent|Count net rent|Average net rent|Median net rent|Average total income or loss|Median total income or loss|Average total deductions|Median total deductions|Proportion with total business income|Count total business income|Average total business income|Median total business income|Proportion with total business expenses|Count total business expenses|Average total business expenses|Median total business expenses|Proportion with net tax|Count net tax|Average net tax|Median net tax|Count super total accounts balance|Average super total accounts balance|Median super total accounts balance|dayofmonth_encoded|dayofweek_encoded| month_encoded|      tags_encoded|state_encoded|gender_encoded|   postcode_encoded|\n",
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+\n",
      "|     6263.03|          0|                0|      0|        14.681704|            0.0|      2.7101815|       4.0600557|241562.580078125|                                              4994|                         56564|                        44772|                              1|                 3916|                  57393|                 49510|                       1|           690|             863|            255|                       59730|                      47123|                    2865|                    598|                                    1|                        457|                        93034|                       32873|                                      1|                          436|                          76035|                         20422|                      1|         3801|          17124|         10380|                              5584|                              157038|                              62394|   (30,[17],[1.0])|    (6,[0],[1.0])|(11,[6],[1.0])|(3207,[954],[1.0])|(7,[2],[1.0])| (2,[1],[1.0])|(3160,[1229],[1.0])|\n",
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_processed = category_processing(X, \"outcome\")\n",
    "category_processed.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71813"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X.select(\"fraud_probability\")\n",
    "y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:======================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary| fraud_probability|\n",
      "+-------+------------------+\n",
      "|  count|             71813|\n",
      "|   mean|14.717076242080452|\n",
      "| stddev| 9.404555316490011|\n",
      "|    min|          8.287144|\n",
      "|    max|          97.62981|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+\n",
      "|dollar_value|Natural_var|Potential_Outlier|holiday|fraud_probability|avg_monthly_inc|monthly_entropy|postcode_entropy|         revenue|Number of individuals lodging an income tax return|Average taxable income or loss|Median taxable income or loss|Proportion with salary or wages|Count salary or wages|Average salary or wages|Median salary or wages|Proportion with net rent|Count net rent|Average net rent|Median net rent|Average total income or loss|Median total income or loss|Average total deductions|Median total deductions|Proportion with total business income|Count total business income|Average total business income|Median total business income|Proportion with total business expenses|Count total business expenses|Average total business expenses|Median total business expenses|Proportion with net tax|Count net tax|Average net tax|Median net tax|Count super total accounts balance|Average super total accounts balance|Median super total accounts balance|dayofmonth_encoded|dayofweek_encoded| month_encoded|      tags_encoded|state_encoded|gender_encoded|   postcode_encoded|fraud_buckets|\n",
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+\n",
      "|     6263.03|          0|                0|      0|        14.681704|            0.0|      2.7101815|       4.0600557|241562.580078125|                                              4994|                         56564|                        44772|                              1|                 3916|                  57393|                 49510|                       1|           690|             863|            255|                       59730|                      47123|                    2865|                    598|                                    1|                        457|                        93034|                       32873|                                      1|                          436|                          76035|                         20422|                      1|         3801|          17124|         10380|                              5584|                              157038|                              62394|   (30,[17],[1.0])|    (6,[0],[1.0])|(11,[6],[1.0])|(3207,[954],[1.0])|(7,[2],[1.0])| (2,[1],[1.0])|(3160,[1229],[1.0])|          1.0|\n",
      "+------------+-----------+-----------------+-------+-----------------+---------------+---------------+----------------+----------------+--------------------------------------------------+------------------------------+-----------------------------+-------------------------------+---------------------+-----------------------+----------------------+------------------------+--------------+----------------+---------------+----------------------------+---------------------------+------------------------+-----------------------+-------------------------------------+---------------------------+-----------------------------+----------------------------+---------------------------------------+-----------------------------+-------------------------------+------------------------------+-----------------------+-------------+---------------+--------------+----------------------------------+------------------------------------+-----------------------------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "buckets = Bucketizer(splits=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], inputCol=\"fraud_probability\", outputCol=\"fraud_buckets\")\n",
    "X_bucks = buckets.transform(category_processed)\n",
    "\n",
    "X_bucks.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 350:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 1934|\n",
      "|          4.0|  990|\n",
      "|          5.0|  576|\n",
      "|          6.0|  360|\n",
      "|          7.0|  193|\n",
      "|          8.0|  102|\n",
      "|          9.0|   11|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_bucks.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96272"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "X_bucks_9 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 9.0)\n",
    "X_bucks_8 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 8.0)\n",
    "X_bucks_7 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 7.0)\n",
    "X_bucks_6 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 6.0)\n",
    "X_bucks_5 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 5.0)\n",
    "X_bucks_4 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 4.0)\n",
    "X_bucks_3 = X_bucks.filter(X_bucks[\"fraud_buckets\"] == 3.0)\n",
    "\n",
    "X_bucks_9_over = X_bucks_9.sample(withReplacement=True, fraction=350.0, seed=69)\n",
    "X_bucks_8_over = X_bucks_8.sample(withReplacement=True, fraction=35.0, seed=69)\n",
    "X_bucks_7_over = X_bucks_7.sample(withReplacement=True, fraction=20.0, seed=69)\n",
    "X_bucks_6_over = X_bucks_6.sample(withReplacement=True, fraction=15.0, seed=69)\n",
    "X_bucks_5_over = X_bucks_5.sample(withReplacement=True, fraction=7.0, seed=69)\n",
    "X_bucks_4_over = X_bucks_4.sample(withReplacement=True, fraction=4.0, seed=69)\n",
    "X_bucks_3_over = X_bucks_3.sample(withReplacement=True, fraction=2.0, seed=69)\n",
    "\n",
    "X_adjusted = reduce(DataFrame.unionAll, [X_bucks_9_over, X_bucks_8_over, X_bucks_7_over, X_bucks_6_over, X_bucks_5_over, X_bucks_4_over, X_bucks_3_over])\n",
    "X_adjusted = reduce(DataFrame.unionAll, [X_adjusted, X_bucks.filter(X_bucks[\"fraud_buckets\"] == 2.0), X_bucks.filter(X_bucks[\"fraud_buckets\"] == 1.0), X_bucks.filter(X_bucks[\"fraud_buckets\"] == 0.0)])\n",
    "X_adjusted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 380:=================================================>     (81 + 8) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 3910|\n",
      "|          4.0| 3931|\n",
      "|          5.0| 4025|\n",
      "|          6.0| 5450|\n",
      "|          7.0| 3876|\n",
      "|          8.0| 3583|\n",
      "|          9.0| 3850|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_adjusted.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:23:31 WARN DAGScheduler: Broadcasting large task binary with size 2027.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67492\n",
      "22/10/06 00:24:31 WARN DAGScheduler: Broadcasting large task binary with size 2028.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19067\n",
      "22/10/06 00:25:16 WARN DAGScheduler: Broadcasting large task binary with size 2028.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9713"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = X_adjusted.randomSplit([0.7, 0.2, 0.1], seed=69)\n",
    "\n",
    "print(train.count())\n",
    "print(val.count())\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dollar_value', 'Natural_var', 'Potential_Outlier', 'holiday', 'fraud_probability', 'avg_monthly_inc', 'monthly_entropy', 'postcode_entropy', 'revenue', 'Number of individuals lodging an income tax return', 'Average taxable income or loss', 'Median taxable income or loss', 'Proportion with salary or wages', 'Count salary or wages', 'Average salary or wages', 'Median salary or wages', 'Proportion with net rent', 'Count net rent', 'Average net rent', 'Median net rent', 'Average total income or loss', 'Median total income or loss', 'Average total deductions', 'Median total deductions', 'Proportion with total business income', 'Count total business income', 'Average total business income', 'Median total business income', 'Proportion with total business expenses', 'Count total business expenses', 'Average total business expenses', 'Median total business expenses', 'Proportion with net tax', 'Count net tax', 'Average net tax', 'Median net tax', 'Count super total accounts balance', 'Average super total accounts balance', 'Median super total accounts balance', 'dayofmonth_encoded', 'dayofweek_encoded', 'month_encoded', 'tags_encoded', 'state_encoded', 'gender_encoded', 'postcode_encoded', 'fraud_buckets']\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numerical(data: DataFrame):\n",
    "    \"\"\"\n",
    "    Function to scale and process numerical columns\n",
    "    \"\"\"\n",
    "    # Scaler\n",
    "    columns = ['dollar_value', 'avg_monthly_inc',\n",
    "    'monthly_entropy', 'postcode_entropy', 'revenue', 'Number of individuals lodging an income tax return', \n",
    "    'Average taxable income or loss', 'Median taxable income or loss', 'Proportion with salary or wages', 'Count salary or wages', \n",
    "    'Average salary or wages', 'Median salary or wages', 'Proportion with net rent', 'Count net rent', 'Average net rent', \n",
    "    'Median net rent', 'Average total income or loss', 'Median total income or loss', 'Average total deductions', \n",
    "    'Median total deductions', 'Proportion with total business income', 'Count total business income', \n",
    "    'Average total business income', 'Median total business income', 'Proportion with total business expenses', \n",
    "    'Count total business expenses', 'Average total business expenses', 'Median total business expenses', \n",
    "    'Proportion with net tax', 'Count net tax', 'Average net tax', 'Median net tax', 'Count super total accounts balance', \n",
    "    'Average super total accounts balance', 'Median super total accounts balance']\n",
    "\n",
    "    va = VectorAssembler(inputCols=columns, outputCol=\"to_scale\")\n",
    "    sc = StandardScaler(inputCol=\"to_scale\", outputCol=\"scaled\")\n",
    "\n",
    "    va_data = va.transform(data)\n",
    "    data = sc.fit(va_data).transform(va_data)\n",
    "    \n",
    "    # Drop other columns\n",
    "    for c in columns:\n",
    "        data = data.drop(c)\n",
    "    return data.drop(\"to_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:41:35 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 404:======================================================>(89 + 1) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:41:48 WARN DAGScheduler: Broadcasting large task binary with size 1473.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:41:53 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 410:==================================================>    (82 + 8) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:42:02 WARN DAGScheduler: Broadcasting large task binary with size 1473.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:42:06 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 416:===================================================>   (85 + 5) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:42:15 WARN DAGScheduler: Broadcasting large task binary with size 1473.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:42:19 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|Natural_var|Potential_Outlier|holiday|fraud_probability|dayofmonth_encoded|dayofweek_encoded| month_encoded|      tags_encoded|state_encoded|gender_encoded|   postcode_encoded|fraud_buckets|              scaled|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|          0|                0|      0|         92.99139|   (30,[29],[1.0])|        (6,[],[])|(11,[2],[1.0])|(3207,[183],[1.0])|(7,[1],[1.0])| (2,[0],[1.0])|(3160,[1772],[1.0])|          9.0|[0.00119578017432...|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "22/10/06 00:42:21 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|Natural_var|Potential_Outlier|holiday|fraud_probability|dayofmonth_encoded|dayofweek_encoded| month_encoded|      tags_encoded|state_encoded|gender_encoded|   postcode_encoded|fraud_buckets|              scaled|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|          0|                0|      0|         92.99139|   (30,[29],[1.0])|        (6,[],[])|(11,[2],[1.0])|(3207,[183],[1.0])|(7,[1],[1.0])| (2,[0],[1.0])|(3160,[1772],[1.0])|          9.0|[0.00119575374245...|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "22/10/06 00:42:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|Natural_var|Potential_Outlier|holiday|fraud_probability|dayofmonth_encoded|dayofweek_encoded| month_encoded|      tags_encoded|state_encoded|gender_encoded|   postcode_encoded|fraud_buckets|              scaled|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "|          0|                0|      0|         92.99139|   (30,[29],[1.0])|        (6,[],[])|(11,[2],[1.0])|(3207,[183],[1.0])|(7,[1],[1.0])| (2,[0],[1.0])|(3160,[1772],[1.0])|          9.0|[0.00121487741123...|\n",
      "+-----------+-----------------+-------+-----------------+------------------+-----------------+--------------+------------------+-------------+--------------+-------------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_processed = process_numerical(train)\n",
    "val_processed = process_numerical(val)\n",
    "test_processed = process_numerical(test)\n",
    "\n",
    "train_processed.show(1)\n",
    "val_processed.show(1)\n",
    "test_processed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data: DataFrame, outcome: str):\n",
    "    \"\"\"\n",
    "    Function to vectorize all the processed data\n",
    "    \"\"\"\n",
    "    data = data.withColumnRenamed(outcome, \"label\")\n",
    "    return VectorAssembler(\n",
    "        inputCols= [c for c in data.drop(\"label\").columns],\n",
    "        outputCol=\"features\"\n",
    "    ).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 00:56:04 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/10/06 00:56:06 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/10/06 00:56:09 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Natural_var=0, Potential_Outlier=0, holiday=0, fraud_probability=92.99138641357422, dayofmonth_encoded=SparseVector(30, {29: 1.0}), dayofweek_encoded=SparseVector(6, {}), month_encoded=SparseVector(11, {2: 1.0}), tags_encoded=SparseVector(3207, {183: 1.0}), state_encoded=SparseVector(7, {1: 1.0}), gender_encoded=SparseVector(2, {0: 1.0}), postcode_encoded=SparseVector(3160, {1772: 1.0}), label=9.0, scaled=DenseVector([0.0012, -0.3014, 9.7089, 5.4351, 0.2911, 0.0152, 3.2695, 5.1927, 0.0, 0.0161, 4.9966, 5.6526, 40.2451, 0.0126, -1.8313, -3.0797, 3.1927, 5.5668, 1.1192, 2.7861, 44.084, 0.0097, 2.472, 1.2469, 37.2616, 0.0088, 2.4669, 0.9598, 0.0, 0.0144, 1.8605, 2.9789, 0.0199, 0.8657, 0.5944]), features=SparseVector(6462, {3: 92.9914, 33: 1.0, 42: 1.0, 234: 1.0, 3259: 1.0, 3265: 1.0, 5039: 1.0, 6427: 0.0012, 6428: -0.3014, 6429: 9.7089, 6430: 5.4351, 6431: 0.2911, 6432: 0.0152, 6433: 3.2695, 6434: 5.1927, 6436: 0.0161, 6437: 4.9966, 6438: 5.6526, 6439: 40.2451, 6440: 0.0126, 6441: -1.8313, 6442: -3.0797, 6443: 3.1927, 6444: 5.5668, 6445: 1.1192, 6446: 2.7861, 6447: 44.084, 6448: 0.0097, 6449: 2.472, 6450: 1.2469, 6451: 37.2616, 6452: 0.0088, 6453: 2.4669, 6454: 0.9598, 6456: 0.0144, 6457: 1.8605, 6458: 2.9789, 6459: 0.0199, 6460: 0.8657, 6461: 0.5944}))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector = vectorize(train_processed, \"fraud_buckets\")\n",
    "val_vector = vectorize(val_processed, \"fraud_buckets\")\n",
    "test_vector = vectorize(test_processed, \"fraud_buckets\")\n",
    "\n",
    "train_vector.head(1)\n",
    "val_vector.head(1)\n",
    "test_vector.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCount = 6462                # Seen from sparse vector column\n",
    "layers = [512, 256, 64, 10]\n",
    "model = MultilayerPerceptronClassifier(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    maxIter=50,\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=669)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(train_vector.select(\"features\", \"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = model_fit.transform(train_vector)\n",
    "val_output = model_fit.transform(val_vector)\n",
    "test_output = model_fit.transform(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_vector\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39mparquet(\u001b[39m\"\u001b[39m\u001b[39m../models/train_vector\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m val_vector\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mparquet(\u001b[39m\"\u001b[39m\u001b[39m../models/val_vector\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y140sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test_vector\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mparquet(\u001b[39m\"\u001b[39m\u001b[39m../models/test_vector\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/sql/dataframe.py:338\u001b[0m, in \u001b[0;36mDataFrame.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrameWriter:\n\u001b[1;32m    328\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39m    storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m    :class:`DataFrameWriter`\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrameWriter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/sql/readwriter.py:731\u001b[0m, in \u001b[0;36mDataFrameWriter.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df \u001b[39m=\u001b[39m df\n\u001b[1;32m    730\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msparkSession\n\u001b[0;32m--> 731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwrite()\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "train_vector.write.parquet(\"../models/train_vector\", mode=\"overwrite\")\n",
    "val_vector.write.parquet(\"../models/val_vector\", mode=\"overwrite\")\n",
    "test_vector.write.parquet(\"../models/test_vector\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 01:11:58 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 575:>                                                       (0 + 8) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 01:11:59 ERROR Executor: Exception in task 7.0 in stage 575.0 (TID 8500)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n",
      "\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n",
      "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n",
      "\t... 19 more\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 7.0 in stage 575.0 (TID 8500) (172.18.71.108 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n",
      "\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n",
      "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n",
      "\t... 19 more\n",
      "\n",
      "22/10/06 01:11:59 ERROR TaskSetManager: Task 7 in stage 575.0 failed 1 times; aborting job\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 1.0 in stage 575.0 (TID 8494) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 3.0 in stage 575.0 (TID 8496) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 6.0 in stage 575.0 (TID 8499) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 5.0 in stage 575.0 (TID 8498) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 2.0 in stage 575.0 (TID 8495) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 4.0 in stage 575.0 (TID 8497) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 0.0 in stage 575.0 (TID 8493) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n",
      "22/10/06 01:11:59 WARN TaskSetManager: Lost task 8.0 in stage 575.0 (TID 8501) (172.18.71.108 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2754.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 575.0 failed 1 times, most recent failure: Lost task 7.0 in stage 575.0 (TID 8500) (172.18.71.108 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedPrecision$lzycompute(MulticlassMetrics.scala:218)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedPrecision(MulticlassMetrics.scala:218)\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:155)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     evaluator \u001b[39m=\u001b[39m MulticlassClassificationEvaluator(metricName\u001b[39m=\u001b[39mmetric)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m metric \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(evaluator\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/jay/Classes/Apl-DS/generic-buy-now-pay-later-project-group-3/notebooks/model.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         val_output\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m))))\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/ml/evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(dataset)\n\u001b[1;32m    112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mevaluate(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Classes/Apl-DS/virtual-p2/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2754.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 575.0 failed 1 times, most recent failure: Lost task 7.0 in stage 575.0 (TID 8500) (172.18.71.108 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedPrecision$lzycompute(MulticlassMetrics.scala:218)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedPrecision(MulticlassMetrics.scala:218)\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:155)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (ProbabilisticClassificationModel$$Lambda$5189/0x0000000801fdc458: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage41.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: A & B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:42)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:121)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(\n",
    "        val_output.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('virtual-p2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf6849838b3a8621666e21fdc4cc1583090fffb5f1906a909fbc1c95ae1bb65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
