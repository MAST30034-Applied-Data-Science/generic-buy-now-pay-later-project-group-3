{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "\n",
    "The following notebook contains the classification model for the fraud detection algorithm. It is assumed that the fraud data given is a small true sample which can be used to train and predict the rest of the unlabeled transactions. Since this notebook is large, it has been divided into checkpoints so the spark session does not run out of memory. To make sure the notebook is reproducible, ensure you execute all cells just before the \"Model\" heading, and restart the kernel and read from the last saved file.\n",
    "\n",
    "The notebook tries 2 approaches\n",
    "- Multilayer Perceptron Classifier\n",
    "- Random Forest Classifier\n",
    "\n",
    "The RF yieleded better results overall with more categories being predicted slighter higher probabilities than true values. To achieve the final results, running the MLP model is not necessary. Instead run the RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH Variables\n",
    "# Change path environement to specific use case\n",
    "\n",
    "dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:40:36 WARN Utils: Your hostname, J-L resolves to a loopback address: 127.0.1.1; using 172.21.176.78 instead (on interface eth0)\n",
      "22/10/17 02:40:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:40:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.176.78:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0da29e4250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = (\n",
    "    SparkSession.builder.appName(\"Model\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"+11\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .config('spark.sql.parquet.cacheMetadata', 'True')\n",
    "    .getOrCreate()\n",
    ")\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|order_id|user_id|merchant_abn|dollar_value|order_datetime|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "|       3|      3| 60956456424|      136.68|    2021-08-20|          0|                0|      0|        20|    8|        6|\n",
      "+--------+-------+------------+------------+--------------+-----------+-----------------+-------+----------+-----+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "|merchant_abn|         name|Earnings_Class|BNPL_Fee|tags|avg_monthly_inc|monthly_entropy|postcode_entropy|          revenue|\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "| 10023283211|Felis Limited|             e|    0.18|   0|     -0.0952381|      2.9858725|        7.439226|703277.8708539009|\n",
      "+------------+-------------+--------------+--------+----+---------------+---------------+----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "22/10/17 02:23:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(state='ACT', postcode=200, gender='Female', user_id=71674, Number of individuals lodging an income tax return=5524, Average taxable income or loss=66722, Median taxable income or loss=52958, Proportion with salary or wages=1, Count salary or wages=5009, Average salary or wages=64930, Median salary or wages=55579, Proportion with net rent=1, Count net rent=762, Average net rent=-4289, Median net rent=-2448, Average total income or loss=68991, Median total income or loss=54988, Average total deductions=2244, Median total deductions=872, Proportion with total business income=1, Count total business income=382, Average total business income=56170, Median total business income=18742, Proportion with total business expenses=1, Count total business expenses=343, Average total business expenses=42645, Median total business expenses=8664, Proportion with net tax=1, Count net tax=4586, Average net tax=18805, Median net tax=11482, Count super total accounts balance=7620, Average super total accounts balance=79163, Median super total accounts balance=10415)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = sp.read.option(\"inferSchema\", True).parquet(dir + \"processed/transactions\")\n",
    "merchants = sp.read.option(\"inferSchema\", True).parquet(dir + \"processed/merchants\")\n",
    "customers = sp.read.option(\"inferSchema\", True).parquet(dir + \"processed/customers\")\n",
    "\n",
    "transactions.show(1)\n",
    "merchants.show(1)\n",
    "customers.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING CUSTOMER FRAUD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+\n",
      "|user_id|order_datetime|fraud_probability|\n",
      "+-------+--------------+-----------------+\n",
      "|   6228|    2021-12-19|         97.62981|\n",
      "|  21419|    2021-12-10|         99.24738|\n",
      "+-------+--------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud = sp.read.option(\"inferSchema\", True).parquet(dir + \"curated/customer_fraud\")\n",
    "c_fraud = c_fraud.withColumn(\"order_datetime\", col(\"order_datetime\").cast(DateType()))\n",
    "c_fraud.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|user_id|order_datetime|order_id|merchant_abn|dollar_value|Natural_var|Potential_Outlier|holiday|dayofmonth|month|dayofweek|fraud_probability|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "|    448|    2021-08-20|    1005| 94380689142|     6263.03|          0|                0|      0|        20|    8|        6|        14.681704|\n",
      "|   3116|    2021-08-20|    6989| 22248828825|     3958.86|          0|                0|      0|        20|    8|        6|         8.809071|\n",
      "+-------+--------------+--------+------------+------------+-----------+-----------------+-------+----------+-----+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_fraud_full = transactions.join(c_fraud, on=[\"user_id\", \"order_datetime\"])\n",
    "c_fraud_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80560"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_fraud_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=448, merchant_abn=94380689142, order_datetime=datetime.date(2021, 8, 20), order_id=1005, dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, holiday=0, dayofmonth=20, month=8, dayofweek=6, fraud_probability=14.681703567504883, name='Aliquet Ltd', Earnings_Class='b', BNPL_Fee=3.77, tags=12, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, state='WA', postcode=6170, gender='Female', Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = c_fraud_full.join(merchants, on=\"merchant_abn\").join(customers, on=\"user_id\")\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now the full dataset with all merge combinations. To this we will further create categorical columns and standardize the numerical columns (after train test split). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- fraud_probability: float (nullable = true)\n",
      " |-- Earnings_Class: string (nullable = true)\n",
      " |-- BNPL_Fee: double (nullable = true)\n",
      " |-- tags: integer (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Number of individuals lodging an income tax return: long (nullable = true)\n",
      " |-- Average taxable income or loss: long (nullable = true)\n",
      " |-- Median taxable income or loss: long (nullable = true)\n",
      " |-- Proportion with salary or wages: long (nullable = true)\n",
      " |-- Count salary or wages: long (nullable = true)\n",
      " |-- Average salary or wages: long (nullable = true)\n",
      " |-- Median salary or wages: long (nullable = true)\n",
      " |-- Proportion with net rent: long (nullable = true)\n",
      " |-- Count net rent: long (nullable = true)\n",
      " |-- Average net rent: long (nullable = true)\n",
      " |-- Median net rent: long (nullable = true)\n",
      " |-- Average total income or loss: long (nullable = true)\n",
      " |-- Median total income or loss: long (nullable = true)\n",
      " |-- Average total deductions: long (nullable = true)\n",
      " |-- Median total deductions: long (nullable = true)\n",
      " |-- Proportion with total business income: long (nullable = true)\n",
      " |-- Count total business income: long (nullable = true)\n",
      " |-- Average total business income: long (nullable = true)\n",
      " |-- Median total business income: long (nullable = true)\n",
      " |-- Proportion with total business expenses: long (nullable = true)\n",
      " |-- Count total business expenses: long (nullable = true)\n",
      " |-- Average total business expenses: long (nullable = true)\n",
      " |-- Median total business expenses: long (nullable = true)\n",
      " |-- Proportion with net tax: long (nullable = true)\n",
      " |-- Count net tax: long (nullable = true)\n",
      " |-- Average net tax: long (nullable = true)\n",
      " |-- Median net tax: long (nullable = true)\n",
      " |-- Count super total accounts balance: long (nullable = true)\n",
      " |-- Average super total accounts balance: long (nullable = true)\n",
      " |-- Median super total accounts balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(\"user_id\", \"merchant_abn\", \"order_datetime\", \"order_id\", \"name\", \"postcode\", \"holiday\")\n",
    "X.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize\n",
    "\n",
    "- dayofmonth\n",
    "- dayofweek\n",
    "- month\n",
    "- tags\n",
    "- state\n",
    "- gender\n",
    "- Earnings Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_processing(data: DataFrame, outcome: str):\n",
    "    categories = [\n",
    "        \"dayofmonth\",\n",
    "        \"dayofweek\",\n",
    "        \"month\",\n",
    "        \"tags\",\n",
    "        \"state\",\n",
    "        \"gender\",\n",
    "        \"Earnings_Class\"\n",
    "    ]\n",
    "\n",
    "    # Pipeline\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categories]\n",
    "    encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_encoded\") for c in categories]\n",
    "    transformer = Pipeline(stages=indexers + encoders).fit(data)\n",
    "    transformed = transformer.transform(data)\n",
    "\n",
    "    for c in categories:\n",
    "        transformed = transformed.drop(c).drop(c+\"_index\")\n",
    "    return transformer, transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, fraud_probability=14.681703567504883, BNPL_Fee=3.77, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {5: 1.0}), state_encoded=SparseVector(7, {2: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_transformer, category_processed = category_processing(X, \"outcome\")\n",
    "category_processed.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE QUANTILES\n",
    "\n",
    "Creating quantiles of 0-10%, 10-20%, and so on of the fraud probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dollar_value=6263.02978515625, Natural_var=0, Potential_Outlier=0, BNPL_Fee=3.77, avg_monthly_inc=0.0, monthly_entropy=2.710181474685669, postcode_entropy=4.060055732727051, revenue=241562.580078125, Number of individuals lodging an income tax return=4994, Average taxable income or loss=56564, Median taxable income or loss=44772, Proportion with salary or wages=1, Count salary or wages=3916, Average salary or wages=57393, Median salary or wages=49510, Proportion with net rent=1, Count net rent=690, Average net rent=863, Median net rent=255, Average total income or loss=59730, Median total income or loss=47123, Average total deductions=2865, Median total deductions=598, Proportion with total business income=1, Count total business income=457, Average total business income=93034, Median total business income=32873, Proportion with total business expenses=1, Count total business expenses=436, Average total business expenses=76035, Median total business expenses=20422, Proportion with net tax=1, Count net tax=3801, Average net tax=17124, Median net tax=10380, Count super total accounts balance=5584, Average super total accounts balance=157038, Median super total accounts balance=62394, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {5: 1.0}), state_encoded=SparseVector(7, {2: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), fraud_buckets=1.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "buckets = Bucketizer(splits=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], inputCol=\"fraud_probability\", outputCol=\"fraud_buckets\")\n",
    "X_bucks = buckets.transform(category_processed).drop(\"fraud_probability\")\n",
    "\n",
    "X_bucks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 1934|\n",
      "|          4.0|  990|\n",
      "|          5.0|  576|\n",
      "|          6.0|  360|\n",
      "|          7.0|  193|\n",
      "|          8.0|  102|\n",
      "|          9.0|   11|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_bucks.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95171"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fractions = [0, 0, 0, 2, 4, 7, 15, 20, 35, 250]\n",
    "\n",
    "X_adjusted = reduce(\n",
    "    DataFrame.unionAll,\n",
    "    [X_bucks.filter(X_bucks.fraud_buckets == float(x)).sample(withReplacement=True, fraction=float(fractions[x]), seed=69) for x in range(3, 10)]\n",
    ")\n",
    "X_adjusted = reduce(\n",
    "    DataFrame.unionAll,\n",
    "    [X_adjusted] + [X_bucks.filter(X_bucks.fraud_buckets == float(x)) for x in range(0, 3)]\n",
    ")\n",
    "\n",
    "X_adjusted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:==================================================>     (81 + 8) / 90]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|fraud_buckets|count|\n",
      "+-------------+-----+\n",
      "|          0.0|22923|\n",
      "|          1.0|38611|\n",
      "|          2.0| 6113|\n",
      "|          3.0| 3910|\n",
      "|          4.0| 3931|\n",
      "|          5.0| 4025|\n",
      "|          6.0| 5450|\n",
      "|          7.0| 3876|\n",
      "|          8.0| 3583|\n",
      "|          9.0| 2749|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X_adjusted.groupBy(\"fraud_buckets\").count().orderBy(\"fraud_buckets\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = X_adjusted.randomSplit([0.7, 0.2, 0.1], seed=69)\n",
    "\n",
    "#print(train.count())\n",
    "#print(val.count())\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:10:58 WARN DAGScheduler: Broadcasting large task binary with size 1555.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1555.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:12:52 WARN DAGScheduler: Broadcasting large task binary with size 1555.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.parquet(\"../models/train_raw\", mode=\"overwrite\")\n",
    "val.write.parquet(\"../models/val_raw\", mode=\"overwrite\")\n",
    "test.write.parquet(\"../models/test_raw\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "        ### RESTART KERNEL HERE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = sp.read.option(\"inferSchema\", True).parquet(\"../models/train_raw/\", mode=\"overwrite\")\n",
    "val = sp.read.option(\"inferSchema\", True).parquet(\"../models/val_raw/\", mode=\"overwrite\")\n",
    "test = sp.read.option(\"inferSchema\", True).parquet(\"../models/test_raw/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_numerical(data: DataFrame):\n",
    "    \"\"\"\n",
    "    Function to scale and process numerical columns\n",
    "    \"\"\"\n",
    "    # Scaler\n",
    "    columns = ['dollar_value', 'avg_monthly_inc', 'BNPL_Fee',\n",
    "    'monthly_entropy', 'postcode_entropy', 'revenue', 'Number of individuals lodging an income tax return', \n",
    "    'Average taxable income or loss', 'Median taxable income or loss', 'Proportion with salary or wages', 'Count salary or wages', \n",
    "    'Average salary or wages', 'Median salary or wages', 'Proportion with net rent', 'Count net rent', 'Average net rent', \n",
    "    'Median net rent', 'Average total income or loss', 'Median total income or loss', 'Average total deductions', \n",
    "    'Median total deductions', 'Proportion with total business income', 'Count total business income', \n",
    "    'Average total business income', 'Median total business income', 'Proportion with total business expenses', \n",
    "    'Count total business expenses', 'Average total business expenses', 'Median total business expenses', \n",
    "    'Proportion with net tax', 'Count net tax', 'Average net tax', 'Median net tax', 'Count super total accounts balance', \n",
    "    'Average super total accounts balance', 'Median super total accounts balance']\n",
    "\n",
    "    va = VectorAssembler(inputCols=columns, outputCol=\"to_scale\")\n",
    "    sc = StandardScaler(inputCol=\"to_scale\", outputCol=\"scaled\")\n",
    "\n",
    "    va_data = va.transform(data)\n",
    "    data = sc.fit(va_data).transform(va_data)\n",
    "    \n",
    "    # Drop other columns\n",
    "    for c in columns:\n",
    "        data = data.drop(c)\n",
    "    return data.drop(\"to_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:17:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Natural_var=0, Potential_Outlier=0, dayofmonth_encoded=SparseVector(30, {6: 1.0}), dayofweek_encoded=SparseVector(6, {5: 1.0}), month_encoded=SparseVector(11, {5: 1.0}), tags_encoded=SparseVector(24, {17: 1.0}), state_encoded=SparseVector(7, {0: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {2: 1.0}), fraud_buckets=1.0, scaled=DenseVector([0.0001, 0.01, 1.1552, 11.8533, 5.5901, 0.3913, 1.0881, 4.0182, 6.3873, 0.0, 1.0838, 6.1235, 7.2708, 44.0159, 1.1971, 0.061, -0.0291, 3.9179, 6.5207, 1.2957, 3.8719, 37.2041, 0.9587, 1.9516, 1.9602, 31.132, 0.9613, 1.4767, 0.9168, 0.0, 1.0796, 2.6103, 4.6455, 1.1213, 2.7123, 4.0691]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed = process_numerical(train)\n",
    "val_processed = process_numerical(val)\n",
    "test_processed = process_numerical(test)\n",
    "\n",
    "train_processed.head(1)\n",
    "val_processed.head(1)\n",
    "test_processed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data: DataFrame, outcome: str):\n",
    "    \"\"\"\n",
    "    Function to vectorize all the processed data\n",
    "    \"\"\"\n",
    "    data = data.withColumnRenamed(outcome, \"label\")\n",
    "    return VectorAssembler(\n",
    "        inputCols= [c for c in data.drop(\"label\").columns],\n",
    "        outputCol=\"features\"\n",
    "    ).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector = vectorize(train_processed, \"fraud_buckets\")\n",
    "val_vector = vectorize(val_processed, \"fraud_buckets\")\n",
    "test_vector = vectorize(test_processed, \"fraud_buckets\")\n",
    "\n",
    "#train_vector.head(1)\n",
    "#val_vector.head(1)\n",
    "#test_vector.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Safety check\n",
    "target_dir = \"../models/\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "train_vector.select(\"features\", \"label\").write.parquet(\"../models/train_vector\", mode=\"overwrite\")\n",
    "val_vector.select(\"features\", \"label\").write.parquet(\"../models/val_vector\", mode=\"overwrite\")\n",
    "test_vector.select(\"features\", \"label\").write.parquet(\"../models/test_vector\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "        #### RESTART KERNEL HERE\n",
    "\n",
    "---\n",
    "\n",
    "To save memory and increase speed, save the data and restart kernel and start from below. You may have to run the first 2 cells of the notebook before running the cells below. Again, it is unnecessary to run MLP, it is just for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/train_vector/\")\n",
    "val_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/val_vector/\")\n",
    "test_vector = sp.read.option(\"inferSchema\", True).parquet(\"../models/test_vector/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI LAYER PERCEPTRON CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCount = 122                            # Seen from sparse vector column\n",
    "layers = [122, 256, 64, 10]\n",
    "model = MultilayerPerceptronClassifier(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    solver='gd',\n",
    "    maxIter=100,\n",
    "    layers=layers,\n",
    "    blockSize=64,\n",
    "    seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 16:10:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/10/08 16:10:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/10/08 16:10:41 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_vector.select(\"features\", \"label\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = model_fit.transform(train_vector)\n",
    "val_output = model_fit.transform(val_vector.dropna())\n",
    "test_output = model_fit.transform(test_vector.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.3971909532169782\n",
      "Train weightedFalsePositiveRate = 0.3971909532169782\n"
     ]
    }
   ],
   "source": [
    "# metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "metrics = [\"accuracy\"]\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Validation ' + metric + ' = ' + str(evaluator.evaluate(\n",
    "        val_output.select(\"prediction\", \"label\"))))\n",
    "    print('Test ' + metric + ' = ' + str(evaluator.evaluate(\n",
    "        test_output.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining how good the scores are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|       rawPrediction|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  1.0|[1.21844183743563...|       1.0|\n",
      "|  1.0|[1.27374537812617...|       1.0|\n",
      "|  1.0|[1.25855328047493...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_output.select(\"label\", \"rawPrediction\", \"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---+\n",
      "|label|prediction|MSE|\n",
      "+-----+----------+---+\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "+-----+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_square_error_val_score = val_output.select(\"label\", \"prediction\").withColumn(\"MSE\" , (col(\"label\") - col(\"prediction\")))\n",
    "mean_square_error_val_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 994:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| MSE|count|\n",
      "+----+-----+\n",
      "| 0.0| 7617|\n",
      "|-1.0| 4518|\n",
      "| 1.0| 1176|\n",
      "| 3.0|  793|\n",
      "| 2.0|  791|\n",
      "| 4.0|  851|\n",
      "| 5.0| 1104|\n",
      "| 7.0|  705|\n",
      "| 6.0|  742|\n",
      "| 8.0|  537|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mean_square_error_val_score.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---+\n",
      "|label|prediction|MSE|\n",
      "+-----+----------+---+\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "|  1.0|       1.0|0.0|\n",
      "+-----+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_square_error_test_score = test_output.select(\"label\", \"prediction\").withColumn(\"MSE\" , ((col(\"label\") - col(\"prediction\")) ** 2) ** 0.5)\n",
    "mean_square_error_test_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|MSE|count|\n",
      "+---+-----+\n",
      "|0.0| 3846|\n",
      "|1.0| 3013|\n",
      "|4.0|  418|\n",
      "|3.0|  448|\n",
      "|2.0|  369|\n",
      "|5.0|  563|\n",
      "|7.0|  389|\n",
      "|6.0|  391|\n",
      "|8.0|  246|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_square_error_test_score.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------------+\n",
      "|MSE|count|       Weighted MSE|\n",
      "+---+-----+-------------------+\n",
      "|0.0| 3846|                0.0|\n",
      "|1.0| 3013|0.31116389548693585|\n",
      "|4.0|  418|0.17267375813281008|\n",
      "|3.0|  448|0.13879995869048847|\n",
      "|2.0|  369|0.07621604874522359|\n",
      "|5.0|  563| 0.2907156872869978|\n",
      "|7.0|  389| 0.2812144996385418|\n",
      "|6.0|  391|0.24228028503562946|\n",
      "|8.0|  246|0.20324279665392958|\n",
      "+---+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted MSE\n",
    "MSE_count = mean_square_error_test_score.groupBy(\"MSE\").count().groupBy().sum().select(\"sum(count)\")\n",
    "MSE_test = mean_square_error_test_score.groupBy(\"MSE\").count().withColumn(\"Weighted MSE\", col(\"MSE\") * col(\"count\") / MSE_count.collect()[0][\"sum(count)\"])\n",
    "MSE_test.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| sum(Weighted MSE)|\n",
      "+------------------+\n",
      "|1.7163069296705566|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MSE_test.groupBy().sum().select(\"sum(Weighted MSE)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the Mean Squared Error between the categories is 1.7. Which means overall, the MSE Is 17% for the fraud probability which is a great value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    #### RESTART KERNEL AGAIN IF NECESSARY\n",
    "---\n",
    "\n",
    "Here, restart kernel and read both full dataset and train, val, test vectorized datasets to feed into the RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(122,[11,32,39,53...|  1.0|[27.7622795157378...|[0.27762279515737...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "model = rf.fit(train_vector)\n",
    "val_pred = model.transform(val_vector)\n",
    "val_pred.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:19:40 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:19:42 WARN DAGScheduler: Broadcasting large task binary with size 1028.4 KiB\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "|       6.0|\n",
      "|       5.0|\n",
      "|       7.0|\n",
      "|       8.0|\n",
      "|       9.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val_pred.select(\"prediction\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4674524795582457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(val_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.transform(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4334400495714138"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = evaluator.evaluate(test_pred)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1095.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1032.3 KiB\n",
      "+---+-----+\n",
      "|MSE|count|\n",
      "+---+-----+\n",
      "|0.0| 8787|\n",
      "|1.0| 5825|\n",
      "|4.0|  780|\n",
      "|3.0|  760|\n",
      "|2.0|  860|\n",
      "|5.0|  843|\n",
      "|7.0|  434|\n",
      "|6.0|  545|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Always postive metric\n",
    "mean_square_error_val_score = val_pred.select(\"label\", \"prediction\").withColumn(\"MSE\" , ((col(\"label\") - col(\"prediction\")) ** 2) ** 0.5)\n",
    "mean_square_error_val_score.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| MSE|count|\n",
      "+----+-----+\n",
      "| 0.0| 8726|\n",
      "|-1.0| 4617|\n",
      "| 1.0| 1210|\n",
      "|-4.0|    3|\n",
      "|-2.0|   57|\n",
      "|-3.0|   13|\n",
      "| 3.0|  746|\n",
      "| 2.0|  796|\n",
      "| 4.0|  764|\n",
      "| 5.0|  875|\n",
      "| 7.0|  498|\n",
      "| 6.0|  529|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dispersed metric\n",
    "mean_square_error_val_score_disp = val_pred.select(\"label\", \"prediction\").withColumn(\"MSE\" , (col(\"label\") - col(\"prediction\")))\n",
    "mean_square_error_val_score_disp.groupBy(\"MSE\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICTING FOR THE ENTIRE DATATSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_full(data: DataFrame, drop: list):\n",
    "    \"\"\"\n",
    "    Function to vectorize all the processed data\n",
    "    \"\"\"\n",
    "    return VectorAssembler(\n",
    "        inputCols= [c for c in data.columns if not c in drop],\n",
    "        outputCol=\"features\"\n",
    "    ).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(user_id=3, merchant_abn=60956456424, dollar_value=136.67999267578125, order_datetime=datetime.date(2021, 8, 20), Natural_var=0, Potential_Outlier=0, BNPL_Fee=4.69, avg_monthly_inc=-4.238095283508301, monthly_entropy=2.985382318496704, postcode_entropy=7.979236125946045, revenue=8026969.561502457, postcode=862, Number of individuals lodging an income tax return=1099, Average taxable income or loss=56030, Median taxable income or loss=45125, Proportion with salary or wages=1, Count salary or wages=821, Average salary or wages=56184, Median salary or wages=49641, Proportion with net rent=1, Count net rent=155, Average net rent=950, Median net rent=818, Average total income or loss=58776, Median total income or loss=47279, Average total deductions=2575, Median total deductions=695, Proportion with total business income=1, Count total business income=144, Average total business income=64171, Median total business income=25168, Proportion with total business expenses=1, Count total business expenses=141, Average total business expenses=47773, Median total business expenses=14002, Proportion with net tax=1, Count net tax=795, Average net tax=16719, Median net tax=11089, Count super total accounts balance=1227, Average super total accounts balance=187532, Median super total accounts balance=80365, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {1: 1.0}), state_encoded=SparseVector(7, {6: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = transactions.join(merchants, on=\"merchant_abn\").join(customers, on=\"user_id\")\n",
    "\n",
    "# X = X.drop(\"user_id\", \"merchant_abn\", \"order_datetime\", \"order_id\", \"name\", \"postcode\", \"holiday\")\n",
    "full = full.drop(\"name\", \"order_id\", \"holiday\")\n",
    "category_full = cat_transformer.transform(full)\n",
    "\n",
    "categories = [\n",
    "    \"dayofmonth\",\n",
    "    \"dayofweek\",\n",
    "    \"month\",\n",
    "    \"tags\",\n",
    "    \"state\",\n",
    "    \"gender\",\n",
    "    \"Earnings_Class\"\n",
    "]\n",
    "\n",
    "for c in categories:\n",
    "    category_full = category_full.drop(c).drop(c+\"_index\")\n",
    "\n",
    "category_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At numerical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Vectorize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, merchant_abn=60956456424, order_datetime=datetime.date(2021, 8, 20), Natural_var=0, Potential_Outlier=0, postcode=862, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {1: 1.0}), state_encoded=SparseVector(7, {6: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), scaled=DenseVector([0.2962, -0.824, 2.6978, 143.6152, 15.7495, 2.3418, 0.1505, 3.2388, 5.1963, 0.0, 0.1382, 4.646, 5.7305, 49.2032, 0.1392, 0.3585, 0.5033, 3.2237, 5.3209, 1.4794, 3.0258, 42.7062, 0.1964, 1.2211, 2.11, 34.5515, 0.2057, 0.909, 1.0993, 0.0, 0.141, 2.0222, 3.4683, 0.1507, 2.8041, 3.5449]), features=SparseVector(122, {19: 1.0, 32: 1.0, 44: 1.0, 50: 1.0, 79: 1.0, 81: 1.0, 83: 1.0, 86: 0.2962, 87: -0.824, 88: 2.6978, 89: 143.6152, 90: 15.7495, 91: 2.3418, 92: 0.1505, 93: 3.2388, 94: 5.1963, 96: 0.1382, 97: 4.646, 98: 5.7305, 99: 49.2032, 100: 0.1392, 101: 0.3585, 102: 0.5033, 103: 3.2237, 104: 5.3209, 105: 1.4794, 106: 3.0258, 107: 42.7062, 108: 0.1964, 109: 1.2211, 110: 2.11, 111: 34.5515, 112: 0.2057, 113: 0.909, 114: 1.0993, 116: 0.141, 117: 2.0222, 118: 3.4683, 119: 0.1507, 120: 2.8041, 121: 3.5449}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"At bucketing\")\n",
    "#buckets_full = buckets.transform(category_full)\n",
    "print(\"At numerical\")\n",
    "numerical_full = process_numerical(category_full)\n",
    "\n",
    "print(\"At Vectorize\")\n",
    "full = vectorize_full(numerical_full, [\"user_id\", \"merchant_abn\", \"order_datetime\", \"postcode\"])\n",
    "\n",
    "full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full.write.parquet(\"../models/full_raw\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, merchant_abn=60956456424, order_datetime=datetime.date(2021, 8, 20), Natural_var=0, Potential_Outlier=0, postcode=862, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {1: 1.0}), state_encoded=SparseVector(7, {6: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), scaled=DenseVector([0.2962, -0.824, 2.6978, 143.6152, 15.7495, 2.3418, 0.1505, 3.2388, 5.1963, 0.0, 0.1382, 4.646, 5.7305, 49.2032, 0.1392, 0.3585, 0.5033, 3.2237, 5.3209, 1.4794, 3.0258, 42.7062, 0.1964, 1.2211, 2.11, 34.5515, 0.2057, 0.909, 1.0993, 0.0, 0.141, 2.0222, 3.4683, 0.1507, 2.8041, 3.5449]), features=SparseVector(122, {19: 1.0, 32: 1.0, 44: 1.0, 50: 1.0, 79: 1.0, 81: 1.0, 83: 1.0, 86: 0.2962, 87: -0.824, 88: 2.6978, 89: 143.6152, 90: 15.7495, 91: 2.3418, 92: 0.1505, 93: 3.2388, 94: 5.1963, 96: 0.1382, 97: 4.646, 98: 5.7305, 99: 49.2032, 100: 0.1392, 101: 0.3585, 102: 0.5033, 103: 3.2237, 104: 5.3209, 105: 1.4794, 106: 3.0258, 107: 42.7062, 108: 0.1964, 109: 1.2211, 110: 2.11, 111: 34.5515, 112: 0.2057, 113: 0.909, 114: 1.0993, 116: 0.141, 117: 2.0222, 118: 3.4683, 119: 0.1507, 120: 2.8041, 121: 3.5449}))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full prediction\n",
    "full = sp.read.option(\"inferSchema\", True).parquet(\"../models/full_raw/\")\n",
    "full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:41:48 WARN DAGScheduler: Broadcasting large task binary with size 1049.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, merchant_abn=60956456424, order_datetime=datetime.date(2021, 8, 20), Natural_var=0, Potential_Outlier=0, postcode=862, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {1: 1.0}), state_encoded=SparseVector(7, {6: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), scaled=DenseVector([0.2962, -0.824, 2.6978, 143.6152, 15.7495, 2.3418, 0.1505, 3.2388, 5.1963, 0.0, 0.1382, 4.646, 5.7305, 49.2032, 0.1392, 0.3585, 0.5033, 3.2237, 5.3209, 1.4794, 3.0258, 42.7062, 0.1964, 1.2211, 2.11, 34.5515, 0.2057, 0.909, 1.0993, 0.0, 0.141, 2.0222, 3.4683, 0.1507, 2.8041, 3.5449]), features=SparseVector(122, {19: 1.0, 32: 1.0, 44: 1.0, 50: 1.0, 79: 1.0, 81: 1.0, 83: 1.0, 86: 0.2962, 87: -0.824, 88: 2.6978, 89: 143.6152, 90: 15.7495, 91: 2.3418, 92: 0.1505, 93: 3.2388, 94: 5.1963, 96: 0.1382, 97: 4.646, 98: 5.7305, 99: 49.2032, 100: 0.1392, 101: 0.3585, 102: 0.5033, 103: 3.2237, 104: 5.3209, 105: 1.4794, 106: 3.0258, 107: 42.7062, 108: 0.1964, 109: 1.2211, 110: 2.11, 111: 34.5515, 112: 0.2057, 113: 0.909, 114: 1.0993, 116: 0.141, 117: 2.0222, 118: 3.4683, 119: 0.1507, 120: 2.8041, 121: 3.5449}), rawPrediction=DenseVector([26.8525, 45.1699, 6.4048, 3.5018, 3.3815, 3.4577, 4.6198, 3.0566, 2.9752, 0.5804]), probability=DenseVector([0.2685, 0.4517, 0.064, 0.035, 0.0338, 0.0346, 0.0462, 0.0306, 0.0298, 0.0058]), prediction=1.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rf_pred = model.transform(full.drop(\"rawPrediction\", \"probability\", \"prediction\"))\n",
    "full_rf_pred.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:41:52 WARN DAGScheduler: Broadcasting large task binary with size 1003.9 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(user_id=3, merchant_abn=60956456424, order_datetime=datetime.date(2021, 8, 20), Natural_var=0, Potential_Outlier=0, postcode=862, dayofmonth_encoded=SparseVector(30, {17: 1.0}), dayofweek_encoded=SparseVector(6, {0: 1.0}), month_encoded=SparseVector(11, {6: 1.0}), tags_encoded=SparseVector(24, {1: 1.0}), state_encoded=SparseVector(7, {6: 1.0}), gender_encoded=SparseVector(2, {1: 1.0}), Earnings_Class_encoded=SparseVector(4, {1: 1.0}), prediction=1.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_rf_pred = full_rf_pred.drop(\"scaled\", \"features\", \"rawPrediction\", \"probability\")\n",
    "full_rf_pred.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 02:41:55 WARN DAGScheduler: Broadcasting large task binary with size 1210.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_rf_pred.write.parquet(\"../models/random_forest_output_full\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('virtual-p2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf6849838b3a8621666e21fdc4cc1583090fffb5f1906a909fbc1c95ae1bb65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
