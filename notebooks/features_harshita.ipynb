{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unregistered customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import countDistinct, col\n",
    "import pyspark.sql.functions as F\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:10:36 WARN Utils: Your hostname, Harshitas-MacBook-Air-8.local resolves to a loopback address: 127.0.0.1; using 10.13.133.97 instead (on interface en0)\n",
      "22/10/04 16:10:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:10:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sp = SparkSession.builder.appName(\"Fraud detection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read relevant datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_folder(trans_group: str):\n",
    "    \"\"\"\n",
    "    Function to merge everything within yellow or green or fhvhv\n",
    "    \"\"\"\n",
    "    dir = \"../data/tables/\" + trans_group +\"/\"\n",
    "    folder_locs = os.listdir(dir)\n",
    "\n",
    "    group_list = []\n",
    "    for folder in folder_locs:\n",
    "        path = dir + \"/\" + folder\n",
    "        if os.path.isdir(path):\n",
    "            # print(\"At current path\", path)\n",
    "            group_list.append(sp.read.parquet(path))\n",
    "\n",
    "    return reduce(DataFrame.unionAll, group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20210228_20210827_snapshot/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20210828_20220227_snapshot/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20220228_20220828_snapshot/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dir = \"../data/tables/\"\n",
    "groups = [\"transactions_20210228_20210827_snapshot/\", \"transactions_20210828_20220227_snapshot/\", \"transactions_20220228_20220828_snapshot/\"]\n",
    "\n",
    "final_list = []\n",
    "for g in groups:\n",
    "    print(\"Started group: \", g)\n",
    "    final_list.append(sp.read.parquet(dir + g))\n",
    "\n",
    "transactions = reduce(DataFrame.unionAll, final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merch = sp.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "cust  = sp.read.option(\"header\", True).option(\"delimiter\", \"|\") \\\n",
    "        .csv(\"../data/tables/tbl_consumer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unregistered_customers(merchants, customers, transactions):\n",
    "    '''\n",
    "    Args:\n",
    "        merchants (pyspark.sql.DataFrame)    : Df with details about all the  merchants, including their 'merchant_abn'\n",
    "\n",
    "        customers (pyspark.sql.DataFrame)    : Df with details about all the customers, including their 'consumer_id'\n",
    "\n",
    "        transactions (pyspark.sql.DataFrame) : Df with details about all the transactions made between merchants and customers\n",
    "\n",
    "    Returns:\n",
    "        A pyspark.sql.DataFrame with all the transactions that have a registered Merchant ABN but an unknown user/customer ID.\n",
    "    '''\n",
    "    \n",
    "    # list of registered merchant ABNs\n",
    "    abn_list = merchants.rdd.map(lambda x: x.merchant_abn).collect()\n",
    "\n",
    "    # transactions with registered merchant ABNs\n",
    "    reg_merchant_trans = transactions[transactions.merchant_abn.isin(abn_list)]\n",
    "\n",
    "    # total transactions with unidentified customers\n",
    "    unknown_cust = (transactions.select('user_id').distinct()) \\\n",
    "                    .subtract(cust.select(col('consumer_id')))\n",
    "    unknown_cust_list = unknown_cust.rdd.map(lambda x: x.user_id).collect()\n",
    "\n",
    "    # transactions with registered merchant ABNs but unknown customer IDs\n",
    "    return reg_merchant_trans[reg_merchant_trans.user_id.isin(unknown_cust_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(unknown_cust_trans, merchants):\n",
    "    '''\n",
    "    Args:\n",
    "        unknown_cust_trans (pyspark.sql.DataFrame) : Df with all the transactions that have a registered Merchant ABN but an unknown user/customer ID.\n",
    "        \n",
    "        merchants (pyspark.sql.DataFrame)          : Df with details about all the  merchants, including their 'merchant_abn'\n",
    "\n",
    "    Returns:\n",
    "        Updated 'merchants' df with two new columns.\n",
    "    '''\n",
    "\n",
    "    # number of transactions with unknown users for each merchant \n",
    "    trans_count = unknown_cust_trans.groupBy(\"merchant_abn\").count() \\\n",
    "                    .withColumnRenamed(\"count\", \"unknown_users_trans\")\n",
    "\n",
    "    # number of unknown customers for each merchant\n",
    "    users_count = unknown_cust_trans.groupBy(\"merchant_abn\") \\\n",
    "                    .agg(countDistinct(\"user_id\")) \\\n",
    "                    .withColumnRenamed(\"count(user_id)\", \"unknown_users_count\")\n",
    "\n",
    "    # add relevant counts as new columns to the merchant dataset\n",
    "    merchants = merchants.join(trans_count, [\"merchant_abn\"])\n",
    "    merchants = merchants.join(users_count, [\"merchant_abn\"])\n",
    "\n",
    "    return merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features to merchant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unreg_custs = unregistered_customers(merch, cust, transactions)\n",
    "merch = create_columns(unreg_custs, merch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1205.4 KiB\n",
      "22/10/04 16:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1200.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                (0 + 8) / 26][Stage 49:>                 (0 + 0) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:50:43 WARN DAGScheduler: Broadcasting large task binary with size 1191.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:50:51 WARN DAGScheduler: Broadcasting large task binary with size 1202.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:50:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:51:00 WARN DAGScheduler: Broadcasting large task binary with size 1198.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|merchant_abn|                name|                tags|     avg_monthly_inc|unknown_users_trans|unknown_users_count|\n",
      "+------------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "| 12516851436|        Mollis Corp.|((watch, clock, a...|                -0.2|                134|                134|\n",
      "| 15613631617|     Ante Industries|[[motor vehicle s...|                 0.0|               1211|               1162|\n",
      "| 19839532017|Pellentesque Habi...|([cable, Satellit...|-0.04761904761904...|                494|                484|\n",
      "| 15700338102| Aliquam Ornare Inc.|((furniture, home...|                -0.3|                148|                148|\n",
      "| 20497101151|Arcu Vestibulum A...|[[telecom], [b], ...|               -0.15|                173|                172|\n",
      "+------------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merch.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Customer base growth\n",
    "The monthly increase in the number of customers per merchant, used to quantify business growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cust_growth_column(merchants, transactions):\n",
    "    '''\n",
    "    Args:\n",
    "        merchants (pyspark.sql.DataFrame)    : Df with details about all the  merchants, including their 'merchant_abn'\n",
    "\n",
    "        transactions (pyspark.sql.DataFrame) : Df with details about all the transactions made between merchants and customers\n",
    "\n",
    "    Returns:\n",
    "        Updated 'merchants' df with one new column.\n",
    "    '''\n",
    "\n",
    "    # add monthly customer increase as a new column to the merchant dataset\n",
    "    cust_growth = aggregate_monthly(transactions)\n",
    "    merchants = merchants.join(cust_growth, [\"merchant_abn\"])\n",
    "\n",
    "    return merchants\n",
    "\n",
    "\n",
    "def aggregate_monthly(transactions):\n",
    "    '''\n",
    "    Args:\n",
    "        transactions (pyspark.sql.DataFrame) : Df with details about all the transactions made between merchants and customers\n",
    "\n",
    "    Returns:\n",
    "        A pyspark.sql.DataFrame with the average monthly increase in the number of customer for every merchant_abn\n",
    "    '''\n",
    "    monthly_trans = transactions.withColumn(\"order_month\", \n",
    "                                F.date_format('order_datetime','yyyy-MM'))\n",
    "    monthly = monthly_trans.groupBy(\"merchant_abn\", \"order_month\").agg(countDistinct('user_id')).withColumnRenamed(\"count(user_id)\", \"distinct_customers\")\n",
    "    sorted_monthly = monthly.sort(['merchant_abn', 'order_month'])\n",
    "\n",
    "    return get_monthly_increase(sorted_monthly.toPandas())\n",
    "    \n",
    "\n",
    "def get_monthly_increase(monthly_df):\n",
    "    '''\n",
    "    Args:\n",
    "        monthly_df (pandas.DataFrame) : Df with the distinct number of customers that made transactions with a particular merchant every month\n",
    "\n",
    "    Returns:\n",
    "        A pyspark.sql.DataFrame with the average monthly increase in the number of customer for every merchant_abn\n",
    "    '''\n",
    "    curr_abn = monthly_df['merchant_abn'][0]\n",
    "    differences = []\n",
    "    abns = []\n",
    "    incs = []\n",
    "    for i in range(monthly_df.shape[0] - 1):\n",
    "        if monthly_df['merchant_abn'][i] != curr_abn:\n",
    "            abns.append(curr_abn)\n",
    "            incs.append(sum(differences) / len(differences))\n",
    "\n",
    "            curr_abn = monthly_df['merchant_abn'][i]\n",
    "            differences = []\n",
    "\n",
    "        differences.append(monthly_df['distinct_customers'][i+1] - monthly_df['distinct_customers'][i])\n",
    "\n",
    "    growth = pd.DataFrame.from_dict({\"merchant_abn\": abns, \"avg_monthly_inc\": incs})\n",
    "    return sp.createDataFrame(growth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 16:48:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merch = create_cust_growth_column(merch, transactions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
