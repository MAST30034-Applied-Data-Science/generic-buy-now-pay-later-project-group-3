{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unregistered customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import countDistinct, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:21:11 WARN Utils: Your hostname, Harshitas-MacBook-Air-8.local resolves to a loopback address: 127.0.0.1; using 192.168.0.227 instead (on interface en0)\n",
      "22/10/04 00:21:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/04 00:21:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sp = SparkSession.builder.appName(\"Fraud detection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read relevant datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_folder(trans_group: str):\n",
    "    \"\"\"\n",
    "    Function to merge everything within yellow or green or fhvhv\n",
    "    \"\"\"\n",
    "    dir = \"../data/tables/\" + trans_group +\"/\"\n",
    "    folder_locs = os.listdir(dir)\n",
    "\n",
    "    group_list = []\n",
    "    for folder in folder_locs:\n",
    "        path = dir + \"/\" + folder\n",
    "        if os.path.isdir(path):\n",
    "            # print(\"At current path\", path)\n",
    "            group_list.append(sp.read.parquet(path))\n",
    "\n",
    "    return reduce(DataFrame.unionAll, group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20210228_20210827_snapshot/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20210828_20220227_snapshot/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started group:  transactions_20220228_20220828_snapshot/\n"
     ]
    }
   ],
   "source": [
    "dir = \"../data/tables/\"\n",
    "groups = [\"transactions_20210228_20210827_snapshot/\", \"transactions_20210828_20220227_snapshot/\", \"transactions_20220228_20220828_snapshot/\"]\n",
    "\n",
    "final_list = []\n",
    "for g in groups:\n",
    "    print(\"Started group: \", g)\n",
    "    final_list.append(sp.read.parquet(dir + g))\n",
    "\n",
    "transactions = reduce(DataFrame.unionAll, final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merch = sp.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "cust  = sp.read.option(\"header\", True).option(\"delimiter\", \"|\") \\\n",
    "        .csv(\"../data/tables/tbl_consumer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unregistered_customers(merchants, customers, transactions):\n",
    "    '''\n",
    "    Args:\n",
    "        merchants (pyspark.sql.DataFrame)    : Df with details about all the  merchants, including their 'merchant_abn'\n",
    "\n",
    "        customers (pyspark.sql.DataFrame)    : Df with details about all the customers, including their 'consumer_id'\n",
    "\n",
    "        transactions (pyspark.sql.DataFrame) : Df with details about all the transactions made between merchants and customers\n",
    "\n",
    "    Returns:\n",
    "        A pyspark.sql.DataFrame with all the transactions that have a registered Merchant ABN but an unknown user/customer ID.\n",
    "    '''\n",
    "    \n",
    "    # list of registered merchant ABNs\n",
    "    abn_list = merchants.rdd.map(lambda x: x.merchant_abn).collect()\n",
    "\n",
    "    # transactions with registered merchant ABNs\n",
    "    reg_merchant_trans = transactions[transactions.merchant_abn.isin(abn_list)]\n",
    "\n",
    "    # total transactions with unidentified customers\n",
    "    unknown_cust = (transactions.select('user_id').distinct()) \\\n",
    "                    .subtract(cust.select(col('consumer_id')))\n",
    "    unknown_cust_list = unknown_cust.rdd.map(lambda x: x.user_id).collect()\n",
    "\n",
    "    # transactions with registered merchant ABNs but unknown customer IDs\n",
    "    return reg_merchant_trans[reg_merchant_trans.user_id.isin(unknown_cust_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(unknown_cust_trans, merchants):\n",
    "    '''\n",
    "    Args:\n",
    "        unknown_cust_trans (pyspark.sql.DataFrame) : Df with all the transactions that have a registered Merchant ABN but an unknown user/customer ID.\n",
    "        \n",
    "        merchants (pyspark.sql.DataFrame)          : Df with details about all the  merchants, including their 'merchant_abn'\n",
    "\n",
    "    Returns:\n",
    "        Updated 'merchants' df with two new columns.\n",
    "    '''\n",
    "\n",
    "    # number of transactions with unknown users for each merchant \n",
    "    trans_count = unknown_cust_trans.groupBy(\"merchant_abn\").count() \\\n",
    "                    .withColumnRenamed(\"count\", \"unknown_users_trans\")\n",
    "\n",
    "    # number of unknown customers for each merchant\n",
    "    users_count = unknown_cust_trans.groupBy(\"merchant_abn\") \\\n",
    "                    .agg(countDistinct(\"user_id\")) \\\n",
    "                    .withColumnRenamed(\"count(user_id)\", \"unknown_users_count\")\n",
    "\n",
    "    # add relevant counts as new columns to the merchant dataset\n",
    "    merchants = merchants.join(trans_count, [\"merchant_abn\"])\n",
    "    merchants = merchants.join(users_count, [\"merchant_abn\"])\n",
    "\n",
    "    return merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features to merchant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unreg_custs = unregistered_customers(merch, cust, transactions)\n",
    "merch = create_columns(unreg_custs, merch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:23:05 WARN DAGScheduler: Broadcasting large task binary with size 1205.4 KiB\n",
      "22/10/04 00:23:06 WARN DAGScheduler: Broadcasting large task binary with size 1200.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:23:20 WARN DAGScheduler: Broadcasting large task binary with size 1202.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/10/04 00:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1198.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 00:23:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "+------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|merchant_abn|                name|                tags|unknown_users_trans|unknown_users_count|\n",
      "+------------+--------------------+--------------------+-------------------+-------------------+\n",
      "| 38700038932|Etiam Bibendum In...|[(tent and awning...|               4715|               4090|\n",
      "| 19839532017|Pellentesque Habi...|([cable, Satellit...|                494|                484|\n",
      "| 35344855546|Quis Tristique Ac...|[(watch, clock, a...|                984|                957|\n",
      "| 83412691377|Suspendisse Sagit...|([watch, clock, a...|               9408|               7146|\n",
      "| 15613631617|     Ante Industries|[[motor vehicle s...|               1211|               1162|\n",
      "+------------+--------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merch.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
