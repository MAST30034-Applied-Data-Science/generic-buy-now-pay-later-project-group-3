{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant Ranking Algorithm\n",
    "The method of ranking each merchant, will take inspiration from the methods proposed in (https://sapinsider.org/leveraging-analytical-method-for-ranking-suppliers/), in which we scale each sub_attribute with min-max norminalisation, then sum them together to get the overall score for a particular Metric\n",
    "\n",
    "The Key Attributes/Metrics are: \n",
    "- Finantial \n",
    "- Customer_Base \n",
    "- Sustainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import feature as H\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import coalesce, col\n",
    "\n",
    "import re, sys, string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_spark():\n",
    "    spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate())\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This value can be changed to point to the directory where all the tables are stored. By default, it's the data folder in the generic-buy-now-pay-later directory.\n",
    "\n",
    "dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_standardisation(dataset, max_columns, min_columns = []): \n",
    "    \"\"\"\n",
    "    Function for scaling every individual score\n",
    "    \"\"\"\n",
    "    # if higher values are preferred\n",
    "    for col_name in max_columns:\n",
    "        values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "        dataset = dataset.withColumn(col_name, F.round((F.col(col_name) - values.select('low').head()[0]) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    # if lower values are preferred\n",
    "    if min_columns:\n",
    "        for col_name in min_columns:\n",
    "            values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "            dataset = dataset.withColumn(col_name, F.round((values.select('high').head()[0] - F.col(col_name)) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Financial Metric\n",
    "For this we will look into:\n",
    "- Total_Revenue\n",
    "- Average_Revenue_Growth\n",
    "\n",
    "These are both taken over the last 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revenue_growth_score(revenue_data):\n",
    "    \"\"\"\n",
    "    Function to calculate revenue growth per merchant\n",
    "    \"\"\"\n",
    "    revenue_data_table = revenue_data.select('merchant_abn').distinct()\n",
    "    \n",
    "    months = [3,4,5,6,7,8,9]\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "    revenue_data_table = revenue_data_table.fillna(1)\n",
    "\n",
    "    # Now need to calculate growth rate for each month\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_4', (F.col('Month_4') - F.col('Month_3')) / F.col('Month_4'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_5', (F.col('Month_5') - F.col('Month_4')) / F.col('Month_5'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "\n",
    "    revenue_data_table = revenue_data_table.withColumn('Revenue_Growth_Avg', F.round((F.col('Growth_4') + F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9')) / 6 , 4))\n",
    "    revenue_data_table = revenue_data_table.select('merchant_abn', 'Revenue_Growth_Avg')\n",
    "    return revenue_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_score(RECENCY, transactions, fraud_data):\n",
    "    \"\"\"\n",
    "    Function to score each merchant on their financial aspects\n",
    "    \"\"\"\n",
    "    revenue_data = transactions.where(F.col('order_datetime') > RECENCY).select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue')\n",
    "    \n",
    "    revenue_data = revenue_data.join(fraud_data, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    # Firstly, calculate the revenue score and weight the revenue with probability\n",
    "    revenue_data = revenue_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction')))\n",
    "\n",
    "    # Now can calculate each Merchant's revenue\n",
    "    revenue_final = revenue_data.groupBy('merchant_abn').agg(F.round(F.sum('BNPL_weighted_Revenue'), 2).alias('Total_Revenue'))\n",
    "\n",
    "    # Next, get the revenue growth score \n",
    "    revenue_final = revenue_final.join(revenue_growth_score(revenue_data), on='merchant_abn', how='left')\n",
    "\n",
    "    # Finally scale\n",
    "    revenue_final = feature_standardisation(revenue_final, ['Revenue_Growth_Avg', 'Total_Revenue'])\n",
    "    \n",
    "    revenue_final.write.parquet('../' + dir + 'curated/Metric_Finantial_scaled', mode ='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability \n",
    "- Experience\n",
    "- Customer Growth (Last 6 months)\n",
    "- Postcode_Entropy\n",
    "- Industry score (added when scoring each metric [next notebook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_growth(full_dataset):\n",
    "    \"\"\"\n",
    "    Function to calculate customer growth for each merchant\n",
    "    \"\"\"\n",
    "    Unique_cust = full_dataset.where(F.col('order_datetime') < F.lit('2022-05-01')).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'))\n",
    "\n",
    "    # now for the past 6 monts calculate \n",
    "    months = [5,6,7,8,9,10]\n",
    "    customer_counting_data = full_dataset.select('merchant_abn', 'user_id', 'order_datetime', 'month')\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "            \n",
    "    Unique_cust = Unique_cust.fillna(1)\n",
    "\n",
    "    # Now need to calculate growth rate for each month\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_5', F.col('Unique_Customers') / F.col('Month_5'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_10',(F.col('Month_10') - F.col('Month_9')) / F.col('Month_10'))\n",
    "\n",
    "    # Finally get the average customer growth rate over the last 6 months\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.round((F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9') + F.col('Growth_10')) / 6 , 4))\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.when(F.col('Customer_Growth_Avg').isNull(), 0).otherwise(F.col('Customer_Growth_Avg')))\n",
    "    return Unique_cust.select('merchant_abn', 'Customer_Growth_Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sustainability_score(merchants, full_dataset):\n",
    "    merchants_sub = merchants.select(['merchant_abn', 'postcode_entropy'])\n",
    "    merchants_sub = merchants_sub.join(customer_growth(full_dataset), on='merchant_abn', how='left')\n",
    "\n",
    "    # calculate experience\n",
    "    number_of_dates_by_merchant = full_dataset.groupBy('merchant_abn','order_datetime').count()\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.drop('order_datetime')\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.groupBy('merchant_abn').sum('count')\n",
    "    \n",
    "    merchants_sub = merchants_sub.join(number_of_dates_by_merchant.withColumnRenamed('sum(count)','Total_Business_Days'), on='merchant_abn', how='left')\n",
    "    merchants_final = feature_standardisation(merchants_sub, ['Total_Business_Days', 'Customer_Growth_Avg'], ['postcode_entropy'])\n",
    "    \n",
    "    merchants_final.write.parquet(dir + 'curated/Metric_Sustainability_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Base\n",
    "In this sections we use the features:\n",
    "- Transaction_per_User\n",
    "- customer_wealth\n",
    "- Unique_Customers\n",
    "- Loyal_AVG\n",
    "- postcode_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loyalty(full_dataset):\n",
    "    \"\"\"\n",
    "    Function to create loyalty feature for customer base score calculation\n",
    "    \"\"\"\n",
    "    grouped = full_dataset.groupBy(\"user_id\", \"merchant_abn\")\n",
    "    RPR = grouped.count().withColumnRenamed(\"count\", \"RPR\")\n",
    "    upSell = RPR.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"upsell\")\n",
    "    CLV = grouped.sum(\"dollar_value\").withColumnRenamed(\"sum(dollar_value)\", \"CLV\")\n",
    "\n",
    "    # Define the window\n",
    "    w = Window.partitionBy([\"user_id\", \"merchant_abn\"]).orderBy(\"order_datetime\")\n",
    "\n",
    "    retention = full_dataset.withColumn(\n",
    "        'diff',\n",
    "        F.datediff(F.col(\"order_datetime\"), F.lag(\"order_datetime\").over(w))\n",
    "    ).groupBy(\"user_id\", \"merchant_abn\").agg(F.avg(F.col(\"diff\")).alias(\"retention\"))\n",
    "\n",
    "    loyal = retention.na.fill(value=365).join(RPR, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(CLV, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(upSell, on=[\"user_id\"], how=\"left\")\n",
    "\n",
    "    loyal = loyal.withColumn(\"loyal\", F.col(\"RPR\") * F.col(\"CLV\") * F.col(\"upSell\") / F.col(\"retention\"))\n",
    "\n",
    "    loyal = loyal.select(*(col(c).cast(\"float\").alias(c) for c in loyal.columns))\n",
    "\n",
    "    loyal = loyal.select('user_id', 'merchant_abn', 'loyal')\n",
    "\n",
    "    magic_percentile = F.expr('percentile_approx(loyal, 0.5)')\n",
    "    loyal = loyal.join(loyal.groupBy('user_id').agg(magic_percentile.alias('med_val')), on='user_id', how='left')\n",
    "    loyal = loyal.withColumn('loyal', coalesce('loyal', 'med_val')).select(['user_id', 'merchant_abn', 'loyal'])\n",
    "    loyal_agg = loyal.groupBy('merchant_abn').agg(F.round(F.avg('loyal'), 2).alias('Loyal_AVG'))\n",
    "    \n",
    "    return loyal_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_entropy(full_dataset):\n",
    "    # Get customer Entropy \n",
    "    cust_entro = full_dataset.select('merchant_abn', 'user_id', 'postcode', 'order_datetime')\n",
    "    Entropy_cust = cust_entro.select('merchant_abn').toDF('merchant_abn').drop_duplicates().crossJoin(cust_entro.select('postcode').toDF('postcode').drop_duplicates())\n",
    "    Entropy_cust = Entropy_cust.join(cust_entro.groupBy('merchant_abn', 'postcode').agg(F.countDistinct('user_id').alias('Count')), on=['merchant_abn', 'postcode'], how='left')\n",
    "    Entropy_cust = Entropy_cust.fillna(1)\n",
    "    Entropy_cust = Entropy_cust.join(Entropy_cust.groupBy(\"merchant_abn\").sum('Count'), on='merchant_abn', how='left')\n",
    "    Entropy_cust = Entropy_cust.withColumn('Probability', F.col('Count') / F.col('sum(Count)'))\n",
    "    Entropy_cust = Entropy_cust.groupBy(\"merchant_abn\").agg((-F.sum(F.col(\"Probability\") * F.log2(col(\"Probability\")))).alias(\"Customer_Entropy\"))\n",
    "    return Entropy_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_score(full_dataset, customers):\n",
    "    full_dataset = full_dataset.join(customers, on='user_id')\n",
    "    # add some of the attributes\n",
    "    cust_data = full_dataset.groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'), F.round(F.count('user_id') / F.countDistinct('user_id'), 2).alias('Transaction_per_User'), F.round(F.avg('Average taxable income or loss'),2).alias('customer_wealth'))\n",
    "    # add the loyalty score\n",
    "    cust_data = cust_data.join(loyalty(full_dataset), on='merchant_abn')\n",
    "    cust_data = cust_data.join(customer_entropy(full_dataset), on='merchant_abn', how='left')\n",
    "    cust_final = feature_standardisation(cust_data, ['Transaction_per_User', 'customer_wealth', 'Unique_Customers', 'Loyal_AVG'], ['Customer_Entropy'])\n",
    "    # save file \n",
    "    cust_final.write.parquet(dir + 'curated/Metric_Customer_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry/Environment \n",
    "We add a general score, based on their description tags, in relation to which industry they belong to, including:\n",
    "- Market Dominance\n",
    "- Survival Rate\n",
    "- Fraud_Prob_Avg\n",
    "- Customer_Base\n",
    "- Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_transformation(spark):\n",
    "    Administrative_Support_Services= [\"equipment tool furniture appliance rent al leasing\",\n",
    "                                      \"florist supply nursery stock flower\",\n",
    "                                      \"lawn garden supply outlet including nursery\"]\n",
    "    Personal_Services= [\"shoe shop\",\n",
    "                        \"gift card novelty souvenir shop\",\n",
    "                        \"antique shop sale repair restoration service\",\n",
    "                        \"watch clock jewelry repair shop\",\n",
    "                        \"jewelry watch clock silverware shop\",\n",
    "                        \"motor vehicle supply new part\",\n",
    "                        \"furniture home furnishing equipment shop manufacturer except appliance\",\n",
    "                        \"tent awning shop\",\n",
    "                        \"optician optical good eyeglass\"]\n",
    "    Arts_Recreation_Services = [\"digital good book movie music\",\n",
    "                                \"music shop musical instrument piano sheet music\",\n",
    "                                \"health beauty spa\",\n",
    "                                \"bicycle shop sale service\",\n",
    "                                \"art dealer gallery\",\n",
    "                                \"hobby toy game shop\",\n",
    "                                \"stationery office supply printing writing paper\"]\n",
    "    Information_Media_Telecommunications = [\"telecom\",\n",
    "                                            \"computer programming data processing integrated system design service\",\n",
    "                                            \"book periodical newspaper\",\n",
    "                                            \"artist supply craft shop\",\n",
    "                                            \"computer computer peripheral equipment software\",\n",
    "                                            \"cable satellite pay television radio service\"]\n",
    "    desc = []\n",
    "    tags = []\n",
    "    for ele in Administrative_Support_Services:\n",
    "        desc.append(\"Administrative_Support_Services\")\n",
    "    tags += Administrative_Support_Services\n",
    "    for ele in Personal_Services:\n",
    "        desc.append(\"Personal_Services\")\n",
    "    tags += Personal_Services\n",
    "    for ele in Arts_Recreation_Services:\n",
    "        desc.append(\"Arts_Recreation_Services\")\n",
    "    tags += Arts_Recreation_Services\n",
    "    for ele in Information_Media_Telecommunications:\n",
    "        desc.append(\"Information_Media_Telecommunications\")\n",
    "    tags += Information_Media_Telecommunications\n",
    "\n",
    "    to_df = {\n",
    "        \"sector\" :  desc,\n",
    "        \"Description\" : tags\n",
    "    }\n",
    "    sectors = spark.createDataFrame(pd.DataFrame(to_df))\n",
    "    # save sectors\n",
    "    return sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry(full_dataset, fraud_probabilities, spark, RECENCY):\n",
    "    desc_lookup = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(dir + \"/tables/description_lookup.csv\")\n",
    "    industry_data = full_dataset.join(desc_lookup, )\n",
    "    industry_data = full_dataset.select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue', 'Description')\n",
    "    # Now join fraud prob \n",
    "    industry_data = industry_data.join(fraud_probabilities, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    industry_data = industry_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction'))\n",
    "                                                                 )\n",
    "    dataset_sector = description_transformation(spark)\n",
    "    industry_data = industry_data.join(dataset_sector, on=['Description'], how='inner').drop('Description')\n",
    "    Merchant_Industry = industry_data.select('merchant_abn', 'sector').distinct()\n",
    "    # Market Dominance\n",
    "    industry_table = industry_data.where(F.col('order_datetime') > RECENCY).groupBy('sector').agg(F.sum('BNPL_weighted_Revenue').alias('Total_Weighted_Revenue'), F.count('BNPL_weighted_Revenue').alias('Total_Transactions'), F.avg('BNPL_weighted_Revenue').alias('Average_Weighted_Revenue'))\n",
    "    total_rev = industry_table.groupBy().sum().collect()[0][0]\n",
    "    total_transa = industry_table.groupBy().sum().collect()[0][0]\n",
    "    industry_table = industry_table.withColumn('Total_Weighted_Revenue', F.col('Total_Weighted_Revenue') / total_rev).withColumnRenamed('Total_Weighted_Revenue', 'Portion_of_Total_Revenue')\n",
    "    industry_table = industry_table.withColumn('Total_Transactions', F.col('Total_Transactions') / total_transa).withColumnRenamed('Total_Transactions', 'Portion_of_Total_Transactions')\n",
    "    # Survival Rate - data/tables/sector_average.parquet\n",
    "    sector_info = spark.read.parquet(dir + 'tables/sector_information.parquet')\n",
    "    industry_table = industry_table.join(sector_info.select(F.col('sector'), F.col('survival_rate').cast('double').alias(\"survival_rate\")), on='sector', how='left')\n",
    "    industry_table = feature_standardisation(industry_table, ['Portion_of_Total_Revenue', 'Portion_of_Total_Transactions', 'survival_rate', 'Average_Weighted_Revenue'])\n",
    "    # ensure assigned to each merchant\n",
    "    industry_table = Merchant_Industry.join(industry_table, on='sector', how='left')\n",
    "    # save file \n",
    "    industry_table.write.parquet(dir + 'curated/Metric_industry_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function, which can run everything\n",
    "\n",
    "spark = open_spark()\n",
    "customers = spark.read.option(\"inferSchema\", True).parquet(dir + 'processed/customers/')\n",
    "transactions = spark.read.option(\"inferSchema\", True).parquet(dir + 'processed/transactions')\n",
    "merchants = spark.read.option(\"inferSchema\", True).parquet(dir + \"processed/merchants\")\n",
    "fraud_probabilities = spark.read.parquet(dir + '../models/random_forest_output_full/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_lookup = spark.read.option(\"header\", False).csv(dir + \"/tables/description_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th></tr>\n",
       "<tr><td>furniture home fu...</td><td> 0</td></tr>\n",
       "<tr><td>cable satellite p...</td><td> 1</td></tr>\n",
       "<tr><td>jewelry watch clo...</td><td> 2</td></tr>\n",
       "<tr><td>watch clock jewel...</td><td> 3</td></tr>\n",
       "<tr><td>music shop musica...</td><td> 4</td></tr>\n",
       "<tr><td>gift card novelty...</td><td> 5</td></tr>\n",
       "<tr><td>computer computer...</td><td> 6</td></tr>\n",
       "<tr><td>computer programm...</td><td> 7</td></tr>\n",
       "<tr><td>equipment tool fu...</td><td> 8</td></tr>\n",
       "<tr><td>artist supply cra...</td><td> 9</td></tr>\n",
       "<tr><td>florist supply nu...</td><td> 10</td></tr>\n",
       "<tr><td>antique shop sale...</td><td> 11</td></tr>\n",
       "<tr><td>motor vehicle sup...</td><td> 12</td></tr>\n",
       "<tr><td>book periodical n...</td><td> 13</td></tr>\n",
       "<tr><td>stationery office...</td><td> 14</td></tr>\n",
       "<tr><td>tent awning shop</td><td> 15</td></tr>\n",
       "<tr><td>art dealer gallery</td><td> 16</td></tr>\n",
       "<tr><td>bicycle shop sale...</td><td> 17</td></tr>\n",
       "<tr><td>digital good book...</td><td> 18</td></tr>\n",
       "<tr><td>shoe shop</td><td> 19</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+---+\n",
       "|                 _c0|_c1|\n",
       "+--------------------+---+\n",
       "|furniture home fu...|  0|\n",
       "|cable satellite p...|  1|\n",
       "|jewelry watch clo...|  2|\n",
       "|watch clock jewel...|  3|\n",
       "|music shop musica...|  4|\n",
       "|gift card novelty...|  5|\n",
       "|computer computer...|  6|\n",
       "|computer programm...|  7|\n",
       "|equipment tool fu...|  8|\n",
       "|artist supply cra...|  9|\n",
       "|florist supply nu...| 10|\n",
       "|antique shop sale...| 11|\n",
       "|motor vehicle sup...| 12|\n",
       "|book periodical n...| 13|\n",
       "|stationery office...| 14|\n",
       "|    tent awning shop| 15|\n",
       "|  art dealer gallery| 16|\n",
       "|bicycle shop sale...| 17|\n",
       "|digital good book...| 18|\n",
       "|           shoe shop| 19|\n",
       "+--------------------+---+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_collection = merchants.select('merchant_abn')\n",
    "fraud_probabilities = fraud_probabilities.select('merchant_abn', 'user_id', 'order_datetime', 'prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# APPLY PREVIOUSELY SHARED & UNIMPLEMENTED FUNCTIONS\n",
    "merchants = merchants.join(desc_lookup, (merchants.tags==desc_lookup._c1)).drop('tags')\n",
    "merchants = merchants.withColumnRenamed('_c0', 'Description').drop('_c1')\n",
    "full_dataset = transactions.join(merchants, on='merchant_abn', how='inner')\n",
    "# calculate the BNPL unweighted revenue \n",
    "full_dataset = full_dataset.withColumn('BNPL_Revenue', F.col('dollar_value') * F.col('BNPL_Fee'))\n",
    "# First calculate the finantial score\n",
    "\n",
    "# We will for now determine as last 6 months (i.e march) \n",
    "RECENCY =  F.lit('2022-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>name</th><th>Earnings_Class</th><th>BNPL_Fee</th><th>avg_monthly_inc</th><th>monthly_entropy</th><th>postcode_entropy</th><th>revenue</th><th>Description</th></tr>\n",
       "<tr><td>10023283211</td><td>Felis Limited</td><td>e</td><td>0.18</td><td>-0.0952381</td><td>2.9858725</td><td>7.439226</td><td>703277.8708539009</td><td>furniture home fu...</td></tr>\n",
       "<tr><td>10142254217</td><td>Arcu Ac Orci Corp...</td><td>b</td><td>4.22</td><td>0.0</td><td>2.9779751</td><td>7.418948</td><td>118355.94002620317</td><td>cable satellite p...</td></tr>\n",
       "<tr><td>10165489824</td><td>Nunc Sed Company</td><td>b</td><td>4.4</td><td>3.0</td><td>1.609438</td><td>1.609438</td><td>56180.481201171875</td><td>jewelry watch clo...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------+--------------+--------+---------------+---------------+----------------+------------------+--------------------+\n",
       "|merchant_abn|                name|Earnings_Class|BNPL_Fee|avg_monthly_inc|monthly_entropy|postcode_entropy|           revenue|         Description|\n",
       "+------------+--------------------+--------------+--------+---------------+---------------+----------------+------------------+--------------------+\n",
       "| 10023283211|       Felis Limited|             e|    0.18|     -0.0952381|      2.9858725|        7.439226| 703277.8708539009|furniture home fu...|\n",
       "| 10142254217|Arcu Ac Orci Corp...|             b|    4.22|            0.0|      2.9779751|        7.418948|118355.94002620317|cable satellite p...|\n",
       "| 10165489824|    Nunc Sed Company|             b|     4.4|            3.0|       1.609438|        1.609438|56180.481201171875|jewelry watch clo...|\n",
       "+------------+--------------------+--------------+--------+---------------+---------------+----------------+------------------+--------------------+"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchants.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now run each score, which will save results to file\n",
    "# Note: customer score takes much linger, and sould probably run seperate\n",
    "financial_score(RECENCY, full_dataset, fraud_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>Total_Revenue</th><th>Revenue_Growth_Avg</th></tr>\n",
       "<tr><td>12516851436</td><td>0.0036</td><td>0.55</td></tr>\n",
       "<tr><td>15613631617</td><td>0.0027</td><td>0.4736</td></tr>\n",
       "<tr><td>19839532017</td><td>0.008</td><td>0.4683</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+-------------+------------------+\n",
       "|merchant_abn|Total_Revenue|Revenue_Growth_Avg|\n",
       "+------------+-------------+------------------+\n",
       "| 12516851436|       0.0036|              0.55|\n",
       "| 15613631617|       0.0027|            0.4736|\n",
       "| 19839532017|        0.008|            0.4683|\n",
       "+------------+-------------+------------------+"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_score = spark.read.parquet(dir + '/curated/Metric_Finantial_scaled')\n",
    "print(fin_score.count())\n",
    "fin_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sustainability_score(merchants, full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>postcode_entropy</th><th>Customer_Growth_Avg</th><th>Total_Business_Days</th></tr>\n",
       "<tr><td>10255988167</td><td>0.1851</td><td>0.4386</td><td>0.0029</td></tr>\n",
       "<tr><td>10430380319</td><td>0.3681</td><td>0.469</td><td>5.0E-4</td></tr>\n",
       "<tr><td>10618089367</td><td>0.0633</td><td>0.4418</td><td>0.0118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------------+-------------------+-------------------+\n",
       "|merchant_abn|postcode_entropy|Customer_Growth_Avg|Total_Business_Days|\n",
       "+------------+----------------+-------------------+-------------------+\n",
       "| 10255988167|          0.1851|             0.4386|             0.0029|\n",
       "| 10430380319|          0.3681|              0.469|             5.0E-4|\n",
       "| 10618089367|          0.0633|             0.4418|             0.0118|\n",
       "+------------+----------------+-------------------+-------------------+"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sust_score = spark.read.parquet(dir + '/curated/Metric_Sustainability_scaled')\n",
    "print(sust_score.count())\n",
    "sust_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_score(full_dataset, customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>Unique_Customers</th><th>Transaction_per_User</th><th>customer_wealth</th><th>Loyal_AVG</th><th>Customer_Entropy</th></tr>\n",
       "<tr><td>10255988167</td><td>0.0338</td><td>0.0018</td><td>0.4196</td><td>0.0186</td><td>0.0842</td></tr>\n",
       "<tr><td>10430380319</td><td>0.0064</td><td>0.0</td><td>0.4133</td><td>0.006</td><td>0.0</td></tr>\n",
       "<tr><td>10618089367</td><td>0.1325</td><td>0.0064</td><td>0.4244</td><td>0.026</td><td>0.6354</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------------+--------------------+---------------+---------+----------------+\n",
       "|merchant_abn|Unique_Customers|Transaction_per_User|customer_wealth|Loyal_AVG|Customer_Entropy|\n",
       "+------------+----------------+--------------------+---------------+---------+----------------+\n",
       "| 10255988167|          0.0338|              0.0018|         0.4196|   0.0186|          0.0842|\n",
       "| 10430380319|          0.0064|                 0.0|         0.4133|    0.006|             0.0|\n",
       "| 10618089367|          0.1325|              0.0064|         0.4244|    0.026|          0.6354|\n",
       "+------------+----------------+--------------------+---------------+---------+----------------+"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_score = spark.read.parquet(dir + '/curated/Metric_Customer_scaled')\n",
    "print(cust_score.count())\n",
    "cust_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "industry(full_dataset, fraud_probabilities, spark, RECENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>sector</th><th>merchant_abn</th><th>Portion_of_Total_Revenue</th><th>Portion_of_Total_Transactions</th><th>Average_Weighted_Revenue</th><th>survival_rate</th></tr>\n",
       "<tr><td>Administrative_Su...</td><td>11633090957</td><td>0.0</td><td>0.0</td><td>0.9449</td><td>0.0</td></tr>\n",
       "<tr><td>Administrative_Su...</td><td>22718657980</td><td>0.0</td><td>0.0</td><td>0.9449</td><td>0.0</td></tr>\n",
       "<tr><td>Administrative_Su...</td><td>68112267199</td><td>0.0</td><td>0.0</td><td>0.9449</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------+------------------------+-----------------------------+------------------------+-------------+\n",
       "|              sector|merchant_abn|Portion_of_Total_Revenue|Portion_of_Total_Transactions|Average_Weighted_Revenue|survival_rate|\n",
       "+--------------------+------------+------------------------+-----------------------------+------------------------+-------------+\n",
       "|Administrative_Su...| 11633090957|                     0.0|                          0.0|                  0.9449|          0.0|\n",
       "|Administrative_Su...| 22718657980|                     0.0|                          0.0|                  0.9449|          0.0|\n",
       "|Administrative_Su...| 68112267199|                     0.0|                          0.0|                  0.9449|          0.0|\n",
       "+--------------------+------------+------------------------+-----------------------------+------------------------+-------------+"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_score = spark.read.parquet(dir + '/curated/Metric_industry_scaled')\n",
    "print(ind_score.count())\n",
    "ind_score.limit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
