{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant Ranking Algorithm\n",
    "The method of ranking each merchant, will take inspiration from the methods proposed in (https://sapinsider.org/leveraging-analytical-method-for-ranking-suppliers/), in which we scale each sub_attribute with min-max norminalisation, then sum them together to get the overall score for a particular Metric\n",
    "\n",
    "The Key Attributes/Metrics are: \n",
    "- Finantial \n",
    "- Customer_Base \n",
    "- Sustainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions (probably already implemented from other processing)\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import feature as H\n",
    "from pyspark.sql.functions import coalesce\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import re, sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This opens the spark session \n",
    "def open_spark():\n",
    "    spark = (\n",
    "    SparkSession.builder.appName(\"Data_Explorer\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate())\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This value can be changed to point to the directory where all the tables are stored. By default, it's the data folder in the generic-buy-now-pay-later directory.\n",
    "\n",
    "dir = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously Unused Functions\n",
    "For some reason these functions are not applied in the process of obtaining the processed/merhcants.parquet file, which will be requierd to be added to ensure this notebook/functions work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following can be removed, once added to the pre_processing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to standardise the description of each merchant\n",
    "def text_process(text):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    # Remove all punctuation and numbers \n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    # Remove all stopwords\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    # lemmatize and output\n",
    "    return ' '.join([stemmer.lemmatize(word) for word in nopunc])\n",
    "\n",
    "# this function standardises the tags attribute, creating a list with the 'description', 'revenue band' and 'BNPL service charge'\n",
    "def tag_extract(tag_string): \n",
    "    # first need to preprocess\n",
    "    string =  re.sub('\\[','(', tag_string.lower())\n",
    "    string = re.sub('\\]',')', string)\n",
    "    # break the string into sections\n",
    "    string_cut = string.split('),')\n",
    "    new_string = []\n",
    "    # first extract the description and pre process\n",
    "    descr = str(string_cut[0].strip('(('))\n",
    "    new_string.append(text_process(descr))\n",
    "    # second extract the band\n",
    "    new_string.append(str(re.search(r'[a-z]',string_cut[1]).group()))\n",
    "    # finally the take rate\n",
    "    new_string.append(float(re.search(r'[0-9]+\\.[0-9]+',string_cut[2]).group()))\n",
    "    return(new_string)\n",
    "\n",
    "# This function takes the pandas dataframe containing merchant information and pre_processes it\n",
    "def merchant_process(merchants, spark):\n",
    "    merchants = merchants.toPandas()\n",
    "    # Lets process the tags\n",
    "    tags = merchants['tags']\n",
    "    processed_tags = []\n",
    "    for i in tags:\n",
    "        processed_tags.append(tag_extract(i))\n",
    "    merchant_tbl = pd.DataFrame(processed_tags, columns=['Description', 'Earnings_Class', 'BNPL_Fee'])\n",
    "    merchant_tbl = pd.concat([merchants, merchant_tbl], axis=1)\n",
    "    # drop the tags column \n",
    "    merchant_tbl.drop(columns='tags', inplace=True)\n",
    "    merchant_tbl = spark.createDataFrame(merchant_tbl)\n",
    "    return merchant_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of additional Pre_processing \n",
    "also need to remove relevant line from main (merchant_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our scaling function\n",
    "def feature_standardisation(dataset, max_columns, min_columns = []): \n",
    "    # if higher values are prevered\n",
    "    for col_name in max_columns:\n",
    "        values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "        dataset = dataset.withColumn(col_name, F.round((F.col(col_name) - values.select('low').head()[0]) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    # if lower values are prefered\n",
    "    if min_columns:\n",
    "        for col_name in min_columns:\n",
    "            values = dataset.select(F.max(col_name).alias('high'), F.min(col_name).alias('low'))\n",
    "            dataset = dataset.withColumn(col_name, F.round((values.select('high').head()[0] - F.col(col_name)) / (values.select('high').head()[0] - values.select('low').head()[0]), 4))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Financial Metric\n",
    "For this we will look into:\n",
    "- Total_Revenue\n",
    "- Average_Revenue_Growth\n",
    "\n",
    "These are both taken over the last 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the revenue growth score\n",
    "def revenue_growth_score(revenue_data):\n",
    "    # Revenye Growth\n",
    "    revenue_data_table = revenue_data.select('merchant_abn').distinct()\n",
    "    # now for the past 6 monts calculate THIS IS HARD CODED\n",
    "    months = [3,4,5,6,7,8,9]\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            revenue_data_table = revenue_data_table.join(revenue_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.sum('BNPL_weighted_Revenue').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "    revenue_data_table = revenue_data_table.fillna(1)\n",
    "\n",
    "    # Now need to calculate growth rate for each month\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_4', (F.col('Month_4') - F.col('Month_3')) / F.col('Month_4'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_5', (F.col('Month_5') - F.col('Month_4')) / F.col('Month_5'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    revenue_data_table = revenue_data_table.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "\n",
    "    revenue_data_table = revenue_data_table.withColumn('Revenue_Growth_Avg', F.round((F.col('Growth_4') + F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9')) / 6 , 4))\n",
    "    revenue_data_table = revenue_data_table.select('merchant_abn', 'Revenue_Growth_Avg')\n",
    "    return revenue_data_table\n",
    "\n",
    "\n",
    "# This function scores each merchant finantially (i.e. Finantial Score)\n",
    "def finantial_score(RECENCY, transactions, fraud_data):\n",
    "    # first d\n",
    "    revenue_data = transactions.where(F.col('order_datetime') > RECENCY).select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue')\n",
    "    # Now join fraud prob \n",
    "    revenue_data = revenue_data.join(fraud_data, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    # Firstly, calculate the revenue score\n",
    "\n",
    "    # Weight the revenue weighted with probability\n",
    "    revenue_data = revenue_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction')))\n",
    "    # Now can caluculate each Merchants revenue\n",
    "    revenue_final = revenue_data.groupBy('merchant_abn').agg(F.round(F.sum('BNPL_weighted_Revenue'), 2).alias('Total_Revenue'))\n",
    "    # Next, get the revenue growth score \n",
    "    revenue_final = revenue_final.join(revenue_growth_score(revenue_data), on='merchant_abn', how='left')\n",
    "    # Finally scale\n",
    "    revenue_final = feature_standardisation(revenue_final, ['Revenue_Growth_Avg', 'Total_Revenue'])\n",
    "    # save file \n",
    "    revenue_final.write.parquet('../' + dir + 'curated/Metric_Finantial_scaled', mode ='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability \n",
    "- Experience\n",
    "- Customer Growth (Last 6 months)\n",
    "- Postcode_Entropy\n",
    "- Industry score (added when scoring each metric [next notebook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_growth(full_dataset):\n",
    "    # First grab total Unique customers \n",
    "    Unique_cust = full_dataset.where(F.col('order_datetime') < F.lit('2022-05-01')).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'))\n",
    "    # now for the past 6 monts calculate \n",
    "    months = [5,6,7,8,9,10]\n",
    "    customer_counting_data = full_dataset.select('merchant_abn', 'user_id', 'order_datetime', 'month')\n",
    "    for month in months:\n",
    "        end_month = month + 1\n",
    "        if end_month < 10:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-0' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "        else:\n",
    "            Unique_cust = Unique_cust.join(customer_counting_data.where((F.col('order_datetime') < F.lit('2022-' + str(end_month)+'-01'))).groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Month_' + str(month))), on='merchant_abn', how='left')\n",
    "    Unique_cust = Unique_cust.fillna(1)\n",
    "    # Now need to calculate growth rate for each month\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_5', F.col('Unique_Customers') / F.col('Month_5'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_6', (F.col('Month_6') - F.col('Month_5')) / F.col('Month_6'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_7', (F.col('Month_7') - F.col('Month_6'))/ F.col('Month_7'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_8', (F.col('Month_8') - F.col('Month_7')) / F.col('Month_8'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_9', (F.col('Month_9') - F.col('Month_8')) / F.col('Month_9'))\n",
    "    Unique_cust = Unique_cust.withColumn('Growth_10',(F.col('Month_10') - F.col('Month_9')) / F.col('Month_10'))\n",
    "    # Finally get the average customer growth rate over the last 6 months\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.round((F.col('Growth_5') + F.col('Growth_6') + F.col('Growth_7') + F.col('Growth_8') + F.col('Growth_9') + F.col('Growth_10')) / 6 , 4))\n",
    "    Unique_cust = Unique_cust.withColumn('Customer_Growth_Avg', F.when(F.col('Customer_Growth_Avg').isNull(), 0).otherwise(F.col('Customer_Growth_Avg')))\n",
    "    return Unique_cust.select('merchant_abn', 'Customer_Growth_Avg')\n",
    "\n",
    "def sustainability_score(merchants, full_dataset):\n",
    "    merchants_sub = merchants.select(['merchant_abn', 'postcode_entropy'])\n",
    "    merchants_sub = merchants_sub.join(customer_growth(full_dataset), on='merchant_abn', how='left')\n",
    "    # calculate experience\n",
    "    number_of_dates_by_merchant = full_dataset.groupBy('merchant_abn','order_datetime').count()\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.drop('order_datetime')\n",
    "    number_of_dates_by_merchant = number_of_dates_by_merchant.groupBy('merchant_abn').sum('count')\n",
    "    merchants_sub = merchants_sub.join(number_of_dates_by_merchant.withColumnRenamed('sum(count)','Total_Business_Days'), on='merchant_abn', how='left')\n",
    "    merchants_final = feature_standardisation(merchants_sub, ['Total_Business_Days', 'Customer_Growth_Avg'], ['postcode_entropy'])\n",
    "    # save file \n",
    "    merchants_final.write.parquet(dir + 'curated/Metric_Sustainability_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Base\n",
    "In this sections we use the features:\n",
    "- Transaction_per_User\n",
    "- customer_wealth\n",
    "- Unique_Customers\n",
    "- Loyal_AVG\n",
    "- postcode_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loyalty(full_dataset):\n",
    "    # Create Loyalfy feature\n",
    "    grouped = full_dataset.groupBy(\"user_id\", \"merchant_abn\")\n",
    "    RPR = grouped.count().withColumnRenamed(\"count\", \"RPR\")\n",
    "    upSell = RPR.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"upsell\")\n",
    "    CLV = grouped.sum(\"dollar_value\").withColumnRenamed(\"sum(dollar_value)\", \"CLV\")\n",
    "    # Define the window\n",
    "    w = Window.partitionBy([\"user_id\", \"merchant_abn\"]).orderBy(\"order_datetime\")\n",
    "\n",
    "    retention = full_dataset.withColumn(\n",
    "        'diff',\n",
    "        F.datediff(F.col(\"order_datetime\"), F.lag(\"order_datetime\").over(w))\n",
    "    ).groupBy(\"user_id\", \"merchant_abn\").agg(F.avg(F.col(\"diff\")).alias(\"retention\"))\n",
    "    loyal = retention.na.fill(value=365).join(RPR, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(CLV, on=[\"user_id\", \"merchant_abn\"], how=\"left\").join(upSell, on=[\"user_id\"], how=\"left\")\n",
    "    loyal = loyal.withColumn(\"loyal\", F.col(\"RPR\") * F.col(\"CLV\") * F.col(\"upSell\") / F.col(\"retention\"))\n",
    "    loyal = loyal.select(*(col(c).cast(\"float\").alias(c) for c in loyal.columns))\n",
    "    loyal = loyal.select('user_id', 'merchant_abn', 'loyal')\n",
    "    magic_percentile = F.expr('percentile_approx(loyal, 0.5)')\n",
    "    loyal = loyal.join(loyal.groupBy('user_id').agg(magic_percentile.alias('med_val')), on='user_id', how='left')\n",
    "    loyal = loyal.withColumn('loyal', coalesce('loyal', 'med_val')).select(['user_id', 'merchant_abn', 'loyal'])\n",
    "    loyal_agg = loyal.groupBy('merchant_abn').agg(F.round(F.avg('loyal'), 2).alias('Loyal_AVG'))\n",
    "    return loyal_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_entropy(full_dataset):\n",
    "    # Get customer Entropy \n",
    "    cust_entro = full_dataset.select('merchant_abn', 'user_id', 'postcode', 'order_datetime')\n",
    "    Entropy_cust = cust_entro.select('merchant_abn').toDF('merchant_abn').drop_duplicates().crossJoin(cust_entro.select('postcode').toDF('postcode').drop_duplicates())\n",
    "    Entropy_cust = Entropy_cust.join(cust_entro.groupBy('merchant_abn', 'postcode').agg(F.countDistinct('user_id').alias('Count')), on=['merchant_abn', 'postcode'], how='left')\n",
    "    Entropy_cust = Entropy_cust.fillna(1)\n",
    "    Entropy_cust = Entropy_cust.join(Entropy_cust.groupBy(\"merchant_abn\").sum('Count'), on='merchant_abn', how='left')\n",
    "    Entropy_cust = Entropy_cust.withColumn('Probability', F.col('Count') / F.col('sum(Count)'))\n",
    "    Entropy_cust = Entropy_cust.groupBy(\"merchant_abn\").agg((-F.sum(F.col(\"Probability\") * F.log2(col(\"Probability\")))).alias(\"Customer_Entropy\"))\n",
    "    return Entropy_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_score(full_dataset, customers):\n",
    "    full_dataset = full_dataset.join(customers, on='user_id')\n",
    "    # add some of the attributes\n",
    "    cust_data = full_dataset.groupBy('merchant_abn').agg(F.countDistinct('user_id').alias('Unique_Customers'), F.round(F.count('user_id') / F.countDistinct('user_id'), 2).alias('Transaction_per_User'), F.round(F.avg('Average taxable income or loss'),2).alias('customer_wealth'))\n",
    "    # add the loyalty score\n",
    "    cust_data = cust_data.join(loyalty(full_dataset), on='merchant_abn')\n",
    "    cust_data = cust_data.join(customer_entropy(full_dataset), on='merchant_abn', how='left')\n",
    "    cust_final = feature_standardisation(cust_data, ['Transaction_per_User', 'customer_wealth', 'Unique_Customers', 'Loyal_AVG'], ['Customer_Entropy'])\n",
    "    # save file \n",
    "    cust_final.write.parquet(dir + 'curated/Metric_Customer_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry/Environment \n",
    "We add a general score, based on their description tags, in relation to which industry they belong to, including:\n",
    "- Market Dominance\n",
    "- Survival Rate\n",
    "- Fraud_Prob_Avg\n",
    "- Customer_Base\n",
    "- Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_transformation(spark):\n",
    "    Administrative_Support_Services= [\"equipment tool furniture appliance rent al leasing\",\n",
    "                                      \"florist supply nursery stock flower\",\n",
    "                                      \"lawn garden supply outlet including nursery\"]\n",
    "    Personal_Services= [\"shoe shop\",\n",
    "                        \"gift card novelty souvenir shop\",\n",
    "                        \"antique shop sale repair restoration service\",\n",
    "                        \"watch clock jewelry repair shop\",\n",
    "                        \"jewelry watch clock silverware shop\",\n",
    "                        \"motor vehicle supply new part\",\n",
    "                        \"furniture home furnishing equipment shop manufacturer except appliance\",\n",
    "                        \"tent awning shop\",\n",
    "                        \"optician optical good eyeglass\"]\n",
    "    Arts_Recreation_Services = [\"digital good book movie music\",\n",
    "                                \"music shop musical instrument piano sheet music\",\n",
    "                                \"health beauty spa\",\n",
    "                                \"bicycle shop sale service\",\n",
    "                                \"art dealer gallery\",\n",
    "                                \"hobby toy game shop\",\n",
    "                                \"stationery office supply printing writing paper\"]\n",
    "    Information_Media_Telecommunications = [\"telecom\",\n",
    "                                            \"computer programming data processing integrated system design service\",\n",
    "                                            \"book periodical newspaper\",\n",
    "                                            \"artist supply craft shop\",\n",
    "                                            \"computer computer peripheral equipment software\",\n",
    "                                            \"cable satellite pay television radio service\"]\n",
    "    desc = []\n",
    "    tags = []\n",
    "    for ele in Administrative_Support_Services:\n",
    "        desc.append(\"Administrative_Support_Services\")\n",
    "    tags += Administrative_Support_Services\n",
    "    for ele in Personal_Services:\n",
    "        desc.append(\"Personal_Services\")\n",
    "    tags += Personal_Services\n",
    "    for ele in Arts_Recreation_Services:\n",
    "        desc.append(\"Arts_Recreation_Services\")\n",
    "    tags += Arts_Recreation_Services\n",
    "    for ele in Information_Media_Telecommunications:\n",
    "        desc.append(\"Information_Media_Telecommunications\")\n",
    "    tags += Information_Media_Telecommunications\n",
    "\n",
    "    to_df = {\n",
    "        \"sector\" :  desc,\n",
    "        \"Description\" : tags\n",
    "    }\n",
    "    sectors = spark.createDataFrame(pd.DataFrame(to_df))\n",
    "    # save sectors\n",
    "    return sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry(full_dataset, fraud_probabilities, spark, RECENCY):\n",
    "    desc_lookup = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(dir + \"/tables/description_lookup.csv\")\n",
    "    industry_data = full_dataset.join(desc_lookup, )\n",
    "    industry_data = full_dataset.select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue', 'Description')\n",
    "    # Now join fraud prob \n",
    "    industry_data = industry_data.join(fraud_probabilities, on=['merchant_abn', 'user_id', 'order_datetime'], how='left')\n",
    "    industry_data = industry_data.withColumn('BNPL_weighted_Revenue', F.col('BNPL_Revenue') * (1 - 0.1 * F.col('prediction'))\n",
    "                                                                 )\n",
    "    dataset_sector = description_transformation(spark)\n",
    "    industry_data = industry_data.join(dataset_sector, on=['Description'], how='inner').drop('Description')\n",
    "    Merchant_Industry = industry_data.select('merchant_abn', 'sector').distinct()\n",
    "    # Market Dominance\n",
    "    industry_table = industry_data.where(F.col('order_datetime') > RECENCY).groupBy('sector').agg(F.sum('BNPL_weighted_Revenue').alias('Total_Weighted_Revenue'), F.count('BNPL_weighted_Revenue').alias('Total_Transactions'), F.avg('BNPL_weighted_Revenue').alias('Average_Weighted_Revenue'))\n",
    "    total_rev = industry_table.groupBy().sum().collect()[0][0]\n",
    "    total_transa = industry_table.groupBy().sum().collect()[0][0]\n",
    "    industry_table = industry_table.withColumn('Total_Weighted_Revenue', F.col('Total_Weighted_Revenue') / total_rev).withColumnRenamed('Total_Weighted_Revenue', 'Portion_of_Total_Revenue')\n",
    "    industry_table = industry_table.withColumn('Total_Transactions', F.col('Total_Transactions') / total_transa).withColumnRenamed('Total_Transactions', 'Portion_of_Total_Transactions')\n",
    "    # Survival Rate\n",
    "    sector_info = spark.read.parquet(dir + 'tables/sector_information.parquet')\n",
    "    industry_table = industry_table.join(sector_info.select(F.col('sector'), F.col('survival_rate').cast('double').alias(\"survival_rate\")), on='sector', how='left')\n",
    "    industry_table = feature_standardisation(industry_table, ['Portion_of_Total_Revenue', 'Portion_of_Total_Transactions', 'survival_rate', 'Average_Weighted_Revenue'])\n",
    "    # ensure assigned to each merchant\n",
    "    industry_table = Merchant_Industry.join(industry_table, on='sector', how='left')\n",
    "    # save file \n",
    "    industry_table.write.parquet(dir + 'curated/Metric_industry_scaled', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function, which can run everything\n",
    "\n",
    "spark = open_spark()\n",
    "customers = spark.read.option(\"inferSchema\", True).parquet(dir + 'processed/customers/')\n",
    "transactions = spark.read.option(\"inferSchema\", True).parquet(dir + 'processed/transactions')\n",
    "merchants = spark.read.option(\"inferSchema\", True).parquet(dir + \"processed/merchants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_lookup = spark.read.option(\"header\", False).csv(dir + \"/tables/description_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th></tr>\n",
       "<tr><td>furniture home fu...</td><td> 0</td></tr>\n",
       "<tr><td>cable satellite p...</td><td> 1</td></tr>\n",
       "<tr><td>jewelry watch clo...</td><td> 2</td></tr>\n",
       "<tr><td>watch clock jewel...</td><td> 3</td></tr>\n",
       "<tr><td>music shop musica...</td><td> 39548</td></tr>\n",
       "<tr><td>gift card novelty...</td><td> 5</td></tr>\n",
       "<tr><td>computer computer...</td><td> 6</td></tr>\n",
       "<tr><td>computer programm...</td><td> 7</td></tr>\n",
       "<tr><td>equipment tool fu...</td><td> 8</td></tr>\n",
       "<tr><td>artist supply cra...</td><td> 9</td></tr>\n",
       "<tr><td>florist supply nu...</td><td> 10</td></tr>\n",
       "<tr><td>antique shop sale...</td><td> 11</td></tr>\n",
       "<tr><td>motor vehicle sup...</td><td> 12</td></tr>\n",
       "<tr><td>book periodical n...</td><td> 13</td></tr>\n",
       "<tr><td>stationery office...</td><td> 14</td></tr>\n",
       "<tr><td>tent awning shop</td><td> 15</td></tr>\n",
       "<tr><td>art dealer gallery</td><td> 16</td></tr>\n",
       "<tr><td>bicycle shop sale...</td><td> 17</td></tr>\n",
       "<tr><td>digital good book...</td><td> 18</td></tr>\n",
       "<tr><td>shoe shop</td><td> 19</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+------+\n",
       "|                 _c0|   _c1|\n",
       "+--------------------+------+\n",
       "|furniture home fu...|     0|\n",
       "|cable satellite p...|     1|\n",
       "|jewelry watch clo...|     2|\n",
       "|watch clock jewel...|     3|\n",
       "|music shop musica...| 39548|\n",
       "|gift card novelty...|     5|\n",
       "|computer computer...|     6|\n",
       "|computer programm...|     7|\n",
       "|equipment tool fu...|     8|\n",
       "|artist supply cra...|     9|\n",
       "|florist supply nu...|    10|\n",
       "|antique shop sale...|    11|\n",
       "|motor vehicle sup...|    12|\n",
       "|book periodical n...|    13|\n",
       "|stationery office...|    14|\n",
       "|    tent awning shop|    15|\n",
       "|  art dealer gallery|    16|\n",
       "|bicycle shop sale...|    17|\n",
       "|digital good book...|    18|\n",
       "|           shoe shop|    19|\n",
       "+--------------------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 10:29:14 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2069753 ms exceeds timeout 120000 ms\n",
      "22/10/19 10:29:14 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "desc_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_collection = merchants.select('merchant_abn')\n",
    "fraud_probabilities = spark.read.parquet(dir + '../models/random_forest_output_full/')\n",
    "fraud_probabilities = fraud_probabilities.select('merchant_abn', 'user_id', 'order_datetime', 'prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# APPLY PREVIOUSELY SHARED & UNIMPLEMENTED FUNCTIONS\n",
    "# CHANGED!! merchants = merchant_process(merchants, spark)\n",
    "full_dataset = transactions.join(merchants, on='merchant_abn', how='inner')\n",
    "# calculate the BNPL unweighted revenue \n",
    "full_dataset = full_dataset.withColumn('BNPL_Revenue', F.col('dollar_value') * F.col('BNPL_Fee'))\n",
    "# First calculate the finantial score\n",
    "\n",
    "# We will for now determine as last 6 months (i.e march) \n",
    "RECENCY =  F.lit('2022-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>name</th><th>Earnings_Class</th><th>BNPL_Fee</th><th>tags</th><th>avg_monthly_inc</th><th>monthly_entropy</th><th>postcode_entropy</th><th>revenue</th></tr>\n",
       "<tr><td>10023283211</td><td>Felis Limited</td><td>e</td><td>0.18</td><td>0</td><td>-0.0952381</td><td>2.9858725</td><td>7.439226</td><td>703277.8708539009</td></tr>\n",
       "<tr><td>10142254217</td><td>Arcu Ac Orci Corp...</td><td>b</td><td>4.22</td><td>1</td><td>0.0</td><td>2.9779751</td><td>7.418948</td><td>118355.94002620317</td></tr>\n",
       "<tr><td>10165489824</td><td>Nunc Sed Company</td><td>b</td><td>4.4</td><td>2</td><td>3.0</td><td>1.609438</td><td>1.609438</td><td>56180.481201171875</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------+--------------+--------+----+---------------+---------------+----------------+------------------+\n",
       "|merchant_abn|                name|Earnings_Class|BNPL_Fee|tags|avg_monthly_inc|monthly_entropy|postcode_entropy|           revenue|\n",
       "+------------+--------------------+--------------+--------+----+---------------+---------------+----------------+------------------+\n",
       "| 10023283211|       Felis Limited|             e|    0.18|   0|     -0.0952381|      2.9858725|        7.439226| 703277.8708539009|\n",
       "| 10142254217|Arcu Ac Orci Corp...|             b|    4.22|   1|            0.0|      2.9779751|        7.418948|118355.94002620317|\n",
       "| 10165489824|    Nunc Sed Company|             b|     4.4|   2|            3.0|       1.609438|        1.609438|56180.481201171875|\n",
       "+------------+--------------------+--------------+--------+----+---------------+---------------+----------------+------------------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchants.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now run each score, which will save results to file\n",
    "# Note: customer score takes much linger, and sould probably run seperate\n",
    "finantial_score(RECENCY, full_dataset, fraud_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>Total_Revenue</th><th>Revenue_Growth_Avg</th></tr>\n",
       "<tr><td>12516851436</td><td>0.0036</td><td>0.55</td></tr>\n",
       "<tr><td>15613631617</td><td>0.0027</td><td>0.4736</td></tr>\n",
       "<tr><td>19839532017</td><td>0.008</td><td>0.4683</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+-------------+------------------+\n",
       "|merchant_abn|Total_Revenue|Revenue_Growth_Avg|\n",
       "+------------+-------------+------------------+\n",
       "| 12516851436|       0.0036|              0.55|\n",
       "| 15613631617|       0.0027|            0.4736|\n",
       "| 19839532017|        0.008|            0.4683|\n",
       "+------------+-------------+------------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_score = spark.read.parquet(dir + '/curated/Metric_Finantial_scaled')\n",
    "print(fin_score.count())\n",
    "fin_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sustainability_score(merchants, full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>postcode_entropy</th><th>Customer_Growth_Avg</th><th>Total_Business_Days</th></tr>\n",
       "<tr><td>12516851436</td><td>0.3367</td><td>0.4398</td><td>7.0E-4</td></tr>\n",
       "<tr><td>15613631617</td><td>0.1129</td><td>0.4213</td><td>0.0062</td></tr>\n",
       "<tr><td>19839532017</td><td>0.1947</td><td>0.4102</td><td>0.0025</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------------+-------------------+-------------------+\n",
       "|merchant_abn|postcode_entropy|Customer_Growth_Avg|Total_Business_Days|\n",
       "+------------+----------------+-------------------+-------------------+\n",
       "| 12516851436|          0.3367|             0.4398|             7.0E-4|\n",
       "| 15613631617|          0.1129|             0.4213|             0.0062|\n",
       "| 19839532017|          0.1947|             0.4102|             0.0025|\n",
       "+------------+----------------+-------------------+-------------------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sust_score = spark.read.parquet(dir + '/curated/Metric_Sustainability_scaled')\n",
    "print(sust_score.count())\n",
    "sust_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_score(full_dataset, customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>Unique_Customers</th><th>Transaction_per_User</th><th>customer_wealth</th><th>Loyal_AVG</th><th>Customer_Entropy</th></tr>\n",
       "<tr><td>10255988167</td><td>0.0338</td><td>0.0018</td><td>0.4196</td><td>0.0188</td><td>0.0842</td></tr>\n",
       "<tr><td>10430380319</td><td>0.0064</td><td>0.0</td><td>0.4133</td><td>0.0061</td><td>0.0</td></tr>\n",
       "<tr><td>10618089367</td><td>0.1325</td><td>0.0064</td><td>0.4244</td><td>0.0262</td><td>0.6354</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------------+--------------------+---------------+---------+----------------+\n",
       "|merchant_abn|Unique_Customers|Transaction_per_User|customer_wealth|Loyal_AVG|Customer_Entropy|\n",
       "+------------+----------------+--------------------+---------------+---------+----------------+\n",
       "| 10255988167|          0.0338|              0.0018|         0.4196|   0.0188|          0.0842|\n",
       "| 10430380319|          0.0064|                 0.0|         0.4133|   0.0061|             0.0|\n",
       "| 10618089367|          0.1325|              0.0064|         0.4244|   0.0262|          0.6354|\n",
       "+------------+----------------+--------------------+---------------+---------+----------------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_score = spark.read.parquet(dir + '/curated/Metric_Customer_scaled')\n",
    "print(cust_score.count())\n",
    "cust_score.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'Description' does not exist. Did you mean one of the following? [merchant_abn, user_id, order_id, Natural_var, holiday, month, revenue, tags, BNPL_Fee, dayofmonth, dayofweek, dollar_value, name, order_datetime, BNPL_Revenue, postcode_entropy, avg_monthly_inc, monthly_entropy, Earnings_Class, Potential_Outlier];\n'Project [merchant_abn#70L, user_id#69L, order_datetime#72, dollar_value#71, BNPL_Revenue#163, 'Description]\n+- Project [merchant_abn#70L, order_id#68L, user_id#69L, dollar_value#71, order_datetime#72, Natural_var#73, Potential_Outlier#74, holiday#75L, dayofmonth#76, month#77, dayofweek#78, name#91, Earnings_Class#92, BNPL_Fee#93, tags#94, avg_monthly_inc#95, monthly_entropy#96, postcode_entropy#97, revenue#98, (cast(dollar_value#71 as double) * BNPL_Fee#93) AS BNPL_Revenue#163]\n   +- Project [merchant_abn#70L, order_id#68L, user_id#69L, dollar_value#71, order_datetime#72, Natural_var#73, Potential_Outlier#74, holiday#75L, dayofmonth#76, month#77, dayofweek#78, name#91, Earnings_Class#92, BNPL_Fee#93, tags#94, avg_monthly_inc#95, monthly_entropy#96, postcode_entropy#97, revenue#98]\n      +- Join Inner, (merchant_abn#70L = merchant_abn#90L)\n         :- Relation [order_id#68L,user_id#69L,merchant_abn#70L,dollar_value#71,order_datetime#72,Natural_var#73,Potential_Outlier#74,holiday#75L,dayofmonth#76,month#77,dayofweek#78] parquet\n         +- Relation [merchant_abn#90L,name#91,Earnings_Class#92,BNPL_Fee#93,tags#94,avg_monthly_inc#95,monthly_entropy#96,postcode_entropy#97,revenue#98] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m industry(full_dataset, fraud_probabilities, spark, RECENCY)\n",
      "\u001b[1;32m/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb Cell 31\u001b[0m in \u001b[0;36mindustry\u001b[0;34m(full_dataset, fraud_probabilities, spark, RECENCY)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mindustry\u001b[39m(full_dataset, fraud_probabilities, spark, RECENCY):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     industry_data \u001b[39m=\u001b[39m full_dataset\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39mmerchant_abn\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39morder_datetime\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdollar_value\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mBNPL_Revenue\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mDescription\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Now join fraud prob \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshitasoni/GitHub/generic-buy-now-pay-later-project-group-3/notebooks/Ranking_System_P1.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     industry_data \u001b[39m=\u001b[39m industry_data\u001b[39m.\u001b[39mjoin(fraud_probabilities, on\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmerchant_abn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_datetime\u001b[39m\u001b[39m'\u001b[39m], how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   2024\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'Description' does not exist. Did you mean one of the following? [merchant_abn, user_id, order_id, Natural_var, holiday, month, revenue, tags, BNPL_Fee, dayofmonth, dayofweek, dollar_value, name, order_datetime, BNPL_Revenue, postcode_entropy, avg_monthly_inc, monthly_entropy, Earnings_Class, Potential_Outlier];\n'Project [merchant_abn#70L, user_id#69L, order_datetime#72, dollar_value#71, BNPL_Revenue#163, 'Description]\n+- Project [merchant_abn#70L, order_id#68L, user_id#69L, dollar_value#71, order_datetime#72, Natural_var#73, Potential_Outlier#74, holiday#75L, dayofmonth#76, month#77, dayofweek#78, name#91, Earnings_Class#92, BNPL_Fee#93, tags#94, avg_monthly_inc#95, monthly_entropy#96, postcode_entropy#97, revenue#98, (cast(dollar_value#71 as double) * BNPL_Fee#93) AS BNPL_Revenue#163]\n   +- Project [merchant_abn#70L, order_id#68L, user_id#69L, dollar_value#71, order_datetime#72, Natural_var#73, Potential_Outlier#74, holiday#75L, dayofmonth#76, month#77, dayofweek#78, name#91, Earnings_Class#92, BNPL_Fee#93, tags#94, avg_monthly_inc#95, monthly_entropy#96, postcode_entropy#97, revenue#98]\n      +- Join Inner, (merchant_abn#70L = merchant_abn#90L)\n         :- Relation [order_id#68L,user_id#69L,merchant_abn#70L,dollar_value#71,order_datetime#72,Natural_var#73,Potential_Outlier#74,holiday#75L,dayofmonth#76,month#77,dayofweek#78] parquet\n         +- Relation [merchant_abn#90L,name#91,Earnings_Class#92,BNPL_Fee#93,tags#94,avg_monthly_inc#95,monthly_entropy#96,postcode_entropy#97,revenue#98] parquet\n"
     ]
    }
   ],
   "source": [
    "industry(full_dataset, fraud_probabilities, spark, RECENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- dollar_value: float (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- Natural_var: integer (nullable = true)\n",
      " |-- Potential_Outlier: integer (nullable = true)\n",
      " |-- holiday: long (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Earnings_Class: string (nullable = true)\n",
      " |-- BNPL_Fee: double (nullable = true)\n",
      " |-- tags: integer (nullable = true)\n",
      " |-- avg_monthly_inc: float (nullable = true)\n",
      " |-- monthly_entropy: float (nullable = true)\n",
      " |-- postcode_entropy: float (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- BNPL_Revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full_dataset.select('merchant_abn','user_id', 'order_datetime', 'dollar_value', 'BNPL_Revenue', 'Description')\n",
    "full_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
